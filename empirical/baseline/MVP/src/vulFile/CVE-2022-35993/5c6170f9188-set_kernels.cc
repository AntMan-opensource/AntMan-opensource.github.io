



























namespace tensorflow {

using ShapeArray = sparse::SparseTensor::ShapeArray;
using VarDimArray = sparse::SparseTensor::VarDimArray;


void CheckRankAtLeast2(OpKernelContext* ctx, const TensorShape& shape) {
  const auto rank = shape.dims();
  OP_REQUIRES(ctx, rank >= 2, errors::InvalidArgument("Invalid rank ", rank, "."));
}


Status GroupShape(const VarDimArray& input_shape, ShapeArray* grouped_shape) {
  if (input_shape.size() < 2) {
    
    return errors::InvalidArgument("Shape [", absl::StrJoin(input_shape, ","), "] has rank ", input_shape.size(), " < 2");
  }
  
  *grouped_shape = ShapeArray(input_shape.begin(), input_shape.end() - 1);
  return OkStatus();
}



Status SparseTensorFromContext(OpKernelContext* ctx, const int32_t base_index, const bool validate_indices, sparse::SparseTensor* tensor) {

  
  TensorShape shape;
  TF_RETURN_IF_ERROR(TensorShape::BuildTensorShape( ctx->input(base_index + 2).vec<int64_t>(), &shape));
  CheckRankAtLeast2(ctx, shape);
  std::vector<int64_t> order(shape.dims());
  std::iota(order.begin(), order.end(), 0);

  Status status = sparse::SparseTensor::Create( ctx->input(base_index), ctx->input(base_index + 1), shape, order, tensor);

  if (!validate_indices || !status.ok()) return status;
  return tensor->IndicesValid();
}





template <typename T> void CheckGroup(OpKernelContext* ctx, const sparse::Group& group, const VarDimArray& sparse_tensor_shape) {

  const auto& indices = group.indices();
  const auto& values = group.values<T>();

  
  const auto num_values = values.dimension(0);
  OP_REQUIRES(ctx, indices.size() > 0, errors::Internal("Empty group."));
  OP_REQUIRES( ctx, indices.dimension(0) == num_values, errors::Internal("shape[0] of group indices ", indices.dimension(0), " != values ", num_values, "."));



  
  const auto group_rank = indices.dimension(1);
  const auto expected_rank = sparse_tensor_shape.size();
  OP_REQUIRES(ctx, expected_rank == group_rank, errors::Internal("Rank expected ", expected_rank, ", got ", group_rank, "."));

  for (int32_t j = 0; j < expected_rank; ++j) {
    const auto dim_size = sparse_tensor_shape[j];
    OP_REQUIRES( ctx, dim_size > 0, errors::Internal("Invalid dim_size[", j, "] = ", dim_size, "."));

    for (int64_t i = 0; i < num_values; ++i) {
      const auto index = indices(i, j);
      OP_REQUIRES(ctx, dim_size > index, errors::Internal("indices[", i, ", ", j, "] expected < ", dim_size, ", got ", index, "."));

    }
  }
}


const ShapeArray Strides(const VarDimArray& shape) {
  ShapeArray result(shape.size());
  int64_t product = 1;
  for (int i = shape.size() - 1; i >= 0; --i) {
    result[i] = product;
    product *= shape[i];
  }
  return result;
}













template <typename T> void OutputSparseTensor( OpKernelContext* ctx, const TensorShape& output_shape, const int64_t num_values, const std::vector<std::pair<std::vector<int64_t>, absl::btree_set<T>>>& sets) {




  
  Tensor *out_indices_t, *out_values_t, *out_shape_t;
  OP_REQUIRES_OK(ctx, ctx->allocate_output( 0, TensorShape({num_values, output_shape.dims()}), &out_indices_t));

  OP_REQUIRES_OK( ctx, ctx->allocate_output(1, TensorShape({num_values}), &out_values_t));
  OP_REQUIRES_OK(ctx, ctx->allocate_output( 2, TensorShape({output_shape.dims()}), &out_shape_t));
  auto out_indices_mat = out_indices_t->matrix<int64_t>();
  auto out_values_flat = out_values_t->vec<T>();

  
  int64_t value_index = 0;
  for (auto it = sets.begin(); it != sets.end(); ++it) {
    const auto& group_indices = it->first;
    OP_REQUIRES( ctx, group_indices.size() == output_shape.dims() - 1, errors::Internal("Invalid number of indices ", group_indices.size(), ", expected ", output_shape.dims() - 1, "."));


    const auto& set = it->second;

    
    int64_t group_value_index = 0;
    for (auto value = set.begin(); value != set.end();
         ++value, ++value_index, ++group_value_index) {
      
      
      for (int32_t i = 0; i < group_indices.size(); ++i) {
        out_indices_mat(value_index, i) = group_indices[i];
      }
      out_indices_mat(value_index, group_indices.size()) = group_value_index;

      out_values_flat(value_index) = *value;
    }
  }

  
  auto out_shape_flat = out_shape_t->vec<int64_t>();
  for (int32_t i = 0; i < output_shape.dims(); ++i) {
    out_shape_flat(i) = output_shape.dim_size(i);
  }
}

bool ValidateIndicesFromContext(OpKernelConstruction* ctx) {
  bool result;
  if (ctx->GetAttr("validate_indices", &result).ok()) {
    return result;
  }
  return true;
}






template <typename T> void PopulateFromDenseGroup(OpKernelContext* ctx, const Tensor& input_tensor, const VarDimArray& input_strides, const std::vector<int64_t>& group_indices, absl::flat_hash_set<T>* result) {



  OP_REQUIRES(ctx, group_indices.size() == input_strides.size() - 1, errors::Internal("group_indices.size ", group_indices.size(), ", !=  input_strides.size-1 ", input_strides.size() - 1, "."));


  result->clear();
  auto input_flat = input_tensor.flat<T>();
  const auto start = std::inner_product( group_indices.begin(), group_indices.end(), input_strides.begin(), 0LL);
  const TensorShape& input_shape = input_tensor.shape();
  const auto end = start + input_shape.dim_size(input_shape.dims() - 1);
  for (int64_t i = start; i < end; ++i) {
    result->insert(input_flat(i));
  }
}




template <typename T> void PopulateFromSparseGroup(OpKernelContext* ctx, const sparse::Group& group, const VarDimArray& sparse_tensor_shape, absl::flat_hash_set<T>* result) {


  CheckGroup<T>(ctx, group, sparse_tensor_shape);
  result->clear();
  const auto& group_values = group.values<T>();
  for (int64_t i = 0; i < group_values.size(); ++i) {
    result->insert(group_values(i));
  }
}

template <typename T> class SetSizeOp : public OpKernel {
 public:
  explicit SetSizeOp(OpKernelConstruction* ctx)
      : OpKernel(ctx), validate_indices_(ValidateIndicesFromContext(ctx)) {}

  void Compute(OpKernelContext* ctx) override;

 private:
  const bool validate_indices_;
};

template <typename T> void SetSizeOp<T>::Compute(OpKernelContext* ctx) {
  sparse::SparseTensor set_st;
  OP_REQUIRES_OK(ctx, SparseTensorFromContext(ctx, 0, validate_indices_, &set_st));

  
  
  ShapeArray output_shape;
  OP_REQUIRES_OK(ctx, GroupShape(set_st.shape(), &output_shape));
  const auto output_strides = Strides(output_shape);

  TensorShape output_shape_ts;
  OP_REQUIRES_OK(ctx, TensorShapeUtils::MakeShape(output_shape, &output_shape_ts));
  Tensor* out_t;
  OP_REQUIRES_OK(ctx, ctx->allocate_output(0, output_shape_ts, &out_t));
  auto out = out_t->flat<int32>();
  out.device(ctx->eigen_cpu_device()) = out.constant(static_cast<int32>(0.0));

  
  
  VarDimArray group_ix = set_st.order().subspan(0, set_st.order().size() - 1);
  absl::flat_hash_set<T> group_set;
  for (const auto& group : set_st.group(group_ix)) {
    PopulateFromSparseGroup<T>(ctx, group, set_st.shape(), &group_set);

    const auto group_key = group.group();
    const auto output_index = std::inner_product( group_key.begin(), group_key.end(), output_strides.begin(), 0LL);
    out(output_index) = group_set.size();
  }
}




_SET_SIZE_REGISTER_KERNEL_BUILDER(int8);
_SET_SIZE_REGISTER_KERNEL_BUILDER(int16);
_SET_SIZE_REGISTER_KERNEL_BUILDER(int32);
_SET_SIZE_REGISTER_KERNEL_BUILDER(int64_t);
_SET_SIZE_REGISTER_KERNEL_BUILDER(uint8);
_SET_SIZE_REGISTER_KERNEL_BUILDER(uint16);
_SET_SIZE_REGISTER_KERNEL_BUILDER(tstring);


enum InputTypes {
  DENSE_DENSE = 0, DENSE_SPARSE = 1, SPARSE_SPARSE = 2, };



enum SetOperation { A_MINUS_B = 0, B_MINUS_A = 1, INTERSECTION = 2, UNION = 3 };

SetOperation SetOperationFromContext(OpKernelConstruction* ctx) {
  string set_operation_str;
  if (!ctx->GetAttr("set_operation", &set_operation_str).ok()) {
    ctx->CtxFailure(errors::InvalidArgument("Missing set_operation."));
  } else {
    std::transform(set_operation_str.begin(), set_operation_str.end(), set_operation_str.begin(), ::tolower);
    if ("a-b" == set_operation_str) {
      return A_MINUS_B;
    }
    if ("b-a" == set_operation_str) {
      return B_MINUS_A;
    }
    if ("intersection" == set_operation_str) {
      return INTERSECTION;
    }
    if ("union" != set_operation_str) {
      ctx->CtxFailure(errors::InvalidArgument("Invalid set_operation ", set_operation_str, "."));
    }
  }
  
  
  return UNION;
}



template <typename T> class SetOperationOp : public OpKernel {
 public:
  SetOperationOp(OpKernelConstruction* ctx, InputTypes input_types)
      : OpKernel(ctx), set_operation_(SetOperationFromContext(ctx)), validate_indices_(ValidateIndicesFromContext(ctx)), input_types_(input_types) {}



  void Compute(OpKernelContext* ctx) override;

 private:
  void ApplySetOperation(const absl::flat_hash_set<T>& set1, const absl::flat_hash_set<T>& set2, absl::btree_set<T>* result) const;

  void ComputeDenseToDense(OpKernelContext* ctx) const;
  void ComputeDenseToSparse(OpKernelContext* ctx) const;
  void ComputeSparseToSparse(OpKernelContext* ctx) const;
  const SetOperation set_operation_;
  const bool validate_indices_;
  const InputTypes input_types_;
};

template <typename T> void SetDifference(const absl::flat_hash_set<T>& set1, const absl::flat_hash_set<T>& set2, absl::btree_set<T>* result) {


  for (const T& elem : set1) {
    if (!set2.contains(elem)) result->insert(elem);
  }
}

template <typename T> void SetIntersection(const absl::flat_hash_set<T>& set1, const absl::flat_hash_set<T>& set2, absl::btree_set<T>* result) {


  if (set1.size() <= set2.size()) {
    for (const T& elem : set1) {
      if (set2.contains(elem)) result->insert(elem);
    }
  } else {
    for (const T& elem : set2) {
      if (set1.contains(elem)) result->insert(elem);
    }
  }
}

template <typename T> void SetUnion(const absl::flat_hash_set<T>& set1, const absl::flat_hash_set<T>& set2, absl::btree_set<T>* result) {

  result->insert(set1.begin(), set1.end());
  result->insert(set2.begin(), set2.end());
}

template <typename T> void SetOperationOp<T>::ApplySetOperation(const absl::flat_hash_set<T>& set1, const absl::flat_hash_set<T>& set2, absl::btree_set<T>* result) const {


  switch (set_operation_) {
    case A_MINUS_B:
      SetDifference<T>(set1, set2, result);
      break;
    case B_MINUS_A:
      SetDifference<T>(set2, set1, result);
      break;
    case INTERSECTION:
      SetIntersection<T>(set1, set2, result);
      break;
    case UNION:
      SetUnion<T>(set1, set2, result);
      break;
  }
}


Status CheckShapesMatch(VarDimArray shape1, VarDimArray shape2) {
  if (shape1 != shape2) {
    return errors::InvalidArgument("Mismatched shapes [", absl::StrJoin(shape1, ","), "] vs [", absl::StrJoin(shape2, ","), "]");

  }
  return OkStatus();
}



Status GroupShapeFromInputs(VarDimArray shape1, VarDimArray shape2, ShapeArray* group_shape) {
  ShapeArray group_shape_1;
  TF_RETURN_IF_ERROR(GroupShape(shape1, &group_shape_1));
  ShapeArray group_shape_2;
  TF_RETURN_IF_ERROR(GroupShape(shape2, &group_shape_2));
  TF_RETURN_IF_ERROR(CheckShapesMatch(group_shape_1, group_shape_2));
  *group_shape = group_shape_1;
  return OkStatus();
}


void PopulateGroupIndices(const int64_t flat_group_index, VarDimArray group_shape, std::vector<int64_t>* group_indices) {

  group_indices->clear();
  int64_t running_flat_group_index = flat_group_index;
  for (int group_dim_index = group_shape.size() - 1; group_dim_index >= 0;
       --group_dim_index) {
    const auto group_dim = group_shape[group_dim_index];
    group_indices->insert(group_indices->begin(), running_flat_group_index % group_dim);
    running_flat_group_index /= group_dim;
  }
}

ShapeArray TensorShapeToArray(const TensorShape& t) {
  ShapeArray vec(t.dims());
  for (int i = 0; i < t.dims(); ++i) vec[i] = t.dim_size(i);
  return vec;
}





template <typename T> void SetOperationOp<T>::ComputeDenseToDense(OpKernelContext* ctx) const {
  const Tensor& set1_t = ctx->input(0);
  const Tensor& set2_t = ctx->input(1);
  
  
  
  ShapeArray group_shape;
  const auto shape1 = TensorShapeToArray(set1_t.shape());
  const auto shape2 = TensorShapeToArray(set2_t.shape());
  OP_REQUIRES_OK(ctx, GroupShapeFromInputs(shape1, shape2, &group_shape));

  const auto set1_strides = Strides(shape1);
  const auto set2_strides = Strides(shape2);

  std::vector<std::pair<std::vector<int64_t>, absl::btree_set<T>>> group_sets;
  int64_t num_result_values = 0;
  int64_t max_set_size = 0;

  absl::flat_hash_set<T> set1_group_set;
  absl::flat_hash_set<T> set2_group_set;
  std::vector<int64_t> group_indices;
  int64_t num_elements;
  OP_REQUIRES_OK(ctx, TensorShapeUtils::NumElements(group_shape, &num_elements));
  for (int64_t flat_group_index = 0; flat_group_index < num_elements;
       ++flat_group_index) {
    PopulateGroupIndices(flat_group_index, group_shape, &group_indices);
    PopulateFromDenseGroup<T>(ctx, set1_t, set1_strides, group_indices, &set1_group_set);
    PopulateFromDenseGroup<T>(ctx, set2_t, set2_strides, group_indices, &set2_group_set);

    absl::btree_set<T> group_set;
    ApplySetOperation(set1_group_set, set2_group_set, &group_set);
    if (!group_set.empty()) {
      const auto set_size = group_set.size();
      if (set_size > max_set_size) {
        max_set_size = set_size;
      }
      num_result_values += set_size;
      group_sets.push_back({group_indices, std::move(group_set)});
    }
  }

  TensorShape output_shape;
  OP_REQUIRES_OK(ctx, TensorShapeUtils::MakeShape(group_shape, &output_shape));
  output_shape.AddDim(max_set_size);
  OutputSparseTensor<T>(ctx, output_shape, num_result_values, group_sets);
}





template <typename T> void SetOperationOp<T>::ComputeDenseToSparse(OpKernelContext* ctx) const {
  const Tensor& set1_t = ctx->input(0);
  sparse::SparseTensor set2_st;
  OP_REQUIRES_OK(ctx, SparseTensorFromContext(ctx, 1, validate_indices_, &set2_st));
  
  
  
  ShapeArray group_shape;
  OP_REQUIRES_OK(ctx, GroupShapeFromInputs(TensorShapeToArray(set1_t.shape()), set2_st.shape(), &group_shape));

  const ShapeArray set1_strides = Strides(TensorShapeToArray(set1_t.shape()));

  std::vector<std::pair<std::vector<int64_t>, absl::btree_set<T>>> group_sets;
  int64_t num_result_values = 0;
  int64_t max_set_size = 0;

  absl::flat_hash_set<T> set1_group_set;
  absl::flat_hash_set<T> set2_group_set;
  auto set2_grouper = set2_st.group(set2_st.order().subspan(0, set2_st.order().size() - 1));
  auto set2_group_it = set2_grouper.begin();
  std::vector<int64_t> group_indices;
  int64_t num_elements;
  OP_REQUIRES_OK(ctx, TensorShapeUtils::NumElements(group_shape, &num_elements));
  for (int64_t flat_group_index = 0; flat_group_index < num_elements;
       ++flat_group_index) {
    PopulateGroupIndices(flat_group_index, group_shape, &group_indices);

    
    PopulateFromDenseGroup<T>(ctx, set1_t, set1_strides, group_indices, &set1_group_set);

    
    set2_group_set.clear();
    if (set2_group_it != set2_grouper.end()) {
      const auto& group = *set2_group_it;
      const auto set2_group_indices = group.group();
      OP_REQUIRES( ctx, set2_group_indices.size() == group_indices.size(), errors::InvalidArgument("Invalid number of group indices ", set2_group_indices.size(), ", expected ", group_indices.size(), "."));



      bool group_match = true;
      for (int32_t i = 0; group_match && (i < set2_group_indices.size()); ++i) {
        if (set2_group_indices[i] != group_indices[i]) {
          group_match = false;
        }
      }
      if (group_match) {
        PopulateFromSparseGroup<T>(ctx, group, set2_st.shape(), &set2_group_set);
        ++set2_group_it;
      }
    }

    absl::btree_set<T> group_set;
    ApplySetOperation(set1_group_set, set2_group_set, &group_set);
    if (!group_set.empty()) {
      const auto set_size = group_set.size();
      if (set_size > max_set_size) {
        max_set_size = set_size;
      }
      num_result_values += set_size;
      group_sets.push_back({group_indices, std::move(group_set)});
    }
  }

  TensorShape output_shape;
  OP_REQUIRES_OK(ctx, TensorShapeUtils::MakeShape(group_shape, &output_shape));
  output_shape.AddDim(max_set_size);
  OutputSparseTensor<T>(ctx, output_shape, num_result_values, group_sets);
}








void CompareGroups(OpKernelContext* ctx, const std::vector<int64_t>& set1_group_indices, const std::vector<int64_t>& set2_group_indices, int64_t* result) {


  if (set1_group_indices.empty()) {
    *result = set2_group_indices.empty() ? 0 : 1;
    return;
  }
  if (set2_group_indices.empty()) {
    *result = set1_group_indices.empty() ? 0 : -1;
    return;
  }
  OP_REQUIRES(ctx, set1_group_indices.size() == set2_group_indices.size(), errors::InvalidArgument("Mismatched group dims ", set1_group_indices.size(), " vs ", set2_group_indices.size(), "."));


  for (int32_t i = 0; i < set1_group_indices.size(); ++i) {
    *result = set1_group_indices[i] - set2_group_indices[i];
    if (*result != 0) {
      return;
    }
  }
}





template <typename T> void SetOperationOp<T>::ComputeSparseToSparse(OpKernelContext* ctx) const {
  sparse::SparseTensor set1_st;
  OP_REQUIRES_OK(ctx, SparseTensorFromContext(ctx, 0, validate_indices_, &set1_st));

  sparse::SparseTensor set2_st;
  OP_REQUIRES_OK(ctx, SparseTensorFromContext(ctx, 3, validate_indices_, &set2_st));

  
  
  
  ShapeArray group_shape;
  OP_REQUIRES_OK(ctx, GroupShapeFromInputs(set1_st.shape(), set2_st.shape(), &group_shape));

  std::vector<std::pair<std::vector<int64_t>, absl::btree_set<T>>> group_sets;
  int64_t num_result_values = 0;
  int64_t max_set_size = 0;

  absl::flat_hash_set<T> set1_group_set;
  absl::flat_hash_set<T> set2_group_set;
  auto set1_grouper = set1_st.group(set1_st.order().subspan(0, set1_st.order().size() - 1));
  auto set1_group_it = set1_grouper.begin();
  auto set2_grouper = set2_st.group(set2_st.order().subspan(0, set2_st.order().size() - 1));
  auto set2_group_it = set2_grouper.begin();

  
  const std::vector<int64_t> group_iter_end;
  
  
  while ((set1_group_it != set1_grouper.end()) || (set2_group_it != set2_grouper.end())) {
    const std::vector<int64_t>& set1_group_indices = (set1_group_it == set1_grouper.end()) ? group_iter_end : (*set1_group_it).group();

    const std::vector<int64_t>& set2_group_indices = (set2_group_it == set2_grouper.end()) ? group_iter_end : (*set2_group_it).group();


    int64_t compare_groups;
    CompareGroups(ctx, set1_group_indices, set2_group_indices, &compare_groups);
    const std::vector<int64_t>* group_indices = nullptr;

    
    set1_group_set.clear();
    if (compare_groups <= 0) {
      PopulateFromSparseGroup<T>(ctx, *set1_group_it, set1_st.shape(), &set1_group_set);
      ++set1_group_it;
      group_indices = &set1_group_indices;
    }

    
    set2_group_set.clear();
    if (compare_groups >= 0) {
      PopulateFromSparseGroup<T>(ctx, *set2_group_it, set2_st.shape(), &set2_group_set);
      ++set2_group_it;
      group_indices = &set2_group_indices;
    }

    absl::btree_set<T> group_set;
    ApplySetOperation(set1_group_set, set2_group_set, &group_set);
    if (!group_set.empty()) {
      const auto set_size = group_set.size();
      if (set_size > max_set_size) {
        max_set_size = set_size;
      }
      num_result_values += set_size;
      group_sets.push_back({*group_indices, std::move(group_set)});
    }
  }

  TensorShape output_shape;
  OP_REQUIRES_OK(ctx, TensorShapeUtils::MakeShape(group_shape, &output_shape));
  output_shape.AddDim(max_set_size);
  OutputSparseTensor<T>(ctx, output_shape, num_result_values, group_sets);
}





template <typename T> void SetOperationOp<T>::Compute(OpKernelContext* ctx) {
  switch (input_types_) {
    case DENSE_DENSE:
      ComputeDenseToDense(ctx);
      break;
    case DENSE_SPARSE:
      ComputeDenseToSparse(ctx);
      break;
    case SPARSE_SPARSE:
      ComputeSparseToSparse(ctx);
      break;
  }
}

template <typename T> class DenseToDenseSetOperationOp : public SetOperationOp<T> {
 public:
  explicit DenseToDenseSetOperationOp(OpKernelConstruction* ctx)
      : SetOperationOp<T>(ctx, DENSE_DENSE) {}
};





_DENSE_TO_DENSE_SET_OPERATION_REGISTER_KERNEL_BUILDER(int8);
_DENSE_TO_DENSE_SET_OPERATION_REGISTER_KERNEL_BUILDER(int16);
_DENSE_TO_DENSE_SET_OPERATION_REGISTER_KERNEL_BUILDER(int32);
_DENSE_TO_DENSE_SET_OPERATION_REGISTER_KERNEL_BUILDER(int64_t);
_DENSE_TO_DENSE_SET_OPERATION_REGISTER_KERNEL_BUILDER(uint8);
_DENSE_TO_DENSE_SET_OPERATION_REGISTER_KERNEL_BUILDER(uint16);
_DENSE_TO_DENSE_SET_OPERATION_REGISTER_KERNEL_BUILDER(tstring);


template <typename T> class DenseToSparseSetOperationOp : public SetOperationOp<T> {
 public:
  explicit DenseToSparseSetOperationOp(OpKernelConstruction* ctx)
      : SetOperationOp<T>(ctx, DENSE_SPARSE) {}
};





_DENSE_TO_SPARSE_SET_OPERATION_REGISTER_KERNEL_BUILDER(int8);
_DENSE_TO_SPARSE_SET_OPERATION_REGISTER_KERNEL_BUILDER(int16);
_DENSE_TO_SPARSE_SET_OPERATION_REGISTER_KERNEL_BUILDER(int32);
_DENSE_TO_SPARSE_SET_OPERATION_REGISTER_KERNEL_BUILDER(int64_t);
_DENSE_TO_SPARSE_SET_OPERATION_REGISTER_KERNEL_BUILDER(uint8);
_DENSE_TO_SPARSE_SET_OPERATION_REGISTER_KERNEL_BUILDER(uint16);
_DENSE_TO_SPARSE_SET_OPERATION_REGISTER_KERNEL_BUILDER(tstring);


template <typename T> class SparseToSparseSetOperationOp : public SetOperationOp<T> {
 public:
  explicit SparseToSparseSetOperationOp(OpKernelConstruction* ctx)
      : SetOperationOp<T>(ctx, SPARSE_SPARSE) {}
};





_SPARSE_TO_SPARSE_SET_OPERATION_REGISTER_KERNEL_BUILDER(int8);
_SPARSE_TO_SPARSE_SET_OPERATION_REGISTER_KERNEL_BUILDER(int16);
_SPARSE_TO_SPARSE_SET_OPERATION_REGISTER_KERNEL_BUILDER(int32);
_SPARSE_TO_SPARSE_SET_OPERATION_REGISTER_KERNEL_BUILDER(int64_t);
_SPARSE_TO_SPARSE_SET_OPERATION_REGISTER_KERNEL_BUILDER(uint8);
_SPARSE_TO_SPARSE_SET_OPERATION_REGISTER_KERNEL_BUILDER(uint16);
_SPARSE_TO_SPARSE_SET_OPERATION_REGISTER_KERNEL_BUILDER(tstring);


}  





















namespace tensorflow {
namespace {


struct KeyTensorLess {
  bool operator()(const Tensor& lhs, const Tensor& rhs) const {
    return std::less<int64>{}(lhs.scalar<int64>()(), rhs.scalar<int64>()());
  }
};


struct KeyTensorEqual {
  bool operator()(const Tensor& lhs, const Tensor& rhs) const {
    return std::equal_to<int64>{}(lhs.scalar<int64>()(), rhs.scalar<int64>()());
  }
};


struct KeyTensorHash {
  std::size_t operator()(const Tensor& key) const {
    return std::hash<int64>{}(key.scalar<int64>()());
  }
};


template <bool Ordered, typename Data> struct MapTraits;


template <typename Data> struct MapTraits<true, Data> {
  using KeyType = Tensor;
  using DataType = Data;
  using MapType = std::map<KeyType, Data, KeyTensorLess>;
};


template <typename Data> struct MapTraits<false, Data> {
  using KeyType = Tensor;
  using DataType = Data;
  using MapType = std::unordered_map<KeyType, Data, KeyTensorHash, KeyTensorEqual>;
};


template <bool Ordered> class StagingMap : public ResourceBase {
 public:
  
  using Tuple = std::vector<Tensor>;
  using OptionalTensor = gtl::optional<Tensor>;
  using OptionalTuple = std::vector<OptionalTensor>;

  using MapType = typename MapTraits<Ordered, OptionalTuple>::MapType;
  using KeyType = typename MapTraits<Ordered, OptionalTuple>::KeyType;

  using IncompleteType = typename MapTraits<false, OptionalTuple>::MapType;

 private:
  
  DataTypeVector dtypes_ TF_GUARDED_BY(mu_);
  std::size_t capacity_ TF_GUARDED_BY(mu_);
  std::size_t memory_limit_ TF_GUARDED_BY(mu_);
  std::size_t current_bytes_ TF_GUARDED_BY(mu_);
  tensorflow::mutex mu_;
  tensorflow::condition_variable not_empty_;
  tensorflow::condition_variable full_;
  IncompleteType incomplete_ TF_GUARDED_BY(mu_);
  MapType map_ TF_GUARDED_BY(mu_);

 private:
  

  
  
  void notify_inserters_if_bounded() TF_EXCLUSIVE_LOCKS_REQUIRED(mu_) {
    if (has_capacity() || has_memory_limit()) {
      
      
      
      full_.notify_all();
    }
  }

  
  
  void notify_removers() {
    
    
    
    not_empty_.notify_all();
  }

  bool has_capacity() const TF_EXCLUSIVE_LOCKS_REQUIRED(mu_) {
    return capacity_ > 0;
  }

  bool has_memory_limit() const TF_EXCLUSIVE_LOCKS_REQUIRED(mu_) {
    return memory_limit_ > 0;
  }

  bool would_exceed_memory_limit(std::size_t bytes) const TF_EXCLUSIVE_LOCKS_REQUIRED(mu_) {
    return has_memory_limit() && bytes + current_bytes_ > memory_limit_;
  }

  bool is_capacity_full() const TF_EXCLUSIVE_LOCKS_REQUIRED(mu_) {
    return has_capacity() && map_.size() >= capacity_;
  }

  
  std::size_t get_tuple_bytes(const Tuple& tuple) {
    return std::accumulate(tuple.begin(), tuple.end(), static_cast<std::size_t>(0), [](const std::size_t& lhs, const Tensor& rhs) {

                             return lhs + rhs.TotalBytes();
                           });
  }

  
  std::size_t get_tuple_bytes(const OptionalTuple& tuple) {
    return std::accumulate( tuple.begin(), tuple.end(), static_cast<std::size_t>(0), [](const std::size_t& lhs, const OptionalTensor& rhs) {

          return (lhs + rhs.has_value()) ? rhs.value().TotalBytes() : 0;
        });
  }

  
  Status check_index(const Tensor& key, std::size_t index)
      TF_EXCLUSIVE_LOCKS_REQUIRED(mu_) {
    if (index >= dtypes_.size()) {
      return Status(errors::InvalidArgument( "Index '", index, "' for key '", key.scalar<int64>()(), "' was out of bounds '", dtypes_.size(), "'."));

    }

    return Status::OK();
  }

  Status copy_or_move_tensors(OptionalTuple* map_tuple, const Tensor& key, const Tensor& indices, Tuple* output, bool copy = false)

      TF_EXCLUSIVE_LOCKS_REQUIRED(mu_) {
    auto findices = indices.flat<int>();

    
    for (std::size_t i = 0; i < findices.dimension(0); ++i) {
      std::size_t index = findices(i);

      TF_RETURN_IF_ERROR(check_index(key, index));

      
      if (!(*map_tuple)[index].has_value()) {
        return Status(errors::InvalidArgument( "Tensor at index '", index, "' for key '", key.scalar<int64>()(), "' has already been removed."));

      }

      
      
      output->push_back((*map_tuple)[index].value());

      
      if (!copy) {
        (*map_tuple)[index].reset();
      }
    }

    return Status::OK();
  }

  
  
  Status check_index_uninitialized(const Tensor& key, std::size_t index, const OptionalTuple& tuple)
      TF_EXCLUSIVE_LOCKS_REQUIRED(mu_) {
    if (tuple[index].has_value()) {
      return Status(errors::InvalidArgument( "The tensor for index '", index, "' for key '", key.scalar<int64>()(), "' was already initialized '", dtypes_.size(), "'."));

    }

    return Status::OK();
  }

  
  Status check_index_ordering(const Tensor& indices) {
    auto findices = indices.flat<int>();

    for (std::size_t i = 0; i < findices.dimension(0) - 1; ++i) {
      if (findices(i) < findices(i + 1)) {
        continue;
      }

      return Status( errors::InvalidArgument("Indices are not strictly ordered"));
    }

    return Status::OK();
  }

  
  Status check_memory_limit(std::size_t bytes)
      TF_EXCLUSIVE_LOCKS_REQUIRED(mu_) {
    if (has_memory_limit() && bytes > memory_limit_) {
      return Status(errors::ResourceExhausted( "Attempted to insert tensors with combined size of '", bytes, "' bytes into Staging Area with a memory limit of '", memory_limit_, "'."));


    }

    return Status::OK();
  }

  
  Status put_incomplete(const KeyType& key, const Tensor& indices, OptionalTuple* tuple, tensorflow::mutex_lock* lock)
      TF_EXCLUSIVE_LOCKS_REQUIRED(mu_) {
    auto findices = indices.flat<int>();

    
    auto it = incomplete_.find(key);

    
    std::size_t tuple_bytes = get_tuple_bytes(*tuple);
    TF_RETURN_IF_ERROR(check_memory_limit(tuple_bytes));

    
    while (would_exceed_memory_limit(tuple_bytes)) {
      full_.wait(*lock);
    }

    
    
    if (it == incomplete_.end()) {
      OptionalTuple empty(dtypes_.size());

      
      for (std::size_t i = 0; i < findices.dimension(0); ++i) {
        std::size_t index = findices(i);
        TF_RETURN_IF_ERROR(check_index(key, index));

        
        empty[index] = std::move((*tuple)[i]);
      }

      
      incomplete_.insert({key, std::move(empty)});

      
      current_bytes_ += tuple_bytes;
    }
    
    
    
    else {
      
      OptionalTuple& present = it->second;

      
      for (std::size_t i = 0; i < findices.dimension(0); ++i) {
        std::size_t index = findices(i);
        TF_RETURN_IF_ERROR(check_index(key, index));
        TF_RETURN_IF_ERROR(check_index_uninitialized(key, index, present));

        
        present[index] = std::move((*tuple)[i]);
      }

      
      current_bytes_ += tuple_bytes;

      
      bool complete = std::all_of(present.begin(), present.end(), [](const OptionalTensor& v) { return v.has_value(); });


      
      if (complete) {
        OptionalTuple insert_tuple = std::move(it->second);

        
        incomplete_.erase(it);

        TF_RETURN_IF_ERROR(put_complete(key, &insert_tuple));
      }
    }

    return Status::OK();
  }

  
  Status put_complete(const KeyType& key, OptionalTuple* tuple)
      TF_EXCLUSIVE_LOCKS_REQUIRED(mu_) {
    
    map_.insert({key, std::move(*tuple)});

    notify_removers();

    return Status::OK();
  }

 public:
  
  explicit StagingMap(const DataTypeVector& dtypes, std::size_t capacity, std::size_t memory_limit)
      : dtypes_(dtypes), capacity_(capacity), memory_limit_(memory_limit), current_bytes_(0) {}



  Status put(KeyType* key, const Tensor* indices, OptionalTuple* tuple) {
    tensorflow::mutex_lock lock(mu_);

    
    TF_RETURN_IF_ERROR(check_index_ordering(*indices));

    
    if (indices->NumElements() != dtypes_.size()) {
      return put_incomplete(*key, *indices, tuple, &lock);
    }

    std::size_t tuple_bytes = get_tuple_bytes(*tuple);
    
    TF_RETURN_IF_ERROR(check_memory_limit(tuple_bytes));

    
    while (would_exceed_memory_limit(tuple_bytes) || is_capacity_full()) {
      full_.wait(lock);
    }

    
    TF_RETURN_IF_ERROR(put_complete(*key, tuple));

    
    current_bytes_ += tuple_bytes;

    return Status::OK();
  }

  Status get(const KeyType* key, const Tensor* indices, Tuple* tuple) {
    tensorflow::mutex_lock lock(mu_);

    
    TF_RETURN_IF_ERROR(check_index_ordering(*indices));

    typename MapType::iterator it;

    
    while ((it = map_.find(*key)) == map_.end()) {
      not_empty_.wait(lock);
    }

    TF_RETURN_IF_ERROR( copy_or_move_tensors(&it->second, *key, *indices, tuple, true));

    
    current_bytes_ -= get_tuple_bytes(*tuple);

    return Status::OK();
  }

  Status pop(const KeyType* key, const Tensor* indices, Tuple* tuple) {
    tensorflow::mutex_lock lock(mu_);

    
    TF_RETURN_IF_ERROR(check_index_ordering(*indices));

    typename MapType::iterator it;

    
    while ((it = map_.find(*key)) == map_.end()) {
      not_empty_.wait(lock);
    }

    TF_RETURN_IF_ERROR( copy_or_move_tensors(&it->second, *key, *indices, tuple));

    
    if (!std::any_of( it->second.begin(), it->second.end(), [](const OptionalTensor& tensor) { return tensor.has_value(); })) {

      map_.erase(it);
    }

    
    current_bytes_ -= get_tuple_bytes(*tuple);

    notify_inserters_if_bounded();

    return Status::OK();
  }

  Status popitem(KeyType* key, const Tensor* indices, Tuple* tuple) {
    tensorflow::mutex_lock lock(mu_);

    
    TF_RETURN_IF_ERROR(check_index_ordering(*indices));

    
    while (this->map_.empty()) {
      not_empty_.wait(lock);
    }

    

    auto it = map_.begin();

    TF_RETURN_IF_ERROR( copy_or_move_tensors(&it->second, *key, *indices, tuple));

    *key = it->first;

    
    if (!std::any_of( it->second.begin(), it->second.end(), [](const OptionalTensor& tensor) { return tensor.has_value(); })) {

      map_.erase(it);
    }

    
    current_bytes_ -= get_tuple_bytes(*tuple);

    notify_inserters_if_bounded();

    return Status::OK();
  }

  Status clear() {
    tensorflow::mutex_lock lock(mu_);
    map_.clear();
    incomplete_.clear();
    current_bytes_ = 0;

    notify_inserters_if_bounded();

    return Status::OK();
  }

  std::size_t incomplete_size() {
    tensorflow::mutex_lock lock(mu_);
    return incomplete_.size();
  }

  std::size_t size() {
    tensorflow::mutex_lock lock(mu_);
    return map_.size();
  }

  string DebugString() const override { return "StagingMap"; }
};

template <bool Ordered> Status GetStagingMap(OpKernelContext* ctx, const NodeDef& ndef, StagingMap<Ordered>** map) {

  auto rm = ctx->resource_manager();
  ContainerInfo cinfo;

  
  auto create_fn = [&ndef](StagingMap<Ordered>** ret) -> Status {
    DataTypeVector dtypes;
    int64_t capacity;
    int64_t memory_limit;
    TF_RETURN_IF_ERROR(GetNodeAttr(ndef, "dtypes", &dtypes));
    TF_RETURN_IF_ERROR(GetNodeAttr(ndef, "capacity", &capacity));
    TF_RETURN_IF_ERROR(GetNodeAttr(ndef, "memory_limit", &memory_limit));
    *ret = new StagingMap<Ordered>(dtypes, capacity, memory_limit);
    return Status::OK();
  };

  TF_RETURN_IF_ERROR(cinfo.Init(rm, ndef, true ));
  TF_RETURN_IF_ERROR(rm->LookupOrCreate<StagingMap<Ordered>>( cinfo.container(), cinfo.name(), map, create_fn));
  return Status::OK();
}

template <bool Ordered> class MapStageOp : public OpKernel {
 public:
  explicit MapStageOp(OpKernelConstruction* ctx) : OpKernel(ctx) {}

  void Compute(OpKernelContext* ctx) override {
    StagingMap<Ordered>* map = nullptr;
    OP_REQUIRES_OK(ctx, GetStagingMap(ctx, def(), &map));
    core::ScopedUnref scope(map);
    typename StagingMap<Ordered>::OptionalTuple tuple;

    const Tensor* key_tensor;
    const Tensor* indices_tensor;
    OpInputList values_tensor;

    OP_REQUIRES_OK(ctx, ctx->input("key", &key_tensor));
    OP_REQUIRES_OK(ctx, ctx->input("indices", &indices_tensor));
    OP_REQUIRES_OK(ctx, ctx->input_list("values", &values_tensor));

    
    Tensor key(*key_tensor);

    
    for (std::size_t i = 0; i < values_tensor.size(); ++i) {
      tuple.push_back(values_tensor[i]);
    }

    
    OP_REQUIRES_OK(ctx, map->put(&key, indices_tensor, &tuple));
  }
};

REGISTER_KERNEL_BUILDER(Name("MapStage").Device(DEVICE_CPU), MapStageOp<false>);
REGISTER_KERNEL_BUILDER(Name("OrderedMapStage").Device(DEVICE_CPU), MapStageOp<true>);


REGISTER_KERNEL_BUILDER( Name("MapStage").HostMemory("key").HostMemory("indices").Device(DEVICE_GPU), MapStageOp<false>);

REGISTER_KERNEL_BUILDER(Name("OrderedMapStage")
                            .HostMemory("key")
                            .HostMemory("indices")
                            .Device(DEVICE_GPU), MapStageOp<true>);



template <bool Ordered> class MapUnstageOp : public OpKernel {
 public:
  explicit MapUnstageOp(OpKernelConstruction* ctx) : OpKernel(ctx) {}

  
  
  void Compute(OpKernelContext* ctx) override {
    StagingMap<Ordered>* map = nullptr;
    OP_REQUIRES_OK(ctx, GetStagingMap(ctx, def(), &map));
    core::ScopedUnref scope(map);
    typename StagingMap<Ordered>::Tuple tuple;

    const Tensor* key_tensor;
    const Tensor* indices_tensor;

    OP_REQUIRES_OK(ctx, ctx->input("key", &key_tensor));
    OP_REQUIRES_OK(ctx, ctx->input("indices", &indices_tensor));
    OP_REQUIRES_OK(ctx, map->pop(key_tensor, indices_tensor, &tuple));

    OP_REQUIRES( ctx, tuple.size() == indices_tensor->NumElements(), errors::InvalidArgument("output/indices size mismatch: ", tuple.size(), " vs. ", indices_tensor->NumElements()));



    for (std::size_t i = 0; i < tuple.size(); ++i) {
      ctx->set_output(i, tuple[i]);
    }
  }
};

REGISTER_KERNEL_BUILDER(Name("MapUnstage").Device(DEVICE_CPU), MapUnstageOp<false>);
REGISTER_KERNEL_BUILDER(Name("OrderedMapUnstage").Device(DEVICE_CPU), MapUnstageOp<true>);


REGISTER_KERNEL_BUILDER(Name("MapUnstage")
                            .HostMemory("key")
                            .HostMemory("indices")
                            .Device(DEVICE_GPU), MapUnstageOp<false>);
REGISTER_KERNEL_BUILDER(Name("OrderedMapUnstage")
                            .HostMemory("key")
                            .HostMemory("indices")
                            .Device(DEVICE_GPU), MapUnstageOp<true>);


template <bool Ordered> class MapPeekOp : public OpKernel {
 public:
  explicit MapPeekOp(OpKernelConstruction* ctx) : OpKernel(ctx) {}

  
  
  void Compute(OpKernelContext* ctx) override {
    StagingMap<Ordered>* map = nullptr;
    OP_REQUIRES_OK(ctx, GetStagingMap(ctx, def(), &map));
    core::ScopedUnref scope(map);
    typename StagingMap<Ordered>::Tuple tuple;

    const Tensor* key_tensor;
    const Tensor* indices_tensor;

    OP_REQUIRES_OK(ctx, ctx->input("key", &key_tensor));
    OP_REQUIRES_OK(ctx, ctx->input("indices", &indices_tensor));
    OP_REQUIRES_OK(ctx, map->get(key_tensor, indices_tensor, &tuple));

    OP_REQUIRES( ctx, tuple.size() == indices_tensor->NumElements(), errors::InvalidArgument("output/indices size mismatch: ", tuple.size(), " vs. ", indices_tensor->NumElements()));



    for (std::size_t i = 0; i < tuple.size(); ++i) {
      ctx->set_output(i, tuple[i]);
    }
  }
};

REGISTER_KERNEL_BUILDER(Name("MapPeek").Device(DEVICE_CPU), MapPeekOp<false>);
REGISTER_KERNEL_BUILDER(Name("OrderedMapPeek").Device(DEVICE_CPU), MapPeekOp<true>);


REGISTER_KERNEL_BUILDER( Name("MapPeek").HostMemory("key").HostMemory("indices").Device(DEVICE_GPU), MapPeekOp<false>);

REGISTER_KERNEL_BUILDER(Name("OrderedMapPeek")
                            .HostMemory("key")
                            .HostMemory("indices")
                            .Device(DEVICE_GPU), MapPeekOp<true>);



template <bool Ordered> class MapUnstageNoKeyOp : public OpKernel {
 public:
  explicit MapUnstageNoKeyOp(OpKernelConstruction* ctx) : OpKernel(ctx) {}

  
  
  void Compute(OpKernelContext* ctx) override {
    StagingMap<Ordered>* map = nullptr;
    OP_REQUIRES_OK(ctx, GetStagingMap(ctx, def(), &map));
    core::ScopedUnref scope(map);

    
    typename StagingMap<Ordered>::KeyType key;
    typename StagingMap<Ordered>::Tuple tuple;

    const Tensor* indices_tensor;

    OP_REQUIRES_OK(ctx, ctx->input("indices", &indices_tensor));
    OP_REQUIRES_OK(ctx, map->popitem(&key, indices_tensor, &tuple));

    
    ctx->set_output(0, key);

    
    OP_REQUIRES( ctx, tuple.size() == indices_tensor->NumElements(), errors::InvalidArgument("output/indices size mismatch: ", tuple.size(), " vs. ", indices_tensor->NumElements()));



    for (std::size_t i = 0; i < tuple.size(); ++i) {
      ctx->set_output(i + 1, tuple[i]);
    }
  }
};

REGISTER_KERNEL_BUILDER(Name("MapUnstageNoKey").Device(DEVICE_CPU), MapUnstageNoKeyOp<false>);
REGISTER_KERNEL_BUILDER(Name("OrderedMapUnstageNoKey").Device(DEVICE_CPU), MapUnstageNoKeyOp<true>);


REGISTER_KERNEL_BUILDER(Name("MapUnstageNoKey")
                            .HostMemory("key")
                            .HostMemory("indices")
                            .Device(DEVICE_GPU), MapUnstageNoKeyOp<false>);
REGISTER_KERNEL_BUILDER(Name("OrderedMapUnstageNoKey")
                            .HostMemory("key")
                            .HostMemory("indices")
                            .Device(DEVICE_GPU), MapUnstageNoKeyOp<true>);



template <bool Ordered> class MapSizeOp : public OpKernel {
 public:
  explicit MapSizeOp(OpKernelConstruction* ctx) : OpKernel(ctx) {}

  void Compute(OpKernelContext* ctx) override {
    StagingMap<Ordered>* map = nullptr;
    OP_REQUIRES_OK(ctx, GetStagingMap(ctx, def(), &map));
    core::ScopedUnref scope(map);

    
    Tensor* size = nullptr;
    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, TensorShape({}), &size));

    
    size->scalar<int32>().setConstant(map->size());
  }
};

REGISTER_KERNEL_BUILDER(Name("MapSize").Device(DEVICE_CPU), MapSizeOp<false>);
REGISTER_KERNEL_BUILDER(Name("OrderedMapSize").Device(DEVICE_CPU), MapSizeOp<true>);


REGISTER_KERNEL_BUILDER(Name("MapSize").Device(DEVICE_GPU).HostMemory("size"), MapSizeOp<false>);
REGISTER_KERNEL_BUILDER( Name("OrderedMapSize").Device(DEVICE_GPU).HostMemory("size"), MapSizeOp<true>);



template <bool Ordered> class MapIncompleteSizeOp : public OpKernel {
 public:
  explicit MapIncompleteSizeOp(OpKernelConstruction* ctx) : OpKernel(ctx) {}

  void Compute(OpKernelContext* ctx) override {
    StagingMap<Ordered>* map = nullptr;
    OP_REQUIRES_OK(ctx, GetStagingMap(ctx, def(), &map));
    core::ScopedUnref scope(map);

    
    Tensor* size = nullptr;
    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, TensorShape({}), &size));

    
    size->scalar<int32>().setConstant(map->incomplete_size());
  }
};

REGISTER_KERNEL_BUILDER(Name("MapIncompleteSize").Device(DEVICE_CPU), MapIncompleteSizeOp<false>);
REGISTER_KERNEL_BUILDER(Name("OrderedMapIncompleteSize").Device(DEVICE_CPU), MapIncompleteSizeOp<true>);


REGISTER_KERNEL_BUILDER( Name("MapIncompleteSize").Device(DEVICE_GPU).HostMemory("size"), MapIncompleteSizeOp<false>);

REGISTER_KERNEL_BUILDER( Name("OrderedMapIncompleteSize").Device(DEVICE_GPU).HostMemory("size"), MapIncompleteSizeOp<true>);



template <bool Ordered> class MapClearOp : public OpKernel {
 public:
  explicit MapClearOp(OpKernelConstruction* ctx) : OpKernel(ctx) {}

  void Compute(OpKernelContext* ctx) override {
    StagingMap<Ordered>* map = nullptr;
    OP_REQUIRES_OK(ctx, GetStagingMap(ctx, def(), &map));
    core::ScopedUnref scope(map);

    OP_REQUIRES_OK(ctx, map->clear());
  }
};

REGISTER_KERNEL_BUILDER(Name("MapClear").Device(DEVICE_CPU), MapClearOp<false>);
REGISTER_KERNEL_BUILDER(Name("OrderedMapClear").Device(DEVICE_CPU), MapClearOp<true>);


REGISTER_KERNEL_BUILDER(Name("MapClear").Device(DEVICE_GPU), MapClearOp<false>);
REGISTER_KERNEL_BUILDER(Name("OrderedMapClear").Device(DEVICE_GPU), MapClearOp<true>);


}  
}  

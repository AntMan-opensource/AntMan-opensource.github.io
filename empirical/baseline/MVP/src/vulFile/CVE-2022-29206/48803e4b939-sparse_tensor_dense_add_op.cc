












namespace tensorflow {

typedef Eigen::ThreadPoolDevice CPUDevice;


namespace {

template <typename Index> Status ValidateInputs(const Tensor *a_indices, const Tensor *a_values, const Tensor *a_shape, const Tensor *b) {

  if (!TensorShapeUtils::IsMatrix(a_indices->shape())) {
    return errors::InvalidArgument( "Input a_indices should be a matrix but received shape: ", a_indices->shape().DebugString());

  }
  if (!TensorShapeUtils::IsVector(a_values->shape()) || !TensorShapeUtils::IsVector(a_shape->shape())) {
    return errors::InvalidArgument( "Inputs a_values and a_shape should be vectors " "but received shapes: ", a_values->shape().DebugString(), " and ", a_shape->shape().DebugString());



  }
  if (a_shape->NumElements() != b->dims()) {
    return errors::InvalidArgument( "Two operands have different ranks; received: ", a_shape->NumElements(), " and ", b->dims());

  }
  const auto a_shape_flat = a_shape->flat<Index>();
  for (int i = 0; i < b->dims(); ++i) {
    if (a_shape_flat(i) != b->dim_size(i)) {
      return errors::InvalidArgument( "Dimension ", i, " does not equal (no broadcasting is supported): sparse side ", a_shape_flat(i), " vs dense side ", b->dim_size(i));


    }
  }
  return Status::OK();
}

}  

template <typename Device, typename T, typename Index> class SparseTensorDenseAddOp : public OpKernel {
 public:
  explicit SparseTensorDenseAddOp(OpKernelConstruction *ctx) : OpKernel(ctx) {}

  void Compute(OpKernelContext *ctx) override {
    const Tensor *a_indices_t, *a_values_t, *a_shape_t, *b;
    OP_REQUIRES_OK(ctx, ctx->input("a_indices", &a_indices_t));
    OP_REQUIRES_OK(ctx, ctx->input("a_values", &a_values_t));
    OP_REQUIRES_OK(ctx, ctx->input("a_shape", &a_shape_t));
    OP_REQUIRES_OK(ctx, ctx->input("b", &b));
    OP_REQUIRES_OK( ctx, ValidateInputs<Index>(a_indices_t, a_values_t, a_shape_t, b));

    Tensor *out_t;
    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, b->shape(), &out_t));

    const int ndims = static_cast<int>(a_indices_t->dim_size(1));
    const auto a_indices_mat = a_indices_t->flat_inner_dims<Index>();
    const auto a_values_flat = a_values_t->flat<T>();

    switch (ndims) {















      NDIMS_CASE(1);
      NDIMS_CASE(2);
      NDIMS_CASE(3);
      NDIMS_CASE(4);
      NDIMS_CASE(5);
      default:
        OP_REQUIRES( ctx, false, errors::InvalidArgument("Only tensors with ranks between 1 and 5 " "are currently supported.  Tensor rank: ", ndims));




    }
  }
};

namespace functor {
template <typename T, typename Index, int NDIMS> struct ScatterNdFunctor<CPUDevice, T, Index, NDIMS, scatter_op::UpdateOp::ADD> {
  Index operator()(const CPUDevice &d, typename TTypes<Index>::ConstMatrix indices, typename TTypes<T>::ConstFlat updates, typename TTypes<T, NDIMS>::Tensor out) {


    Eigen::array<Eigen::DenseIndex, NDIMS> idx;
    const int num_nnz = static_cast<int>(indices.dimension(0));
    for (int i = 0; i < num_nnz; ++i) {
      for (int d = 0; d < NDIMS; ++d) {
        idx[d] = internal::SubtleMustCopy(indices(i, d));
        if (!FastBoundsCheck(idx[d], out.dimension(d))) {
          return d;  
        }
      }
      out(idx) += updates(i);
    }
    return -1;  
  }
};
}  










TF_CALL_NUMBER_TYPES(REGISTER_KERNELS);


}  

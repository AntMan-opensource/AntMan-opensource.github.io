











namespace tensorflow {

namespace {




template <typename VALUE_TYPE, typename SPLITS_TYPE> void WriteValueSlices( const Tensor& params_dense_values_in, const std::vector<std::pair<SPLITS_TYPE, SPLITS_TYPE>>& value_slices, SPLITS_TYPE value_size, Tensor* values_out) {



  const auto& params_dense_values = params_dense_values_in.flat_outer_dims<VALUE_TYPE, 2>();
  auto values = values_out->flat_outer_dims<VALUE_TYPE, 2>();
  int out_pos = 0;
  for (const auto& slice : value_slices) {
    for (int i = slice.first; i < slice.second; ++i) {
      for (int j = 0; j < value_size; ++j) {
        values(out_pos, j) = params_dense_values(i, j);
      }
      ++out_pos;
    }
  }
}

}  

template <typename INDEX_TYPE, typename SPLITS_TYPE> class RaggedGatherOpBase : public OpKernel {
 public:
  using OpKernel::OpKernel;

  void Compute(OpKernelContext* context) override {
    
    OpInputList params_nested_splits_in;
    OP_REQUIRES_OK(context, context->input_list("params_nested_splits", &params_nested_splits_in));
    const Tensor& params_dense_values_in = context->input(params_nested_splits_in.size());
    const Tensor& indices_in = context->input(params_nested_splits_in.size() + 1);

    DCHECK_GT(params_nested_splits_in.size(), 0);  
    SPLITS_TYPE num_params = params_nested_splits_in[0].dim_size(0) - 1;
    OP_REQUIRES_OK(context, ValidateIndices(indices_in, num_params));

    OP_REQUIRES(context, params_dense_values_in.dims() > 0, errors::InvalidArgument("params.rank must be nonzero"));
    SPLITS_TYPE num_params_dense_values = params_dense_values_in.dim_size(0);

    
    
    std::vector<std::pair<SPLITS_TYPE, SPLITS_TYPE>> value_slices;
    SPLITS_TYPE num_values = 0;
    std::vector<std::vector<SPLITS_TYPE>> out_splits;
    OP_REQUIRES_OK(context, MakeSplits(indices_in, params_nested_splits_in, num_params_dense_values, &out_splits, &value_slices, &num_values));


    
    OP_REQUIRES_OK(context, WriteSplits(out_splits, context));
    OP_REQUIRES_OK(context, WriteValues(params_dense_values_in, value_slices, out_splits.size(), num_values, context));

  }

 private:
  using ConstFlatType = typename TTypes<SPLITS_TYPE>::ConstFlat;

  
  ::tensorflow::Status ValidateIndices(const Tensor& indices_in, SPLITS_TYPE num_params) {
    const auto& indices = indices_in.flat<INDEX_TYPE>();
    for (SPLITS_TYPE i = 0; i < indices.size(); ++i) {
      SPLITS_TYPE index = indices(i);
      if (index < 0 || index >= num_params) {
        return errors::InvalidArgument( "indices", SliceDebugString(indices_in.shape(), i), " = ", index, " is not in [0, ", num_params, ")");

      }
    }
    return ::tensorflow::Status::OK();
  }

  
  
  
  
  ::tensorflow::Status MakeSplits( const Tensor& indices_in, const OpInputList& params_nested_splits_in, SPLITS_TYPE num_params_dense_values, std::vector<std::vector<SPLITS_TYPE>>* out_splits, std::vector<std::pair<SPLITS_TYPE, SPLITS_TYPE>>* value_slices, SPLITS_TYPE* num_values) {




    *num_values = 0;
    value_slices->clear();

    int num_splits = indices_in.dims() - 1 + params_nested_splits_in.size();
    out_splits->assign(num_splits, {0});

    
    const auto& indices = indices_in.flat<INDEX_TYPE>();
    std::vector<ConstFlatType> params_nested_splits;
    params_nested_splits.reserve(params_nested_splits_in.size());
    for (const auto& splits_in : params_nested_splits_in) {
      params_nested_splits.push_back(splits_in.flat<SPLITS_TYPE>());
    }

    TF_RETURN_IF_ERROR( ValidateSplits(params_nested_splits, num_params_dense_values));

    
    
    
    
    
    
    
    int nrows = 1;
    for (int dim = 0; dim < indices_in.dims() - 1; ++dim) {
      nrows *= indices_in.dim_size(dim);
      int row_length = indices_in.dim_size(dim + 1);
      for (int i = 1; i < nrows + 1; ++i) {
        out_splits->at(dim).push_back(i * row_length);
      }
    }

    
    
    
    
    
    
    
    
    
    for (int i = 0; i < indices.size(); ++i) {
      int start = indices(i);
      int limit = indices(i) + 1;

      
      for (int dim = 0; dim < params_nested_splits.size(); ++dim) {
        const auto& splits = params_nested_splits[dim];
        int out_dim = dim + indices_in.dims() - 1;
        if (out_dim >= 0) {
          SPLITS_TYPE delta = out_splits->at(out_dim).back() - splits(start);
          for (int j = start; j < limit; ++j) {
            out_splits->at(out_dim).push_back(splits(j + 1) + delta);
          }
        }
        start = splits(start);
        limit = splits(limit);
      }
      if (limit != start) {
        value_slices->emplace_back(start, limit);
        *num_values += limit - start;
      }
    }
    return ::tensorflow::Status::OK();
  }

  ::tensorflow::Status ValidateSplits( const std::vector<ConstFlatType>& params_nested_splits, SPLITS_TYPE num_params_dense_values) {

    
    for (int dim = 0; dim < params_nested_splits.size(); ++dim) {
      const auto& splits = params_nested_splits[dim];
      SPLITS_TYPE last_split = (dim == params_nested_splits.size() - 1)
                                   ? num_params_dense_values : params_nested_splits[dim + 1].size();
      if (splits.size() == 0) {
        return errors::InvalidArgument("Ragged splits may not be empty");
      }
      if (splits(0) < 0) {
        return errors::InvalidArgument("Ragged splits must be non-negative");
      }
      if (splits(splits.size() - 1) > last_split) {
        return errors::InvalidArgument( "Ragged splits must not point past values");
      }
      for (int i = 1; i < splits.size(); ++i) {
        if (splits(i - 1) > splits(i)) {
          return errors::InvalidArgument("Ragged splits must be sorted");
        }
      }
    }
    return ::tensorflow::Status::OK();
  }

  ::tensorflow::Status WriteSplits( const std::vector<std::vector<SPLITS_TYPE>>& out_splits, OpKernelContext* context) {

    OpOutputList splits_out;
    TF_RETURN_IF_ERROR( context->output_list("output_nested_splits", &splits_out));
    for (int i = 0; i < out_splits.size(); ++i) {
      Tensor* splits;
      SPLITS_TYPE num_splits = out_splits[i].size();
      TF_RETURN_IF_ERROR( splits_out.allocate(i, TensorShape({num_splits}), &splits));
      auto splits_flat = splits->flat<SPLITS_TYPE>();
      std::copy_n(out_splits[i].data(), out_splits[i].size(), splits_flat.data());
    }
    return ::tensorflow::Status::OK();
  }

  ::tensorflow::Status WriteValues( const Tensor& params_dense_values_in, const std::vector<std::pair<SPLITS_TYPE, SPLITS_TYPE>>& value_slices, int values_index, SPLITS_TYPE num_values, OpKernelContext* context) const {



    Tensor* values_out = nullptr;
    TensorShape values_shape = params_dense_values_in.shape();
    values_shape.set_dim(0, num_values);
    TF_RETURN_IF_ERROR( context->allocate_output(values_index, values_shape, &values_out));
    const SPLITS_TYPE num_elements = params_dense_values_in.NumElements();
    const SPLITS_TYPE value_size = num_elements == 0 ? 0 : (num_elements / params_dense_values_in.dim_size(0));

    CallWriteValueSlices(params_dense_values_in, value_slices, value_size, values_out);
    return ::tensorflow::Status::OK();
  }

 protected:
  
  
  
  
  
  virtual void CallWriteValueSlices( const Tensor& params_dense_values_in, const std::vector<std::pair<SPLITS_TYPE, SPLITS_TYPE>>& value_slices, SPLITS_TYPE value_size, Tensor* values_out) const = 0;


};

template <typename INDEX_TYPE, typename VALUE_TYPE, typename SPLITS_TYPE> class RaggedGatherOp : public RaggedGatherOpBase<INDEX_TYPE, SPLITS_TYPE> {
 public:
  using RaggedGatherOpBase<INDEX_TYPE, SPLITS_TYPE>::RaggedGatherOpBase;

 private:
  void CallWriteValueSlices( const Tensor& params_dense_values_in, const std::vector<std::pair<SPLITS_TYPE, SPLITS_TYPE>>& value_slices, SPLITS_TYPE value_size, Tensor* values_out) const override {


    WriteValueSlices<VALUE_TYPE>(params_dense_values_in, value_slices, value_size, values_out);
  }
};













TF_CALL_POD_TYPES(REGISTER_CPU_KERNEL);
TF_CALL_tstring(REGISTER_CPU_KERNEL);
TF_CALL_QUANTIZED_TYPES(REGISTER_CPU_KERNEL);
TF_CALL_quint16(REGISTER_CPU_KERNEL);
TF_CALL_qint16(REGISTER_CPU_KERNEL);



}  

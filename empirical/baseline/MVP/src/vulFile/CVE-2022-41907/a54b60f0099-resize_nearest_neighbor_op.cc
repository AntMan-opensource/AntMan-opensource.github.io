



















namespace tensorflow {

typedef Eigen::ThreadPoolDevice CPUDevice;
typedef Eigen::GpuDevice GPUDevice;

template <typename Device, typename T> class ResizeNearestNeighborOp : public OpKernel {
 public:
  explicit ResizeNearestNeighborOp(OpKernelConstruction* context)
      : OpKernel(context) {
    OP_REQUIRES_OK(context, context->GetAttr("align_corners", &align_corners_));
    OP_REQUIRES_OK( context, context->GetAttr("half_pixel_centers", &half_pixel_centers_));
  }

  void Compute(OpKernelContext* context) override {
    ImageResizerState st(align_corners_, half_pixel_centers_);
    st.ValidateAndCreateOutput(context);

    if (!context->status().ok()) return;

    OP_REQUIRES(context, st.in_height < (1 << 24) && st.in_width < (1 << 24), errors::InvalidArgument("nearest neighbor requires max height " "& width of 2^24"));


    
    if (st.output->NumElements() == 0) return;

    typename TTypes<T, 4>::ConstTensor input_data( context->input(0).tensor<T, 4>());
    typename TTypes<T, 4>::Tensor output_data(st.output->tensor<T, 4>());

    bool status;
    if (half_pixel_centers_) {
      if (align_corners_) {
        status = functor::ResizeNearestNeighbor<Device, T, true, true>()( context->eigen_device<Device>(), input_data, st.height_scale, st.width_scale, output_data);



      } else {
        status = functor::ResizeNearestNeighbor<Device, T, true, false>()( context->eigen_device<Device>(), input_data, st.height_scale, st.width_scale, output_data);



      }
    } else {
      if (align_corners_) {
        status = functor::ResizeNearestNeighbor<Device, T, false, true>()( context->eigen_device<Device>(), input_data, st.height_scale, st.width_scale, output_data);



      } else {
        status = functor::ResizeNearestNeighbor<Device, T, false, false>()( context->eigen_device<Device>(), input_data, st.height_scale, st.width_scale, output_data);



      }
    }
    if (!status) {
      context->SetStatus( errors::Internal("Failed launching ResizeNearestNeighbor"));
    }
  }

 private:
  bool align_corners_;
  bool half_pixel_centers_;
};


template <bool half_pixel_centers> struct BoolToScaler {};

struct HalfPixelScalerForNN {
  inline float operator()(const int x, const float scale) const {
    
    
    
    
    return (static_cast<float>(x) + 0.5f) * scale;
  }
};

template <> struct BoolToScaler<true> {
  typedef HalfPixelScalerForNN Scaler;
};

template <> struct BoolToScaler<false> {
  typedef LegacyScaler Scaler;
};


namespace functor {
template <typename T, bool half_pixel_centers, bool align_corners> struct ResizeNearestNeighbor<CPUDevice, T, half_pixel_centers, align_corners> {
  bool operator()(const CPUDevice& d, typename TTypes<T, 4>::ConstTensor input, const float height_scale, const float width_scale, typename TTypes<T, 4>::Tensor output) {

    typename BoolToScaler<half_pixel_centers>::Scaler scaler;
    const Eigen::Index batch_size = input.dimension(0);
    const Eigen::Index in_height = input.dimension(1);
    const Eigen::Index in_width = input.dimension(2);
    const Eigen::Index channels = input.dimension(3);
    const Eigen::Index out_height = output.dimension(1);
    const Eigen::Index out_width = output.dimension(2);


    
    
    
    for (Eigen::Index b = 0; b < batch_size; ++b) {
      for (Eigen::Index y = 0; y < out_height; ++y) {
        Eigen::Index in_y = std::min( (align_corners)
                ? static_cast<Eigen::Index>(roundf(scaler(y, height_scale)))
                : static_cast<Eigen::Index>(floorf(scaler(y, height_scale))), in_height - 1);
        if (half_pixel_centers) {
          in_y = std::max(static_cast<Eigen::Index>(0), in_y);
        }
        for (Eigen::Index x = 0; x < out_width; ++x) {
          Eigen::Index in_x = std::min( (align_corners)
                  ? static_cast<Eigen::Index>(roundf(scaler(x, width_scale)))
                  : static_cast<Eigen::Index>(floorf(scaler(x, width_scale))), in_width - 1);
          if (half_pixel_centers) {
            in_x = std::max(static_cast<Eigen::Index>(0), in_x);
          }
          std::copy_n(&input(b, in_y, in_x, 0), channels, &output(b, y, x, 0));
        }
      }
    }

    auto ParallelResize = [&](Eigen::Index start, Eigen::Index end) {
      for (Eigen::Index b = start; b < end; ++b) {
        Eigen::Index x = b % out_width;
        Eigen::Index y = (b / out_width) % out_height;
        Eigen::Index bs = (b / out_width) / out_height;
        Eigen::Index in_y = std::min( (align_corners)
                ? static_cast<Eigen::Index>(roundf(scaler(y, height_scale)))
                : static_cast<Eigen::Index>(floorf(scaler(y, height_scale))), in_height - 1);
        if (half_pixel_centers) {
          in_y = std::max(static_cast<Eigen::Index>(0), in_y);
        }
        Eigen::Index in_x = std::min( (align_corners)
                ? static_cast<Eigen::Index>(roundf(scaler(x, width_scale)))
                : static_cast<Eigen::Index>(floorf(scaler(x, width_scale))), in_width - 1);
        if (half_pixel_centers) {
          in_x = std::max(static_cast<Eigen::Index>(0), in_x);
        }
        std::copy_n(&input(bs, in_y, in_x, 0), channels, &output(bs, y, x, 0));
      }
    };
    Eigen::Index N = batch_size * out_height * out_width;
    const int input_bytes = channels * sizeof(T);
    const int output_bytes = channels * sizeof(T);
    const int compute_cycles = (Eigen::TensorOpCost::ModCost<T>() * 2 + Eigen::TensorOpCost::DivCost<T>() * 3 + Eigen::TensorOpCost::AddCost<T>() * 2 + Eigen::TensorOpCost::MulCost<T>() * 2);


    const Eigen::TensorOpCost cost(input_bytes, output_bytes, compute_cycles);
    d.parallelFor(N, cost, ParallelResize);

    return true;
  }
};
}  

template <typename Device, typename T> class ResizeNearestNeighborOpGrad : public OpKernel {
 public:
  explicit ResizeNearestNeighborOpGrad(OpKernelConstruction* context)
      : OpKernel(context) {
    OP_REQUIRES_OK(context, context->GetAttr("align_corners", &align_corners_));
    OP_REQUIRES_OK( context, context->GetAttr("half_pixel_centers", &half_pixel_centers_));
  }

  void Compute(OpKernelContext* context) override {
    
    const Tensor& input = context->input(0);
    OP_REQUIRES(context, input.dims() == 4, errors::InvalidArgument("input must be 4-dimensional", input.shape().DebugString()));


    
    const Tensor& shape_t = context->input(1);
    OP_REQUIRES(context, shape_t.dims() == 1, errors::InvalidArgument("shape_t must be 1-dimensional", shape_t.shape().DebugString()));

    OP_REQUIRES(context, shape_t.NumElements() == 2, errors::InvalidArgument("shape_t must have two elements", shape_t.shape().DebugString()));


    auto sizes = shape_t.vec<int32>();
    OP_REQUIRES(context, sizes(0) > 0 && sizes(1) > 0, errors::InvalidArgument("shape_t's elements must be positive"));

    if (std::is_same<Device, GPUDevice>::value) {
      OP_REQUIRES( context, !OpDeterminismRequired(), errors::Unimplemented( "A deterministic GPU implementation of ResizeNearestNeighborGrad" " is not currently available."));



    }

    const int64_t batch_size = input.dim_size(0);
    const int64_t in_height = input.dim_size(1);
    const int64_t in_width = input.dim_size(2);
    const int64_t channels = input.dim_size(3);

    const int64_t out_height = sizes(0);
    const int64_t out_width = sizes(1);

    Tensor* output = nullptr;
    OP_REQUIRES_OK( context, context->allocate_output( 0, TensorShape({batch_size, out_height, out_width, channels}), &output));




    
    if (output->NumElements() == 0) return;

    typename TTypes<T, 4>::ConstTensor input_data(input.tensor<T, 4>());
    typename TTypes<T, 4>::Tensor output_data(output->tensor<T, 4>());

    const float height_scale = CalculateResizeScale(out_height, in_height, align_corners_);
    const float width_scale = CalculateResizeScale(out_width, in_width, align_corners_);

    bool status;
    if (half_pixel_centers_) {
      if (align_corners_) {
        status = functor::ResizeNearestNeighborGrad<Device, T, true, true>()( context->eigen_device<Device>(), input_data, height_scale, width_scale, output_data);



      } else {
        status = functor::ResizeNearestNeighborGrad<Device, T, true, false>()( context->eigen_device<Device>(), input_data, height_scale, width_scale, output_data);



      }
    } else {
      if (align_corners_) {
        status = functor::ResizeNearestNeighborGrad<Device, T, false, true>()( context->eigen_device<Device>(), input_data, height_scale, width_scale, output_data);




      } else {
        status = functor::ResizeNearestNeighborGrad<Device, T, false, false>()( context->eigen_device<Device>(), input_data, height_scale, width_scale, output_data);




      }
    }
    if (!status) {
      context->SetStatus( errors::Internal("Failed launching ResizeNearestNeighborGrad"));
    }
  }

 private:
  bool align_corners_;
  bool half_pixel_centers_;
};


namespace functor {
template <typename T, bool half_pixel_centers, bool align_corners> struct ResizeNearestNeighborGrad<CPUDevice, T, half_pixel_centers, align_corners> {

  bool operator()(const CPUDevice& d, typename TTypes<T, 4>::ConstTensor input, const float height_scale, const float width_scale, typename TTypes<T, 4>::Tensor output) {

    typename BoolToScaler<half_pixel_centers>::Scaler scaler;
    const Eigen::Index batch_size = input.dimension(0);
    const Eigen::Index in_height = input.dimension(1);
    const Eigen::Index in_width = input.dimension(2);
    const Eigen::Index channels = input.dimension(3);

    const Eigen::Index out_height = output.dimension(1);
    const Eigen::Index out_width = output.dimension(2);

    output.setZero();

    for (Eigen::Index y = 0; y < in_height; ++y) {
      const Eigen::Index out_y = std::min( (align_corners)
              ? static_cast<Eigen::Index>(roundf(scaler(y, height_scale)))
              : static_cast<Eigen::Index>(floorf(scaler(y, height_scale))), out_height - 1);
      for (Eigen::Index x = 0; x < in_width; ++x) {
        const Eigen::Index out_x = std::min( (align_corners)
                ? static_cast<Eigen::Index>(roundf(scaler(x, width_scale)))
                : static_cast<Eigen::Index>(floorf(scaler(x, width_scale))), out_width - 1);
        for (Eigen::Index b = 0; b < batch_size; ++b) {
          for (Eigen::Index c = 0; c < channels; ++c) {
            output(b, out_y, out_x, c) += input(b, y, x, c);
          }
        }
      }
    }
    return true;
  }
};
}  












TF_CALL_REAL_NUMBER_TYPES(REGISTER_KERNEL);
















TF_CALL_GPU_NUMBER_TYPES(REGISTER_KERNEL);





}  





































namespace tensorflow {

static const char kErrMsg[] = "Singular Value Decomposition was not successful. The input might not be " "valid.";


typedef Eigen::GpuDevice GPUDevice;

namespace {




template <class Scalar> __global__ void ComputeValueOfVKernel(Gpu2DLaunchConfig config, int64 m, int64 ldu, const Scalar* __restrict__ M, const Scalar* __restrict__ U, const Scalar* __restrict__ S, Scalar* __restrict__ V) {




  GPU_AXIS_KERNEL_LOOP(batch, config.virtual_thread_count.x, X) {
    GPU_AXIS_KERNEL_LOOP(i, config.virtual_thread_count.y, Y) {
      Scalar v = M[i + m * batch] * U[ldu * (i + m * batch)] * S[batch];
      GpuAtomicAdd(V + batch, v);
    }
  }
}



template <class Scalar> __global__ void ExtractSignOfVKernel(GpuLaunchConfig config, Scalar* __restrict__ V) {

  GPU_1D_KERNEL_LOOP(i, config.virtual_thread_count) {
    V[i] = V[i] >= 0 ? Scalar(1) : Scalar(-1);
  }
}
}  


template <class Scalar> class SvdOpGpu : public AsyncOpKernel {
 public:
  using RealScalar = typename Eigen::NumTraits<Scalar>::Real;

  explicit SvdOpGpu(OpKernelConstruction* context) : AsyncOpKernel(context) {
    OP_REQUIRES_OK(context, context->GetAttr("compute_uv", &compute_uv_));
    OP_REQUIRES_OK(context, context->GetAttr("full_matrices", &full_matrices_));
    OP_REQUIRES(context, !tensorflow::OpDeterminismRequired(), errors::Unimplemented("Determinism is not yet supported " "for Svd."));

  }

  void RunSVD(OpKernelContext* context, DoneCallback done, int64 m, int64 n, int64 p, Tensor& M_copy, Tensor* S, Tensor* U, Tensor* V, std::unique_ptr<GpuSolver> solver) {

    
    
    
    
    
    

    
    Scalar* input_ptr;
    RealScalar* outputS_ptr;
    auto input_reshaped = M_copy.template flat_inner_dims<Scalar, 3>();
    input_ptr = input_reshaped.data();
    const int64 batch_size = M_copy.dims() > 2 ? input_reshaped.dimension(0) : 1;
    
    
    
    const bool batched = m <= 32 && n <= 32 && batch_size > 1 && (full_matrices_ || m == n);

    
    Tensor u_copy, v_copy;
    Scalar* outputU_ptr = NULL;
    Scalar* outputV_ptr = NULL;
    if (compute_uv_ || batched) {
      TensorShape u_shape, v_shape;
      if (batched) {
        
        
        TensorShape shapeRaw = M_copy.shape();
        shapeRaw.RemoveLastDims(2);
        u_shape = shapeRaw;
        u_shape.AddDim(m);
        u_shape.AddDim(m);
        v_shape = shapeRaw;
        v_shape.AddDim(n);
        v_shape.AddDim(n);
      } else if (full_matrices_) {
        u_shape = U->shape();
        v_shape = V->shape();
      } else {
        TensorShape shapeRaw = M_copy.shape();
        shapeRaw.RemoveLastDims(2);
        u_shape = shapeRaw;
        u_shape.AddDim(p);
        u_shape.AddDim(m);
        v_shape = shapeRaw;
        v_shape.AddDim(p);
        v_shape.AddDim(n);
      }
      OP_REQUIRES_OK_ASYNC( context, solver->allocate_scoped_tensor(U->dtype(), u_shape, &u_copy), done);

      if (batched) {
        OP_REQUIRES_OK_ASYNC( context, solver->allocate_scoped_tensor(V->dtype(), v_shape, &v_copy), done);

      }
      outputU_ptr = u_copy.template flat_inner_dims<Scalar, 3>().data();
      if (batched) {
        outputV_ptr = v_copy.template flat_inner_dims<Scalar, 3>().data();
      } else {
        outputV_ptr = V->template flat_inner_dims<Scalar, 3>().data();
      }
    }

    outputS_ptr = S->template flat_inner_dims<RealScalar, 2>().data();
    std::vector<DeviceLapackInfo> dev_info;
    dev_info.push_back(solver->GetDeviceLapackInfo(batch_size, "gesvd"));
    int* dev_info_ptr = dev_info.back().mutable_data();

    
    
    Tensor input_copy;
    if (compute_uv_ && n == 1) {
      OP_REQUIRES_OK_ASYNC(context, solver->allocate_scoped_tensor( DataTypeToEnum<Scalar>::v(), TensorShape({batch_size, m}), &input_copy), done);



      const GPUDevice& d = context->eigen_device<GPUDevice>();
      d.memcpy(input_copy.flat<Scalar>().data(), input_ptr, batch_size * m * sizeof(Scalar));
    }

    if (batched) {
      cusolverEigMode_t jobz = CUSOLVER_EIG_MODE_NOVECTOR;
      if (compute_uv_) jobz = CUSOLVER_EIG_MODE_VECTOR;
      OP_REQUIRES_OK_ASYNC( context, solver->GesvdjBatched(jobz, m, n, input_ptr, m, outputS_ptr, outputU_ptr, m, outputV_ptr, n, dev_info_ptr, batch_size), done);




    } else {
      for (int64 batch = 0; batch < batch_size; ++batch) {
        Scalar* input = input_ptr + batch * m * n;
        RealScalar* outputS = outputS_ptr + batch * p;
        Scalar* outputU = NULL;
        Scalar* outputVT = NULL;
        char jobu = 'N';
        char jobvt = 'N';

        if (compute_uv_) {
          if (full_matrices_) {
            outputU = outputU_ptr + batch * m * m;
            outputVT = outputV_ptr + batch * n * n;
            jobu = 'A';
            jobvt = 'A';
          } else {
            outputU = outputU_ptr + batch * m * p;
            outputVT = outputV_ptr + batch * n * p;
            jobu = 'S';
            jobvt = 'S';
          }
        }

        OP_REQUIRES_OK_ASYNC( context, solver->Gesvd(jobu, jobvt, m, n, input, m, outputS, outputU, m, outputVT, n, dev_info_ptr + batch), done);



      }
    }

    
    
    
    
    
    
    
    
    
    
    
    
    
    if (compute_uv_ && n == 1) {
      
      const GPUDevice& d = context->eigen_device<GPUDevice>();
      d.memset(outputV_ptr, 0, batch_size * sizeof(Scalar));
      Gpu2DLaunchConfig cfg2D = GetGpu2DLaunchConfig(batch_size, m, d);
      TF_CHECK_OK(GpuLaunchKernel(ComputeValueOfVKernel<Scalar>, cfg2D.block_count, cfg2D.thread_per_block, 0, d.stream(), cfg2D, m, full_matrices_ ? m : p, input_copy.flat<Scalar>().data(), outputU_ptr, outputS_ptr, outputV_ptr));



      
      GpuLaunchConfig cfg1D = GetGpuLaunchConfig(batch_size, d);
      TF_CHECK_OK(GpuLaunchKernel(ExtractSignOfVKernel<Scalar>, cfg1D.block_count, cfg1D.thread_per_block, 0, d.stream(), cfg1D, outputV_ptr));

    }

    if (compute_uv_) {
      auto device = context->eigen_device<GPUDevice>();
      OP_REQUIRES_OK_ASYNC(context, DoMatrixTranspose(device, u_copy, U), done);
      if (batched) {
        OP_REQUIRES_OK_ASYNC(context, DoMatrixTranspose(device, v_copy, V), done);
      }
    }

    CheckResult(context, std::move(done), dev_info, std::move(solver));
  }

  void CheckResult(OpKernelContext* context, DoneCallback done, const std::vector<DeviceLapackInfo>& dev_info, std::unique_ptr<GpuSolver> solver) {

    auto info_checker = [context, done]( const Status& status, const std::vector<HostLapackInfo>& ) {

      Status full_status = status;
      if (!full_status.ok()) {
        full_status.Update(errors::InvalidArgument(kErrMsg));
      }
      OP_REQUIRES_OK_ASYNC(context, full_status, done);
      done();
    };

    GpuSolver::CheckLapackInfoAndDeleteSolverAsync(std::move(solver), dev_info, std::move(info_checker));
  }

  
  
  
  void PerformSVD_MgeqN(OpKernelContext* context, DoneCallback done, int64 m, int64 n, int64 p, const Tensor& M, Tensor* S, Tensor* U, Tensor* V) {

    
    TensorShape shapeRaw = M.shape();
    shapeRaw.RemoveLastDims(2);
    TensorShape input_shape = shapeRaw;
    input_shape.AddDim(n);
    input_shape.AddDim(m);
    Tensor input_copy;
    
    std::unique_ptr<GpuSolver> solver(new GpuSolver(context));
    OP_REQUIRES_OK_ASYNC( context, solver->allocate_scoped_tensor(M.dtype(), input_shape, &input_copy), done);


    auto device = context->eigen_device<GPUDevice>();
    OP_REQUIRES_OK_ASYNC(context, DoMatrixTranspose(device, M, &input_copy), done);

    
    RunSVD(context, done, m, n, p, input_copy, S, U, V, std::move(solver));
  }

  
  void PerformSVD_MlessN(OpKernelContext* context, DoneCallback done, int64 m, int64 n, int64 p, const Tensor& M, Tensor* S, Tensor* U, Tensor* V) {

    
    

    
    
    
    
    std::unique_ptr<GpuSolver> solver(new GpuSolver(context));
    Tensor input_copy;
    OP_REQUIRES_OK_ASYNC( context, solver->forward_input_or_allocate_scoped_tensor( {0}, DataTypeToEnum<Scalar>::value, M.shape(), &input_copy), done);




    if (!M.SharesBufferWith(input_copy)) {
      const GPUDevice& d = context->eigen_device<GPUDevice>();
      d.memcpy(input_copy.flat<Scalar>().data(), M.flat<Scalar>().data(), M.NumElements() * sizeof(Scalar));
    }

    
    
    RunSVD(context, done, n, m, p, input_copy, S, V, U, std::move(solver));
  }

  void ComputeAsync(OpKernelContext* context, DoneCallback done) final {
    const Tensor& input = context->input(0);
    const int ndims = input.dims();

    
    OP_REQUIRES_ASYNC( context, ndims >= 2, errors::InvalidArgument("Input must have rank >= 2, got ", ndims), done);



    const int64 m = input.dim_size(ndims - 2);
    const int64 n = input.dim_size(ndims - 1);
    const int64 p = std::min(m, n);

    
    Tensor* outputU = NULL;
    Tensor* outputS = NULL;
    Tensor* outputV = NULL;

    
    TensorShape shapeRaw = input.shape();
    shapeRaw.RemoveLastDims(2);
    TensorShape shapeS = shapeRaw;
    TensorShape shapeU = shapeRaw;
    TensorShape shapeV = shapeRaw;
    shapeS.AddDim(p);
    if (compute_uv_) {
      if (full_matrices_) {
        shapeU.AddDim(m);
        shapeU.AddDim(m);
        shapeV.AddDim(n);
        shapeV.AddDim(n);
      } else {
        shapeU.AddDim(m);
        shapeU.AddDim(p);
        shapeV.AddDim(n);
        shapeV.AddDim(p);
      }
    } else {
      shapeU = TensorShape({0});
      shapeV = TensorShape({0});
    }

    
    OP_REQUIRES_OK_ASYNC(context, context->allocate_output(0, shapeS, &outputS), done);
    OP_REQUIRES_OK_ASYNC(context, context->allocate_output(1, shapeU, &outputU), done);
    OP_REQUIRES_OK_ASYNC(context, context->allocate_output(2, shapeV, &outputV), done);

    if (n == 0 || m == 0) {
      if (n == m || !compute_uv_ || !full_matrices_) {
        
        done();
        return;
      }
      auto device = context->eigen_device<GPUDevice>();
      functor::EyeFunctor<GPUDevice, Scalar> eye;
      if (m > 0) {
        
        auto outputU_reshaped = outputU->flat_inner_dims<Scalar, 3>();
        eye(device, outputU_reshaped);
      } else if (n > 0) {
        
        auto outputV_reshaped = outputV->flat_inner_dims<Scalar, 3>();
        eye(device, outputV_reshaped);
      }
      done();
      return;
    }

    
    if (m >= n) {
      PerformSVD_MgeqN(context, done, m, n, p, input, outputS, outputU, outputV);
    } else {
      PerformSVD_MlessN(context, done, m, n, p, input, outputS, outputU, outputV);
    }
  }

 private:
  bool compute_uv_;
  bool full_matrices_;
};


REGISTER_LINALG_OP_GPU("Svd", (SvdOpGpu<float>), float);
REGISTER_LINALG_OP_GPU("Svd", (SvdOpGpu<double>), double);


REGISTER_LINALG_OP_GPU("BatchSvd", (SvdOpGpu<float>), float);
REGISTER_LINALG_OP_GPU("BatchSvd", (SvdOpGpu<double>), double);

}  



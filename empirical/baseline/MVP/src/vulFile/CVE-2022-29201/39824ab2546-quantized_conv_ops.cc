






















namespace tensorflow {















template <class T1, class T2, class T3> class ReferenceConvFunctor {
 public:
  void operator()(OpKernelContext* context, const T1* input_data, int input_batches, int input_height, int input_width, int input_depth, int input_offset, const T2* filter_data, int filter_height, int filter_width, int filter_count, int filter_offset, int stride, Padding padding, T3* output_data, int output_height, int output_width, int output_shift, int output_offset, int output_mult) {





    
    
    const int32_t highest = static_cast<int32>(Eigen::NumTraits<T3>::highest());
    const int32_t lowest = static_cast<int32>(Eigen::NumTraits<T3>::lowest());

    
    
    
    
    
    const int32_t rounding = (output_shift < 1) ? 0 : (1 << (output_shift - 1));

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    int filter_left_offset;
    int filter_top_offset;
    if (padding == VALID) {
      filter_left_offset = ((output_width - 1) * stride + filter_width - input_width + 1) / 2;
      filter_top_offset = ((output_height - 1) * stride + filter_height - input_height + 1) / 2;
    } else {
      filter_left_offset = ((output_width - 1) * stride + filter_width - input_width) / 2;
      filter_top_offset = ((output_height - 1) * stride + filter_height - input_height) / 2;
    }

    
    for (int batch = 0; batch < input_batches; ++batch) {
      
      
      
      for (int out_y = 0; out_y < output_height; ++out_y) {
        for (int out_x = 0; out_x < output_width; ++out_x) {
          
          for (int out_channel = 0; out_channel < filter_count; ++out_channel) {
            
            
            
            
            const int in_x_origin = (out_x * stride) - filter_left_offset;
            const int in_y_origin = (out_y * stride) - filter_top_offset;
            int32_t total = 0;
            for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
              for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
                for (int in_channel = 0; in_channel < input_depth;
                     ++in_channel) {
                  const int in_x = in_x_origin + filter_x;
                  const int in_y = in_y_origin + filter_y;
                  int32_t input_value;
                  
                  
                  if ((in_x >= 0) && (in_x < input_width) && (in_y >= 0) && (in_y < input_height)) {
                    const T1 input_source_value = input_data[(batch * input_height * input_width * input_depth) + (in_y * input_width * input_depth) + (in_x * input_depth) + in_channel];



                    
                    
                    input_value = static_cast<int32>(input_source_value) - input_offset;
                  } else {
                    input_value = 0;
                  }
                  const T2 filter_source_value = filter_data[(filter_y * filter_width * input_depth * filter_count) + (filter_x * input_depth * filter_count) + (in_channel * filter_count) + out_channel];



                  
                  const int32_t filter_value = static_cast<int32>(filter_source_value) - filter_offset;
                  total += (input_value * filter_value);
                }
              }
            }
            
            
            const int32_t output = ((((total + output_offset) * output_mult) + rounding) >> output_shift);

            
            
            const int32_t top_clamped_output = std::min(output, highest);
            const int32_t clamped_output = std::max(top_clamped_output, lowest);
            output_data[(batch * output_height * output_width * filter_count) + (out_y * output_width * filter_count) + (out_x * filter_count) + out_channel] = clamped_output;

          }
        }
      }
    }
  }
};







const size_t kMaxChunkSize = (1 * 1024 * 1024);




template <class T1, class T2, class T3> class Im2ColConvFunctor {
 public:
  void operator()(OpKernelContext* context, const T1* input_data, int input_batches, int input_height, int input_width, int input_depth, int input_offset, const T2* filter_data, int filter_height, int filter_width, int filter_count, int filter_offset, int stride, Padding padding, T3* output_data, int output_height, int output_width, int output_shift, int output_offset, int output_mult) {





    if (input_offset < 0) {
      
      static int warning_count = 0;
      if (warning_count < 10) {
        ++warning_count;
        LOG(WARNING)
            << "For kernel '" << context->op_kernel().name() << "' from input '" << context->op_kernel().requested_input(0)
            << "': Zero is not representable in the quantized range used by the" << " input. This means QuantizedConv2d has to fall back to a slow" << " implementation, since the border of zero values can't be" << " represented easily. You should try to construct graphs that" << " avoid this situation.";



      }
      ReferenceConvFunctor<T1, T2, T3> conv_functor;
      conv_functor(context, input_data, input_batches, input_height, input_width, input_depth, input_offset, filter_data, filter_height, filter_width, filter_count, filter_offset, stride, padding, output_data, output_height, output_width, output_shift, output_offset, output_mult);



      return;
    }

    OP_REQUIRES( context, output_width > 0, errors::InvalidArgument("output_width must be strictly positive"));

    OP_REQUIRES( context, output_height > 0, errors::InvalidArgument("output_height must be strictly positive"));

    int filter_left_offset;
    int filter_top_offset;
    if (padding == VALID) {
      filter_left_offset = ((output_width - 1) * stride + filter_width - input_width + 1) / 2;
      filter_top_offset = ((output_height - 1) * stride + filter_height - input_height + 1) / 2;
    } else {
      filter_left_offset = ((output_width - 1) * stride + filter_width - input_width) / 2;
      filter_top_offset = ((output_height - 1) * stride + filter_height - input_height) / 2;
    }

    
    
    
    
    
    
    
    
    
    
    
    const int filter_value_count = filter_width * filter_height * input_depth;
    OP_REQUIRES(context, filter_value_count > 0, errors::InvalidArgument( "filter patch must contain at least one element"));

    const int64_t patches_per_chunk = kMaxChunkSize / (filter_value_count * sizeof(T1));
    const int64_t chunk_value_count = (kMaxChunkSize + (sizeof(T1) - 1)) / sizeof(T1);
    
    
    
    
    
    
    Im2ColBufferResource<T1, chunk_value_count>* im2col_buffer_resource;
    std::function<Status(Im2ColBufferResource<T1, chunk_value_count>**)> creator = [](Im2ColBufferResource<T1, chunk_value_count>** resource) {

          
          
          
          const int64 chunk_value_count = (kMaxChunkSize + (sizeof(T1) - 1)) / sizeof(T1);

          *resource = new Im2ColBufferResource<T1, chunk_value_count>();
          return Status::OK();
        };
    OP_REQUIRES_OK(context, context->resource_manager()->LookupOrCreate( "Conv2d", "im2col_buffer", &im2col_buffer_resource, creator));

    
    
    
    
    mutex_lock lock_buffer(im2col_buffer_resource->mu);
    core::ScopedUnref unref_buffer(im2col_buffer_resource);
    T1* im2col_buffer = im2col_buffer_resource->data;

    const int64_t patch_count = (input_batches * output_height * output_width);
    const int64_t chunk_count = (patch_count + (patches_per_chunk - 1)) / patches_per_chunk;

    for (int64_t chunk_index = 0; chunk_index < chunk_count; ++chunk_index) {
      const int64_t patch_index_start = chunk_index * patches_per_chunk;
      const int64_t patch_index_end = std::min(patch_index_start + patches_per_chunk, patch_count);
      for (int64_t patch_index = patch_index_start;
           patch_index < patch_index_end; ++patch_index) {
        const int64_t batch = patch_index / (output_height * output_width);
        const int64_t out_y = (patch_index / output_width) % output_height;
        const int64_t out_x = patch_index % output_width;
        const T1* input_batch_start = input_data + (batch * input_height * input_width * input_depth);
        const int in_y_origin = (out_y * stride) - filter_top_offset;
        const int in_x_origin = (out_x * stride) - filter_left_offset;
        const int patch_index_within_chunk = patch_index % patches_per_chunk;
        T1* im2col_patch_start = im2col_buffer + (patch_index_within_chunk * filter_value_count);
        for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
          const int in_y = in_y_origin + filter_y;
          T1* im2col_row_start = im2col_patch_start + (filter_y * filter_width * input_depth);
          
          
          if ((in_y < 0) || (in_y >= input_height)) {
            
            
            memset(im2col_row_start, input_offset, (filter_width * input_depth));
          } else {
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            const int in_x_end = in_x_origin + filter_width;
            const int left_zero_count = std::max(0, 0 - in_x_origin);
            const int right_zero_count = std::max(0, in_x_end - input_width);
            const int center_copy_count = filter_width - (left_zero_count + right_zero_count);
            if (left_zero_count > 0) {
              T1* im2col_left_start = im2col_row_start;
              memset(im2col_left_start, input_offset, (left_zero_count * input_depth));
            }
            if (center_copy_count > 0) {
              const T1* input_row_start = input_batch_start + (in_y * input_width * input_depth) + (std::max(0, in_x_origin) * input_depth);

              T1* im2col_center_start = im2col_row_start + (left_zero_count * input_depth);
              memcpy(im2col_center_start, input_row_start, (center_copy_count * input_depth));
            }
            if (right_zero_count > 0) {
              T1* im2col_right_start = im2col_row_start + ((left_zero_count + center_copy_count) * input_depth);

              memset(im2col_right_start, input_offset, (right_zero_count * input_depth));
            }
          }
        }
      }
      
      
      
      const int how_many_patches = patch_index_end - patch_index_start;
      const bool transpose_a = false;
      const bool transpose_b = false;
      const bool transpose_c = false;
      const int m = how_many_patches;
      const int n = filter_count;
      const int k = filter_value_count;
      const int lda = filter_value_count;
      const int ldb = filter_count;
      const int ldc = filter_count;
      T3* chunk_output_data = output_data + (patch_index_start * filter_count);

      if (meta::IsSupportedAndEnabled() && std::is_same<T1, quint8>() && std::is_same<T2, quint8>() && std::is_same<T3, qint32>() && (output_offset == 0) && (output_mult == 1) && (output_shift == 0) && (transpose_c == false) && (k <= 2048)) {


        meta::QuantizedGemm(context, transpose_a, transpose_b, im2col_buffer, filter_data, chunk_output_data, m, n, k, -input_offset, -filter_offset, lda, ldb, ldc);

      } else if (std::is_same<T1, quint8>() && std::is_same<T2, quint8>() && std::is_same<T3, qint32>() && (output_offset == 0) && (output_mult == 1) && (output_shift == 0)) {

        
        
        
        const uint8* im2col_data_as_uint8 = &(im2col_buffer->value);
        const uint8* filter_data_as_uint8 = &(filter_data->value);
        int32* output_data_as_int32 = &(chunk_output_data->value);
        
        
        
        
        
        
        static const gemmlowp::MapOrder ResultOrder = !transpose_c ? gemmlowp::MapOrder::RowMajor : gemmlowp::MapOrder::ColMajor;

        static const gemmlowp::MapOrder LhsOrder = !transpose_a ? gemmlowp::MapOrder::RowMajor : gemmlowp::MapOrder::ColMajor;

        static const gemmlowp::MapOrder RhsOrder = !transpose_b ? gemmlowp::MapOrder::RowMajor : gemmlowp::MapOrder::ColMajor;

        gemmlowp::MatrixMap<const std::uint8_t, LhsOrder> lhs( im2col_data_as_uint8, m, k, lda);
        gemmlowp::MatrixMap<const std::uint8_t, RhsOrder> rhs( filter_data_as_uint8, k, n, ldb);
        gemmlowp::MatrixMap<std::int32_t, ResultOrder> result( output_data_as_int32, m, n, ldc);
        const std::tuple<> empty_pipeline = {};

        auto& worker_threads = *(context->device()->tensorflow_cpu_worker_threads());
        TensorflowGemmContext context(worker_threads.num_threads, worker_threads.workers);
        gemmlowp::GemmWithOutputPipeline<std::uint8_t, std::int32_t, gemmlowp::DefaultL8R8BitDepthParams>( &context, lhs, rhs, &result, -input_offset, -filter_offset, empty_pipeline);


        
        
        TF_ANNOTATE_MEMORY_IS_INITIALIZED(output_data_as_int32, m * n * sizeof(int32));
      } else {
        ReferenceGemm<T1, T2, T3>( transpose_a, transpose_b, transpose_c, m, n, k, im2col_buffer, input_offset, lda, filter_data, filter_offset, ldb, chunk_output_data, output_shift, output_offset, output_mult, ldc);


      }
    }
  }
};

template <class T1, class T2, class T3, template <class TF1, class TF2, class TF3> class ConvFunctor> class QuantizedConv2DOp : public OpKernel {

 public:
  explicit QuantizedConv2DOp(OpKernelConstruction* context)
      : OpKernel(context) {
    OP_REQUIRES_OK(context, context->GetAttr("strides", &strides_));
    OP_REQUIRES(context, strides_.size() == 4, errors::InvalidArgument("Sliding window strides field must " "specify 4 dimensions"));

    OP_REQUIRES(context, strides_[1] == strides_[2], errors::InvalidArgument( "Current implementation only supports equal length " "strides in the row and column dimensions."));


    OP_REQUIRES( context, (strides_[0] == 1 && strides_[3] == 1), errors::InvalidArgument("Current implementation does not yet support " "strides in the batch and depth dimensions."));


    std::vector<int32> dilations;
    OP_REQUIRES_OK(context, context->GetAttr("dilations", &dilations));
    OP_REQUIRES(context, dilations.size() == 4, errors::InvalidArgument("Dilations field must " "specify 4 dimensions"));

    OP_REQUIRES(context, dilations[1] == 1 && dilations[2] == 1, errors::InvalidArgument( "Current implementation only supports dilated rate as 1 " "in the row and column dimensions."));


    OP_REQUIRES(context, (dilations[0] == 1 && dilations[3] == 1), errors::InvalidArgument( "Current implementation does not yet support " "dilations in the batch and depth dimensions."));


    OP_REQUIRES_OK(context, context->GetAttr("padding", &padding_));
  }

  void Compute(OpKernelContext* context) override {
    
    
    const Tensor& input = context->input(0);

    
    
    const Tensor& filter = context->input(1);

    
    OP_REQUIRES(context, input.dims() == 4, errors::InvalidArgument("input must be 4-dimensional", input.shape().DebugString()));

    OP_REQUIRES(context, filter.dims() == 4, errors::InvalidArgument("filter must be 4-dimensional: ", filter.shape().DebugString()));


    const float min_input = context->input(2).flat<float>()(0);
    const float max_input = context->input(3).flat<float>()(0);
    const float min_filter = context->input(4).flat<float>()(0);
    const float max_filter = context->input(5).flat<float>()(0);
    const int32_t offset_input = FloatToQuantizedUnclamped<T1>(0.0f, min_input, max_input);
    const int32_t offset_filter = FloatToQuantizedUnclamped<T2>(0.0f, min_filter, max_filter);
    const int32_t offset_output = 0;
    const int32_t mult_output = 1;
    const int32_t shift_output = 0;

    
    
    const int64_t in_depth = input.dim_size(3);
    OP_REQUIRES(context, in_depth == filter.dim_size(2), errors::InvalidArgument( "input and filter must have the same depth: ", in_depth, " vs ", filter.dim_size(2)));



    
    const int64_t out_depth = filter.dim_size(3);

    
    
    const int64_t input_rows = input.dim_size(1);
    const int64_t filter_rows = filter.dim_size(0);

    
    
    const int64_t input_cols = input.dim_size(2);
    const int64_t filter_cols = filter.dim_size(1);

    
    const int64_t batch = input.dim_size(0);

    
    
    
    const int stride = strides_[1];

    int64_t out_rows = 0, out_cols = 0, pad_rows = 0, pad_cols = 0;
    OP_REQUIRES_OK(context, GetWindowedOutputSize(input_rows, filter_rows, stride, padding_, &out_rows, &pad_rows));

    OP_REQUIRES_OK(context, GetWindowedOutputSize(input_cols, filter_cols, stride, padding_, &out_cols, &pad_cols));

    CHECK_GT(batch, 0);
    CHECK_GT(out_rows, 0);
    CHECK_GT(out_cols, 0);
    CHECK_GT(out_depth, 0);
    TensorShape out_shape({batch, out_rows, out_cols, out_depth});

    
    
    Tensor* output = nullptr;
    OP_REQUIRES_OK(context, context->allocate_output(0, out_shape, &output));

    
    
    ConvFunctor<T1, T2, T3> conv_functor;
    conv_functor(context, input.flat<T1>().data(), batch, input_rows, input_cols, in_depth, offset_input, filter.flat<T2>().data(), filter_rows, filter_cols, out_depth, offset_filter, stride, padding_, output->flat<T3>().data(), out_rows, out_cols, shift_output, offset_output, mult_output);




    float min_output_value;
    float max_output_value;
    QuantizationRangeForMultiplication<T1, T2, T3>( min_input, max_input, min_filter, max_filter, &min_output_value, &max_output_value);


    Tensor* output_min = nullptr;
    OP_REQUIRES_OK(context, context->allocate_output(1, {}, &output_min));
    output_min->flat<float>()(0) = min_output_value;

    Tensor* output_max = nullptr;
    OP_REQUIRES_OK(context, context->allocate_output(2, {}, &output_max));
    output_max->flat<float>()(0) = max_output_value;
  }

 private:
  std::vector<int32> strides_;
  Padding padding_;
};



REGISTER_KERNEL_BUILDER( Name("QuantizedConv2D")
        .Device(DEVICE_CPU)
        .TypeConstraint<quint8>("Tinput")
        .TypeConstraint<quint8>("Tfilter")
        .TypeConstraint<qint32>("out_type"), QuantizedConv2DOp<quint8, quint8, qint32, Im2ColConvFunctor>);

}  

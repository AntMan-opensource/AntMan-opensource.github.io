





























namespace tensorflow {
namespace {








const size_t kMaxChunkSize = (1 * 1024 * 1024);

const size_t kMaxChunkSize = (16 * 1024 * 1024);

const size_t kResizeCacheSize = (8 * 1024 * 1024);


enum SamplingMode {
  BILINEAR = 0, NEAREST = 1, };















void FusedConvParallelFor( OpKernelContext* context, int64_t begin, int64_t end, const std::function<void(int64_t, int64_t)>& task_function) {




  task_function(begin, end);

  auto& worker_threads = *(context->device()->tensorflow_cpu_worker_threads());
  thread::ThreadPool* thread_pool = worker_threads.workers;
  const int64_t total_elements = end - begin;
  
  
  const int64_t element_cost = 10000000;
  thread_pool->ParallelFor( total_elements, element_cost, [begin, task_function](int64_t begin_offset, int64_t end_offset) {

        const int64_t task_begin = begin + begin_offset;
        const int64_t task_end = begin + end_offset;
        task_function(task_begin, task_end);
      });

}


template <class T1> struct ResizeTaskParameters {
  ResizeTaskParameters() : st(false, false) {}

  int cache_height;
  T1* resize_cache;
  int cache_line_width;
  int input_width;
  int input_depth;
  int top_padding;
  int pad_offset;
  int64_t resized_height;
  ImageResizerState st;
  const T1* input_batch_start;
  int64_t cache_start_x;
  int64_t cache_end_x;
  int left_padding;
  int64_t resized_width;
  int64_t padded_width;
  int64_t padded_height;
};

template <class T1> struct PerCacheLineParameters {
  PerCacheLineParameters() {}
  PerCacheLineParameters(const PerCacheLineParameters<T1>& other)
      : cache_line_start(other.cache_line_start), input_top_row_start(other.input_top_row_start), input_bottom_row_start(other.input_bottom_row_start), y_lerp(other.y_lerp) {}



  T1* cache_line_start;
  const T1* input_top_row_start;
  const T1* input_bottom_row_start;
  T1 y_lerp;
};


template <class T1> struct SampleRect {
  EIGEN_ALWAYS_INLINE SampleRect(const T1* in_top_left, const T1* in_top_right, const T1* in_bottom_left, const T1* in_bottom_right)

      : top_left(in_top_left), top_right(in_top_right), bottom_left(in_bottom_left), bottom_right(in_bottom_right) {}



  EIGEN_ALWAYS_INLINE T1 BilinearSample(int channel, T1 x_lerp, T1 y_lerp) const {
    const T1 top = top_left[channel] + (top_right[channel] - top_left[channel]) * x_lerp;
    const T1 bottom = bottom_left[channel] + (bottom_right[channel] - bottom_left[channel]) * x_lerp;
    return top + (bottom - top) * y_lerp;
  }

  const T1* top_left;
  const T1* top_right;
  const T1* bottom_left;
  const T1* bottom_right;
};


template <class T1> EIGEN_ALWAYS_INLINE PerCacheLineParameters<T1> CalculatePerCacheLineParameters( int64_t cache_height, int64_t cache_y, T1* resize_cache, int64_t cache_line_width, int64_t input_width, int64_t input_depth, int64_t top_padding, int64_t pad_offset, int64_t resized_height, const ImageResizerState& st, const T1* input_batch_start) {




  PerCacheLineParameters<T1> result;
  
  
  
  
  int64_t cache_index_y;
  if (cache_y < 0) {
    cache_index_y = cache_height + (cache_y % cache_height);
  } else {
    cache_index_y = cache_y % cache_height;
  }
  result.cache_line_start = resize_cache + (cache_index_y * cache_line_width * input_depth);
  
  float in_y = (cache_y - top_padding);
  if (in_y < 0) {
    in_y = -(in_y + 1.0f - pad_offset);
  } else if (in_y >= resized_height) {
    in_y = (resized_height * 2.0f) - (in_y + 1.0f + pad_offset);
  }
  
  in_y *= st.height_scale;
  const int64_t top_y_index = static_cast<int64_t>(std::floor(in_y));
  const int64_t bottom_y_index = std::min(static_cast<int64_t>(std::ceil(in_y)), (st.in_height - 1));
  
  result.y_lerp = static_cast<T1>(in_y - top_y_index);
  
  result.input_top_row_start = input_batch_start + (top_y_index * input_width * input_depth);
  result.input_bottom_row_start = input_batch_start + (bottom_y_index * input_width * input_depth);
  return result;
}

template <class T1> struct PerCachePixelParameters {
  PerCachePixelParameters() {}
  PerCachePixelParameters(const PerCachePixelParameters<T1>& other)
      : cache_line_pixel(other.cache_line_pixel), left_x_index(other.left_x_index), right_x_index(other.right_x_index), x_lerp(other.x_lerp) {}



  T1* cache_line_pixel;
  int64_t left_x_index;
  int64_t right_x_index;
  T1 x_lerp;
};


template <class T1> EIGEN_ALWAYS_INLINE PerCachePixelParameters<T1> CalculatePerCachePixelParameters(int64_t cache_x, int64_t cache_start_x, T1* cache_line_start, int64_t input_depth, int64_t left_padding, int64_t pad_offset, int64_t resized_width, const ImageResizerState& st) {





  PerCachePixelParameters<T1> result;
  
  const int cache_index_x = cache_x - cache_start_x;
  result.cache_line_pixel = cache_line_start + (cache_index_x * input_depth);
  
  float in_x = (cache_x - left_padding);
  if (in_x < 0) {
    in_x = -(in_x + 1.0f - pad_offset);
  } else if (in_x >= resized_width) {
    in_x = (resized_width * 2.0f) - (in_x + 1.0f + pad_offset);
  }
  
  in_x *= st.width_scale;
  
  result.left_x_index = static_cast<int64_t>(std::floor(in_x));
  result.right_x_index = std::min(static_cast<int64_t>(std::ceil(in_x)), (st.in_width - 1));
  
  result.x_lerp = static_cast<T1>(in_x - result.left_x_index);
  return result;
}



template <class T1, class T2, class T3, class TGemmFunctor, SamplingMode SampleMode> class FusedResizeAndPadConvFunctor {

 public:
  void operator()(OpKernelContext* context, const Tensor& input, int input_batches, int resized_height, int resized_width, int padded_height, int padded_width, int input_depth, const T2* filter_data, int filter_height, int filter_width, int filter_count, int stride_rows, int stride_cols, Padding padding, T3* output_data, int output_height, int output_width, const ImageResizerState& st, int top_padding, int bottom_padding, int left_padding, int right_padding, int pad_offset) {







    if ((input_batches <= 0) || (padded_width <= 0) || (padded_height <= 0) || (input_depth <= 0)) {
      LOG(WARNING) << "Conv2D was called with bad input dimensions: " << input_batches << ", " << padded_height << ", " << padded_width << ", " << input_depth;

      return;
    }
    if ((filter_width <= 0) || (filter_height <= 0) || (filter_count <= 0)) {
      LOG(WARNING) << "Conv2D was called with bad filter dimensions: " << filter_width << ", " << filter_height << ", " << filter_count;

      return;
    }
    if ((output_width <= 0) || (output_height <= 0)) {
      LOG(WARNING) << "Conv2D was called with bad output width or height: " << output_width << ", " << output_height;
      return;
    }
    OP_REQUIRES( context, ((SampleMode == NEAREST) || (SampleMode == BILINEAR)), errors::InvalidArgument("Bad sample mode passed in", SampleMode));


    
    
    
    int filter_left_offset;
    int filter_top_offset;
    if (padding == VALID) {
      filter_left_offset = ((output_width - 1) * stride_cols + filter_width - padded_width + 1) / 2;

      filter_top_offset = ((output_height - 1) * stride_rows + filter_height - padded_height + 1) / 2;

    } else {
      filter_left_offset = ((output_width - 1) * stride_cols + filter_width - padded_width) / 2;
      filter_top_offset = ((output_height - 1) * stride_rows + filter_height - padded_height) / 2;

    }

    ResizeTaskParameters<T1> task_params;
    task_params.input_depth = input_depth;
    task_params.top_padding = top_padding;
    task_params.pad_offset = pad_offset;
    task_params.resized_height = resized_height;
    task_params.st = st;
    task_params.left_padding = left_padding;
    task_params.resized_width = resized_width;
    task_params.padded_width = padded_width;
    task_params.padded_height = padded_height;

    
    
    
    
    
    
    
    
    
    
    
    const int filter_value_count = filter_width * filter_height * input_depth;

    OP_REQUIRES(context, (filter_value_count * sizeof(T1)) <= kMaxChunkSize, errors::InvalidArgument("Im2Col patch too large for buffer"));
    const size_t patches_per_chunk = kMaxChunkSize / (filter_value_count * sizeof(T1));
    
    
    
    
    Im2ColBufferResource<T1, kMaxChunkSize>* im2col_buffer_resource;
    std::function<Status(Im2ColBufferResource<T1, kMaxChunkSize>**)> creator = [](Im2ColBufferResource<T1, kMaxChunkSize>** resource) {
          *resource = new Im2ColBufferResource<T1, kMaxChunkSize>();
          return OkStatus();
        };
    OP_REQUIRES_OK(context, context->resource_manager()->LookupOrCreate( "Conv2d", "im2col_buffer", &im2col_buffer_resource, creator));


    
    
    
    
    
    
    
    
    
    
    
    
    
    
    task_params.cache_start_x = -filter_left_offset;
    task_params.cache_end_x = (((output_width - 1) * stride_cols) - filter_left_offset) + filter_width;

    task_params.cache_line_width = task_params.cache_end_x - task_params.cache_start_x;
    task_params.cache_height = kResizeCacheSize / (task_params.cache_line_width * input_depth);
    const int needed_resize_cache_count = filter_height * task_params.cache_line_width * input_depth;
    OP_REQUIRES(context, (needed_resize_cache_count * sizeof(T1)) <= kResizeCacheSize, errors::InvalidArgument("Input too large for resize cache"));

    Im2ColBufferResource<T1, kResizeCacheSize>* resize_cache_resource;
    std::function<Status(Im2ColBufferResource<T1, kResizeCacheSize>**)> resize_creator = [](Im2ColBufferResource<T1, kResizeCacheSize>** resource) {

              *resource = new Im2ColBufferResource<T1, kResizeCacheSize>();
              return OkStatus();
            };
    OP_REQUIRES_OK(context, context->resource_manager()->LookupOrCreate( "Conv2d", "resize_cache", &resize_cache_resource, resize_creator));


    
    
    
    
    mutex_lock lock_buffer(im2col_buffer_resource->mu);
    core::ScopedUnref unref_buffer(im2col_buffer_resource);
    T1* im2col_buffer = im2col_buffer_resource->data;

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    mutex_lock resize_lock_buffer(resize_cache_resource->mu);
    core::ScopedUnref unref_resized_cache(resize_cache_resource);
    task_params.resize_cache = resize_cache_resource->data;

    const T1* input_data = input.flat<T1>().data();
    const int64_t input_height = input.shape().dim_sizes()[1];
    task_params.input_width = input.shape().dim_sizes()[2];

    int end_cached_lines = std::numeric_limits<int>::min();

    for (int batch = 0; batch < input_batches; ++batch) {
      task_params.input_batch_start = input_data + (batch * input_height * task_params.input_width * input_depth);

      const int in_y_end = ((output_height * stride_rows) - filter_top_offset) + filter_height;
      for (int out_y = 0; out_y < output_height; ++out_y) {
        const int in_y_origin = (out_y * stride_rows) - filter_top_offset;
        const int cache_start_y = std::max(in_y_origin, end_cached_lines);
        const int cache_end_y = std::min( in_y_end, std::max((in_y_origin + task_params.cache_height), end_cached_lines));

        if (end_cached_lines < (in_y_origin + filter_height)) {
          
          
          FusedConvParallelFor( context, cache_start_y, cache_end_y, [task_params](int64_t task_cache_start_y, int64_t task_cache_end_y) {


                
                
                
                
                
                
                
                
                
                
                
                for (int cache_y = task_cache_start_y;
                     cache_y < task_cache_end_y; ++cache_y) {
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  PerCacheLineParameters<T1> line_params( CalculatePerCacheLineParameters<T1>( task_params.cache_height, cache_y, task_params.resize_cache, task_params.cache_line_width, task_params.input_width, task_params.input_depth, task_params.top_padding, task_params.pad_offset, task_params.resized_height, task_params.st, task_params.input_batch_start));






                  
                  for (int cache_x = task_params.cache_start_x;
                       cache_x < task_params.cache_end_x; ++cache_x) {
                    
                    
                    PerCachePixelParameters<T1> pixel_params( CalculatePerCachePixelParameters<T1>( cache_x, task_params.cache_start_x, line_params.cache_line_start, task_params.input_depth, task_params.left_padding, task_params.pad_offset, task_params.resized_width, task_params.st));





                    
                    
                    
                    if ((cache_x < 0) || (cache_x >= task_params.padded_width) || (cache_y < 0) || (cache_y >= task_params.padded_height)) {


                      std::fill_n(pixel_params.cache_line_pixel, task_params.input_depth, T1(0));
                    } else {
                      
                      
                      
                      
                      if (SampleMode == NEAREST) {
                        const T1* input_top_left_pixel = line_params.input_top_row_start + (pixel_params.left_x_index * task_params.input_depth);



                        std::copy_n(input_top_left_pixel, task_params.input_depth, pixel_params.cache_line_pixel);

                      } else {
                        const SampleRect<T1> rect( line_params.input_top_row_start + (pixel_params.left_x_index * task_params.input_depth), line_params.input_top_row_start + (pixel_params.right_x_index * task_params.input_depth), line_params.input_bottom_row_start + (pixel_params.left_x_index * task_params.input_depth), line_params.input_bottom_row_start + (pixel_params.right_x_index * task_params.input_depth));











                        for (int in_channel = 0;
                             in_channel < task_params.input_depth;
                             ++in_channel) {
                          pixel_params.cache_line_pixel[in_channel] = rect.BilinearSample(in_channel, pixel_params.x_lerp, line_params.y_lerp);


                        }
                      }
                    }
                  }
                }
              });
          end_cached_lines = cache_end_y;
        }
        for (int out_x = 0; out_x < output_width; ++out_x) {
          const int in_x_origin = (out_x * stride_cols) - filter_left_offset;
          const int patch_index = (batch * output_width * output_height) + (out_y * output_width) + out_x;
          const int patch_index_within_chunk = patch_index % patches_per_chunk;
          T1* im2col_patch_start = im2col_buffer + (patch_index_within_chunk * filter_value_count);
          for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
            T1* im2col_row_start = im2col_patch_start + (filter_y * filter_width * task_params.input_depth);

            const int conv_in_y = in_y_origin + filter_y;
            int cache_index_y;
            if (conv_in_y < 0) {
              cache_index_y = task_params.cache_height + (conv_in_y % task_params.cache_height);
            } else {
              cache_index_y = conv_in_y % task_params.cache_height;
            }
            T1* cache_line_start = task_params.resize_cache + (cache_index_y * task_params.cache_line_width * task_params.input_depth);


            T1* cache_filter_row_start = cache_line_start + ((in_x_origin - task_params.cache_start_x) * task_params.input_depth);

            std::copy_n(cache_filter_row_start, (filter_width * task_params.input_depth), im2col_row_start);

          }
          const bool is_last_in_chunk = (patch_index_within_chunk == (patches_per_chunk - 1));
          const bool is_last_overall = ((batch == (input_batches - 1)) && (out_y == (output_height - 1)) && (out_x == (output_width - 1)));

          if (is_last_in_chunk || is_last_overall) {
            
            
            
            
            const int how_many_patches = patch_index_within_chunk + 1;
            const int m = how_many_patches;
            const int n = filter_count;
            const int k = filter_value_count;
            const int lda = filter_value_count;
            const int ldb = filter_count;
            const int ldc = filter_count;
            const size_t start_patch_index = patch_index - (how_many_patches - 1);
            T3* chunk_output_data = output_data + (start_patch_index * filter_count);
            TGemmFunctor gemm_functor;
            gemm_functor(context, m, n, k, im2col_buffer, lda, filter_data, ldb, chunk_output_data, ldc);
          }
        }
      }
    }
  }
};

}  



template <class T, class TConvFunctor, bool DoResize> class FusedResizeConv2DUsingGemmOp : public OpKernel {
 public:
  explicit FusedResizeConv2DUsingGemmOp(OpKernelConstruction* context)
      : OpKernel(context) {
    if (DoResize) {
      OP_REQUIRES_OK(context, context->GetAttr("resize_align_corners", &align_corners_));
    }
    MirrorPadMode mode;
    OP_REQUIRES_OK(context, context->GetAttr("mode", &mode));

    switch (mode) {
      case MirrorPadMode::SYMMETRIC: {
        offset_ = 0;
        break;
      }
      case MirrorPadMode::REFLECT: {
        offset_ = 1;
        break;
      }
      default:
        OP_REQUIRES(context, false, errors::InvalidArgument( "mode must be either REFLECT or SYMMETRIC."));

    }
    OP_REQUIRES_OK(context, context->GetAttr("strides", &strides_));
    OP_REQUIRES(context, strides_.size() == 4, errors::InvalidArgument("Sliding window strides field must " "specify 4 dimensions"));

    const int64_t stride_n = GetTensorDim(strides_, FORMAT_NHWC, 'N');
    const int64_t stride_c = GetTensorDim(strides_, FORMAT_NHWC, 'C');
    OP_REQUIRES( context, stride_n == 1 && stride_c == 1, errors::InvalidArgument("Current implementation does not yet support " "strides in the batch and depth dimensions."));


    OP_REQUIRES_OK(context, context->GetAttr("padding", &padding_));
  }

  void Compute(OpKernelContext* context) override {
    
    
    const Tensor& input = context->input(0);
    OP_REQUIRES(context, (input.shape().num_elements() > 0), errors::InvalidArgument("Input tensor can't be empty"));

    ImageResizerState st(false, false);
    if (DoResize) {
      st = ImageResizerState(align_corners_, false);
      st.ValidateAndCalculateOutputSize(context);
      if (!context->status().ok()) return;
    } else {
      
      st.batch_size = input.dim_size(0);
      st.out_height = input.dim_size(1);
      st.out_width = input.dim_size(2);
      st.in_height = input.dim_size(1);
      st.in_width = input.dim_size(2);
      st.channels = input.dim_size(3);
      st.height_scale = 1.0f;
      st.width_scale = 1.0f;
    }
    TensorShape resized_shape( {input.dim_size(0), st.out_height, st.out_width, input.dim_size(3)});
    int paddings_index;
    int filter_index;
    if (DoResize) {
      paddings_index = 2;
      filter_index = 3;
    } else {
      paddings_index = 1;
      filter_index = 2;
    }
    const Tensor& paddings = context->input(paddings_index);

    const int dims = resized_shape.dims();
    OP_REQUIRES( context, TensorShapeUtils::IsMatrix(paddings.shape()) && paddings.dim_size(1) == 2, errors::InvalidArgument("paddings must be a matrix with 2 columns: ", paddings.shape().DebugString()));




    OP_REQUIRES( context, dims == paddings.dim_size(0), errors::InvalidArgument( "The first dimension of paddings must be the rank of inputs: ", dims, " ", paddings.shape().DebugString(), " ", resized_shape.DebugString()));




    OP_REQUIRES( context, dims == paddings.dim_size(0), errors::InvalidArgument( "The first dimension of paddings must be the rank of inputs: ", dims, " ", paddings.shape().DebugString(), " ", resized_shape.DebugString()));





    OP_REQUIRES( context, dims == 4, errors::InvalidArgument( "Fused mirror padding only supports four-dimensional inputs, but ", dims, " requested"));




    
    TensorShape padded_shape;
    TTypes<int32>::ConstMatrix paddings_matrix = paddings.matrix<int32>();
    for (int d = 0; d < dims; ++d) {
      const int32_t before = paddings_matrix(d, 0);
      const int32_t after = paddings_matrix(d, 1);
      OP_REQUIRES(context, before >= 0 && after >= 0, errors::InvalidArgument( "paddings must be non-negative: ", before, " ", after));

      if (offset_ == 0) {  
        OP_REQUIRES( context, before <= resized_shape.dim_size(d) && after <= resized_shape.dim_size(d), errors::InvalidArgument("paddings must be no greater " "than the dimension size: ", before, ", ", after, " greater than ", resized_shape.dim_size(d)));






      } else if (offset_ == 1) {  
        OP_REQUIRES( context, before < resized_shape.dim_size(d) && after < resized_shape.dim_size(d), errors::InvalidArgument("paddings must be less than" " the dimension size: ", before, ", ", after, " not less than ", resized_shape.dim_size(d)));






      }
      padded_shape.AddDim(before + resized_shape.dim_size(d) + after);
    }

    OP_REQUIRES( context, ((paddings_matrix(0, 0) == 0) && (paddings_matrix(0, 1) == 0)), errors::InvalidArgument( "Fused mirror padding only support spatial padding, not batches: ", paddings.DebugString()));



    OP_REQUIRES( context, ((paddings_matrix(3, 0) == 0) && (paddings_matrix(3, 1) == 0)), errors::InvalidArgument( "Fused mirror padding only support spatial padding, not channels: ", paddings.DebugString()));



    const int32_t top_padding = paddings_matrix(1, 0);
    const int32_t bottom_padding = paddings_matrix(1, 1);
    const int32_t left_padding = paddings_matrix(2, 0);
    const int32_t right_padding = paddings_matrix(2, 1);

    
    
    const Tensor& filter = context->input(filter_index);

    
    OP_REQUIRES(context, padded_shape.dims() == 4, errors::InvalidArgument("input must be 4-dimensional", padded_shape.DebugString()));

    OP_REQUIRES(context, filter.dims() == 4, errors::InvalidArgument("filter must be 4-dimensional: ", filter.shape().DebugString()));


    
    
    for (int i = 0; i < 3; i++) {
      OP_REQUIRES( context, FastBoundsCheck(filter.dim_size(i), std::numeric_limits<int>::max()), errors::InvalidArgument("filter too large"));


    }

    
    
    const int64_t in_depth = padded_shape.dim_size(3);
    OP_REQUIRES(context, in_depth == filter.dim_size(2), errors::InvalidArgument( "input and filter must have the same depth: ", in_depth, " vs ", filter.dim_size(2)));



    
    const int out_depth = static_cast<int>(filter.dim_size(3));

    
    
    const int64_t padded_rows_raw = padded_shape.dim_size(1);
    OP_REQUIRES( context, FastBoundsCheck(padded_rows_raw, std::numeric_limits<int>::max()), errors::InvalidArgument("Input rows too large"));


    const int padded_rows = static_cast<int>(padded_rows_raw);
    const int filter_rows = static_cast<int>(filter.dim_size(0));
    const int resized_rows = static_cast<int>(resized_shape.dim_size(1));

    
    
    const int64_t padded_cols_raw = padded_shape.dim_size(2);
    OP_REQUIRES( context, FastBoundsCheck(padded_cols_raw, std::numeric_limits<int>::max()), errors::InvalidArgument("Input cols too large"));


    const int padded_cols = static_cast<int>(padded_cols_raw);
    const int filter_cols = static_cast<int>(filter.dim_size(1));
    const int resized_cols = static_cast<int>(resized_shape.dim_size(2));

    
    const int64_t batch_raw = padded_shape.dim_size(0);
    OP_REQUIRES(context, FastBoundsCheck(batch_raw, std::numeric_limits<int>::max()), errors::InvalidArgument("batch is too large"));

    const int batch = static_cast<int>(batch_raw);

    
    
    const int stride_rows = GetTensorDim(strides_, FORMAT_NHWC, 'H');
    const int stride_cols = GetTensorDim(strides_, FORMAT_NHWC, 'W');

    int64_t out_rows = 0, out_cols = 0, pad_rows = 0, pad_cols = 0;
    OP_REQUIRES_OK(context, GetWindowedOutputSize(padded_rows, filter_rows, stride_rows, padding_, &out_rows, &pad_rows));

    OP_REQUIRES_OK(context, GetWindowedOutputSize(padded_cols, filter_cols, stride_cols, padding_, &out_cols, &pad_cols));

    TensorShape out_shape = ShapeFromFormat(FORMAT_NHWC, batch, out_rows, out_cols, out_depth);
    OP_REQUIRES(context, (out_shape.num_elements() > 0), errors::InvalidArgument("Output tensor can't be empty"));

    
    
    Tensor* output = nullptr;
    OP_REQUIRES_OK(context, context->allocate_output(0, out_shape, &output));

    VLOG(2) << "FusedConv2D: " << name() << ", in_depth = " << in_depth << ", padded_cols = " << padded_cols << ", resized_cols = " << resized_cols << ", filter_cols = " << filter_cols << ", padded_rows = " << padded_rows << ", resized_rows = " << resized_rows << ", filter_rows = " << filter_rows << ", stride_rows = " << stride_rows << ", stride_cols = " << stride_cols << ", out_depth = " << out_depth << ", DoResize=" << DoResize;









    
    if (out_shape.num_elements() == 0) {
      return;
    }
    TConvFunctor conv_functor;
    conv_functor(context, input, batch, resized_rows, resized_cols, padded_rows, padded_cols, in_depth, filter.flat<T>().data(), filter_rows, filter_cols, out_depth, stride_rows, stride_cols, padding_, output->flat<T>().data(), out_rows, out_cols, st, top_padding, bottom_padding, left_padding, right_padding, offset_);



  }

 private:
  std::vector<int32> strides_;
  Padding padding_;
  bool align_corners_;
  int offset_;

  TF_DISALLOW_COPY_AND_ASSIGN(FusedResizeConv2DUsingGemmOp);
};











TF_CALL_half(REGISTER_FUSED);
TF_CALL_float(REGISTER_FUSED);
TF_CALL_double(REGISTER_FUSED);









TF_CALL_half(REGISTER_PAD_ONLY_FUSED);
TF_CALL_float(REGISTER_PAD_ONLY_FUSED);
TF_CALL_double(REGISTER_PAD_ONLY_FUSED);

}  






































namespace tensorflow {

namespace {



static constexpr const bool kDoNotCheckDuplicates = true;

inline bool IsMerge(const NodeDef& node_def) {
  return node_def.op() == "Merge" || node_def.op() == "RefMerge" || node_def.op() == "_XlaMerge";
}

inline bool IsNextIteration(const NodeDef& node_def) {
  return node_def.op() == "NextIteration" || node_def.op() == "RefNextIteration";
}

bool IsValidNodeName(StringPiece s, bool allow_internal_ops) {
  using ::tensorflow::strings::Scanner;
  Scanner scanner(s);
  scanner .One(allow_internal_ops ? Scanner::LETTER_DIGIT_DOT_UNDERSCORE : Scanner::LETTER_DIGIT_DOT)

      .Any(Scanner::LETTER_DIGIT_DASH_DOT_SLASH_UNDERSCORE);

  while (true) {
    if (!scanner.GetResult())  
      return false;
    if (scanner.empty())  
      return true;

    
    scanner.One(Scanner::RANGLE)
        .One(Scanner::LETTER_DIGIT_DOT)
        .Any(Scanner::LETTER_DIGIT_DASH_DOT_SLASH_UNDERSCORE);
  }
}

class GraphConstructor {
 public:
  struct Options {
    Options(const GraphConstructorOptions& in)  
        : allow_internal_ops(in.allow_internal_ops), expect_device_spec(in.expect_device_spec), importing(false), validate_nodes(in.validate_nodes), validate_colocation_constraints(false), add_default_attributes(in.add_default_attributes) {}




    Options(const ImportGraphDefOptions& in)  
        : allow_internal_ops(false), expect_device_spec(false), prefix(in.prefix.empty() || str_util::EndsWith(in.prefix, "/")

                     ? in.prefix : in.prefix + "/"), uniquify_names(in.uniquify_names), uniquify_prefix(in.uniquify_prefix), input_map(in.input_map.begin(), in.input_map.end()), skip_mapped_nodes(in.skip_mapped_nodes), control_dependencies(in.control_dependencies), return_tensors(in.return_tensors.begin(), in.return_tensors.end()), return_nodes(in.return_nodes), importing(true), validate_nodes(true), validate_colocation_constraints(in.validate_colocation_constraints), validate_shape(in.validate_shape), default_device(in.default_device) {}













    bool allow_internal_ops;
    bool expect_device_spec;

    string prefix;
    bool uniquify_names;
    bool uniquify_prefix;
    std::map<TensorId, TensorId> input_map;
    bool skip_mapped_nodes;
    std::vector<string> control_dependencies;
    std::vector<TensorId> return_tensors;
    std::vector<string> return_nodes;

    
    
    
    
    
    
    
    
    bool importing;
    
    
    
    bool validate_nodes;
    bool validate_colocation_constraints;
    bool validate_shape = true;

    
    
    bool add_default_attributes = true;

    string default_device;
  };

  typedef gtl::ArraySlice<const NodeDef*> NodeDefSlice;

  
  static Status Construct( const Options& opts, NodeDefSlice node_defs, const VersionDef* versions, const FunctionDefLibrary* library, Graph* g, ShapeRefiner* refiner, std::vector<std::pair<Node*, int>>* return_tensors, std::vector<Node*>* return_nodes, std::vector<SafeTensorId>* missing_unused_input_map_keys);





  static Status Construct( const Options& opts, GraphDef&& graph_def, Graph* g, ShapeRefiner* refiner, std::vector<std::pair<Node*, int>>* return_tensors, std::vector<Node*>* return_nodes, std::vector<SafeTensorId>* missing_unused_input_map_keys);




 protected:
  GraphConstructor(const Options& opts, Graph* g, ShapeRefiner* refiner, std::vector<std::pair<Node*, int>>* return_tensors, std::vector<Node*>* return_nodes, std::vector<SafeTensorId>* missing_unused_input_map_keys)


      : opts_(opts), g_(g), original_versions_(g->versions()), prefix_(opts.prefix), refiner_(refiner), return_tensors_(return_tensors), return_nodes_(return_nodes), missing_unused_input_map_keys_(missing_unused_input_map_keys) {}







  virtual ~GraphConstructor() {}

  Status TryImport() {
    TF_RETURN_IF_ERROR(EnsureNoNameCollisions());
    TF_RETURN_IF_ERROR(ValidateInputMapAndControlDependencies());
    TF_RETURN_IF_ERROR(BuildNodeIndex());
    TF_RETURN_IF_ERROR(InitFromEdges());

    
    
    TF_RETURN_IF_ERROR(Convert());

    TF_RETURN_IF_ERROR(AddBackEdges());
    TF_RETURN_IF_ERROR(UpdateVersionDef());
    TF_RETURN_IF_ERROR(PopulateReturnTensors());
    TF_RETURN_IF_ERROR(PopulateReturnNodes());
    TF_RETURN_IF_ERROR(PopulateMissingUnusedInputMapKeys());
    UpdateUniquifiedColocationNames();
    FixupSourceAndSinkEdges(g_);
    return Status::OK();
  }

 private:
  Status EnsureNoNameCollisions();
  Status ValidateInputMapAndControlDependencies();
  Status BuildNodeIndex();
  Status InitFromEdges();
  Status Convert();
  Status AddBackEdges();
  Status UpdateVersionDef();
  Status PopulateReturnTensors();
  Status PopulateReturnNodes();
  Status PopulateMissingUnusedInputMapKeys();

  void Undo();

  
  void PrintCycles();
  
  void DFS(int cur_node, std::vector<int>* cur_branch, std::vector<bool>* is_on_cur_branch, absl::flat_hash_set<int>* unvisited);

  Status IsNodeFullyMapped(const NodeDef& node_def, bool* is_node_mapped);
  Status ValidateColocationConstraints(const NodeDef& node_def);
  Status MakeNode(NodeDef&& node_def, Node** node);
  Status MakeEdge(Node* src, int output_index, Node* dst, int input_index);
  Status ValidateShape(Node* node);
  Status ModifyNodeDefForImport(NodeDef* node_def);
  
  
  
  
  void RemapNodeDefInputs(NodeDef* node_def, std::vector<bool>* input_already_exists);
  
  
  
  void AddControlDependencies(NodeDef* node_def, std::vector<bool>* input_already_exists);
  void AddPrefixToNodeDef(const std::vector<bool>& input_already_exists, NodeDef* node_def);

  
  
  
  void UniquifyNames(const std::vector<bool>& input_already_exists, NodeDef* node_def);

  
  
  
  void UpdateUniquifiedColocationNames();

  
  
  bool NameExistsInGraph(StringPiece name);

  
  
  bool NameExistsInGraphDef(StringPiece name);

  
  
  string FindUniqueName(StringPiece original_name);

  
  
  void UpdatePendingCountAndReady(int processed, bool is_next_iteration);

  
  

  
  virtual size_t node_def_count() const = 0;
  
  
  virtual const NodeDef& get_node_def(int i) const = 0;
  
  
  
  virtual NodeDef consume_node_def(int i) = 0;
  
  
  virtual const VersionDef* versions() const = 0;
  
  
  virtual const FunctionDefLibrary* library() const = 0;

  
  const Options opts_;
  Graph* g_;
  const VersionDef original_versions_;

  
  string prefix_;

  ShapeRefiner* refiner_;

  
  std::vector<std::pair<Node*, int>>* return_tensors_;

  
  std::vector<Node*>* return_nodes_;

  
  std::vector<SafeTensorId>* missing_unused_input_map_keys_;

  
  
  std::set<TensorId> used_input_map_keys_;

  
  absl::flat_hash_set<int> merge_node_indices_;

  
  struct NodeInfo {
    explicit NodeInfo(int i) : gdef_index(i), node(nullptr) {}
    
    NodeInfo() : NodeInfo(-1) {}
    int gdef_index;
    Node* node;  
  };
  gtl::FlatMap<StringPiece, NodeInfo, StringPieceHasher> gdef_nodes_;

  
  
  
  std::vector<string> string_intern_table_;

  
  gtl::FlatSet<StringPiece, StringPieceHasher> gdef_prefixes_;

  
  gtl::FlatMap<StringPiece, Node*, StringPieceHasher> existing_nodes_;

  
  gtl::FlatSet<StringPiece, StringPieceHasher> existing_prefixes_;

  
  
  gtl::FlatMap<string, string> uniquified_names_;

  
  
  std::set<int> ready_;

  
  
  std::vector<int> pending_count_;

  
  
  std::vector<gtl::InlinedVector<int, 4>> outputs_;

  
  
  struct InputInfo {
    explicit InputInfo(const string& node_name, Node* n, int i)
        : name(node_name), node(n), index(i) {}
    
    string name;
    Node* node;
    int index;

    static bool IsControlInput(const InputInfo& input) {
      return input.index == Graph::kControlSlot;
    }
    static int CompareName(const InputInfo& lhs, const InputInfo& rhs) {
      return lhs.name < rhs.name;
    }
    static bool IsSameName(const InputInfo& lhs, const InputInfo& rhs) {
      return lhs.name == rhs.name;
    }
  };

  
  
  struct EdgeInfo {
    explicit EdgeInfo(const string& name, int i1, Node* n, int i2)
        : src_name(name), src_index(i1), dst_node(n), dst_index(i2) {}
    
    string src_name;
    int src_index;
    Node* dst_node;
    int dst_index;
  };
  std::vector<EdgeInfo> back_edges_;

  TF_DISALLOW_COPY_AND_ASSIGN(GraphConstructor);
};






class NodeDefCopyingGraphConstructor : public GraphConstructor {
 public:
  NodeDefCopyingGraphConstructor( const Options& opts, NodeDefSlice node_defs, const VersionDef* versions, const FunctionDefLibrary* library, Graph* g, ShapeRefiner* refiner, std::vector<std::pair<Node*, int>>* return_tensors, std::vector<Node*>* return_nodes, std::vector<SafeTensorId>* missing_unused_input_map_keys)




      : GraphConstructor(opts, g, refiner, return_tensors, return_nodes, missing_unused_input_map_keys), node_defs_(node_defs), versions_(versions), library_(library) {}




 private:
  size_t node_def_count() const override { return node_defs_.size(); }
  const NodeDef& get_node_def(int i) const override { return *node_defs_[i]; }
  NodeDef consume_node_def(int i) override { return *node_defs_[i]; }
  const VersionDef* versions() const override { return versions_; }
  const FunctionDefLibrary* library() const override { return library_; }

  const NodeDefSlice node_defs_;
  const VersionDef* const versions_;
  const FunctionDefLibrary* const library_;
};



class NodeDefMovingGraphConstructor : public GraphConstructor {
 public:
  NodeDefMovingGraphConstructor( const Options& opts, GraphDef&& graph_def, Graph* g, ShapeRefiner* refiner, std::vector<std::pair<Node*, int>>* return_tensors, std::vector<Node*>* return_nodes, std::vector<SafeTensorId>* missing_unused_input_map_keys)



      : GraphConstructor(opts, g, refiner, return_tensors, return_nodes, missing_unused_input_map_keys), graph_def_(std::move(graph_def)), is_consumed_(graph_def_.node_size(), false) {}



 private:
  size_t node_def_count() const override { return graph_def_.node().size(); }
  const NodeDef& get_node_def(int i) const override {
    CHECK(!is_consumed_[i])
        << "NodeDef " << i << " accessed after it was consumed.";
    return graph_def_.node(i);
  }
  NodeDef consume_node_def(int i) override {
    CHECK(!is_consumed_[i]) << "NodeDef " << i << " consumed twice.";
    is_consumed_[i] = true;
    return std::move(*graph_def_.mutable_node(i));
  }
  const VersionDef* versions() const override { return &graph_def_.versions(); }
  const FunctionDefLibrary* library() const override {
    return &graph_def_.library();
  }

  GraphDef graph_def_;
  std::vector<bool> is_consumed_;
};

bool ForwardCompatibilityWindowPassed(const VersionDef& versions) {
  
  
  return (versions.producer() - TF_GRAPH_DEF_VERSION) > 21;
}

Status MaybeAppendVersionWarning(const VersionDef* versions, const Status& import_status) {
  if (versions && ForwardCompatibilityWindowPassed(*versions)) {
    return Status( import_status.code(), absl::StrCat( "Converting GraphDef to Graph has failed with an error: '", import_status.error_message(), "' The binary trying to import the GraphDef was built when " "GraphDef version was ", TF_GRAPH_DEF_VERSION, ". The GraphDef was produced by a binary built when GraphDef " "version was ", versions->producer(), ". The difference between these versions is larger than " "TensorFlow's forward compatibility guarantee, and might be the " "root cause for failing to import the GraphDef."));












  }
  return import_status;
}

 Status GraphConstructor::Construct( const Options& opts, NodeDefSlice node_defs, const VersionDef* versions, const FunctionDefLibrary* library, Graph* g, ShapeRefiner* refiner, std::vector<std::pair<Node*, int>>* return_tensors, std::vector<Node*>* return_nodes, std::vector<SafeTensorId>* missing_unused_input_map_keys) {




  if (versions) {
    TF_RETURN_IF_ERROR(CheckVersions(*versions, TF_GRAPH_DEF_VERSION, TF_GRAPH_DEF_VERSION_MIN_PRODUCER, "GraphDef", "graph"));

  }
  NodeDefCopyingGraphConstructor c(opts, node_defs, versions, library, g, refiner, return_tensors, return_nodes, missing_unused_input_map_keys);

  Status s = c.TryImport();
  if (!s.ok()) {
    c.Undo();
    s = MaybeAppendVersionWarning(versions, s);
  }
  return s;
}

 Status GraphConstructor::Construct( const Options& opts, GraphDef&& graph_def, Graph* g, ShapeRefiner* refiner, std::vector<std::pair<Node*, int>>* return_tensors, std::vector<Node*>* return_nodes, std::vector<SafeTensorId>* missing_unused_input_map_keys) {



  TF_RETURN_IF_ERROR(CheckVersions(graph_def.versions(), TF_GRAPH_DEF_VERSION, TF_GRAPH_DEF_VERSION_MIN_PRODUCER, "GraphDef", "graph"));

  VersionDef version_def = graph_def.versions();
  NodeDefMovingGraphConstructor c(opts, std::move(graph_def), g, refiner, return_tensors, return_nodes, missing_unused_input_map_keys);

  Status s = c.TryImport();
  if (!s.ok()) {
    c.Undo();
    s = MaybeAppendVersionWarning(&version_def, s);
  }
  return s;
}

void GraphConstructor::UpdatePendingCountAndReady(int processed, bool is_next_iteration) {
  for (size_t i = 0; i < outputs_[processed].size(); ++i) {
    const int output = outputs_[processed][i];
    
    
    bool is_next_iteration_to_merge_edge = is_next_iteration && merge_node_indices_.count(output) == 1;
    if (!is_next_iteration_to_merge_edge) {
      int* current_pending_count = &pending_count_[output];
      CHECK_GT(*current_pending_count, 0);
      (*current_pending_count)--;
      if (*current_pending_count == 0) {
        ready_.insert(output);
      }
    }
  }
}



bool NodeNameInValues(const std::map<TensorId, TensorId>& input_map, const StringPiece& node_name) {
  for (auto iter = input_map.begin(); iter != input_map.end(); ++iter) {
    if (iter->second.first == node_name) return true;
  }
  return false;
}

bool NodeNameInValues(const std::vector<string>& control_dependencies, const StringPiece& node_name) {
  return std::find(control_dependencies.begin(), control_dependencies.end(), node_name) != control_dependencies.end();
}



void AddPrefixes(StringPiece node_name, gtl::FlatSet<StringPiece, StringPieceHasher>* prefixes) {
  size_t idx = -1;
  while ((idx = node_name.find('/', idx + 1)) != StringPiece::npos) {
    prefixes->insert(node_name.substr(0, idx));
  }
}

Status GraphConstructor::EnsureNoNameCollisions() {
  existing_nodes_.reserve(g_->num_nodes());
  
  for (Node* n : g_->nodes()) {
    bool already_exists = !existing_nodes_.insert({n->name(), n}).second;
    if (already_exists) {
      if (NodeNameInValues(opts_.input_map, n->name())) {
        return errors::InvalidArgument( "cannot resolve input_map because multiple nodes exist with name '", n->name(), "'");

      }
      if (NodeNameInValues(opts_.control_dependencies, n->name())) {
        return errors::InvalidArgument( "cannot resolve control_dependencies because multiple nodes exist " "with name '", n->name(), "'");


      }
    }
    AddPrefixes(n->name(), &existing_prefixes_);
  }
  if (prefix_.empty() && opts_.importing && !opts_.uniquify_names) {
    for (size_t i = 0; i < node_def_count(); ++i) {
      const string& name = get_node_def(i).name();
      if (NameExistsInGraph(name)) {
        return errors::InvalidArgument("Node name '", name, "' already exists in the Graph");
      }
    }
  } else if (!prefix_.empty()) {
    StringPiece prefix_no_slash(prefix_);
    prefix_no_slash.remove_suffix(1);
    if (!IsValidNodeName(prefix_no_slash, false)) {
      return errors::InvalidArgument("Imported node name prefix '", prefix_, "' would lead to invalid node names");
    }
    if (NameExistsInGraph(prefix_no_slash) && opts_.uniquify_prefix) {
      prefix_ = strings::StrCat(FindUniqueName(prefix_no_slash), "/");
    }
  }
  return Status::OK();
}

Status GraphConstructor::ValidateInputMapAndControlDependencies() {
  for (const auto& mapping : opts_.input_map) {
    TensorId src = mapping.first;
    TensorId dst = mapping.second;
    if (existing_nodes_.count(dst.first) == 0) {
      return errors::InvalidArgument( "node '", dst.first, "' in input_map does not exist in graph ", "(input_map entry: ", src.ToString(), "->", dst.ToString(), ")");

    }
    if ((src.second == Graph::kControlSlot) != (dst.second == Graph::kControlSlot)) {
      return errors::InvalidArgument("input_map entry ", src.ToString(), "->", dst.ToString(), " between ", "control edge and non-control edge");

    }
  }
  for (const string& node : opts_.control_dependencies) {
    if (existing_nodes_.count(node) == 0) {
      return errors::InvalidArgument( "node '", node, "' in control_dependencies does not exist in " "graph");


    }
  }
  return Status::OK();
}

Status GraphConstructor::BuildNodeIndex() {
  
  for (int n = 0; n < node_def_count(); ++n) {
    const NodeDef& node_def = get_node_def(n);
    if (!IsValidNodeName(node_def.name(), opts_.allow_internal_ops)) {
      return errors::InvalidArgument( "Node '", node_def.name(), "': Node name contains invalid characters");

    }
    if (!gdef_nodes_ .insert(std::make_pair(StringPiece(node_def.name()), NodeInfo(n)))
             .second) {
      return errors::InvalidArgument("Node '", node_def.name(), "' is not unique");
    }
    
    if (node_def.op().empty()) {
      return errors::InvalidArgument("Node '", node_def.name(), "' does not specify an operation");
    }
    if (opts_.expect_device_spec && node_def.device().empty()) {
      return errors::InvalidArgument("Node '", node_def.name(), "' is missing a device specification");
    }
    if (IsMerge(node_def)) {
      merge_node_indices_.insert(n);
    }
    
    bool in_control_dependence = false;
    for (int i = 0; i < node_def.input_size(); ++i) {
      StringPiece input_name = node_def.input(i);
      if (!input_name.empty() && absl::StartsWith(input_name, "^")) {
        in_control_dependence = true;
      } else if (in_control_dependence) {
        return errors::InvalidArgument( "Node '", node_def.name(), "': Control dependencies must come after regular dependencies");

      }
    }
    
    AddPrefixes(node_def.name(), &gdef_prefixes_);
  }
  return Status::OK();
}

Status GraphConstructor::InitFromEdges() {
  const int num_nodes = node_def_count();
  pending_count_.reserve(num_nodes);
  outputs_.resize(num_nodes);
  gtl::FlatSet<string> next_iteration_nodes;
  for (int n = 0; n < node_def_count(); ++n) {
    const NodeDef& node_def = get_node_def(n);
    if (IsNextIteration(node_def)) {
      next_iteration_nodes.insert(node_def.name());
    }
  }

  
  for (int n = 0; n < num_nodes; ++n) {
    const NodeDef& node_def = get_node_def(n);
    int pending_count = node_def.input_size();
    if (IsMerge(node_def)) {
      
      
      
      
      int32 num_control_edges = 0;
      bool has_loop_back_edge = false;
      for (int i = 0; i < node_def.input_size(); ++i) {
        StringPiece input_name(node_def.input(i));
        if (absl::StartsWith(input_name, "^")) {
          num_control_edges++;
        } else {
          TensorId id(ParseTensorName(input_name));
          if (next_iteration_nodes.find(string(id.first)) != next_iteration_nodes.end()) {
            has_loop_back_edge = true;
          }
        }
      }
      if (has_loop_back_edge) {
        pending_count = num_control_edges + 1;
      }
    }
    for (int i = 0; i < node_def.input_size(); ++i) {
      StringPiece input_name = node_def.input(i);
      TensorId id(ParseTensorName(input_name));
      if (opts_.input_map.count(id) == 0) {
        
        
        auto iter = gdef_nodes_.find(id.first);
        if (iter == gdef_nodes_.end()) {
          return errors::InvalidArgument("Node '", node_def.name(), "': Unknown input node '", node_def.input(i), "'");

        }
        outputs_[iter->second.gdef_index].push_back(n);
      } else {
        
        
        --pending_count;
        DCHECK_GE(pending_count, 0);
      }
    }
    if (pending_count == 0) {
      ready_.insert(n);
    }
    pending_count_.push_back(pending_count);
  }
  return Status::OK();
}

Status GraphConstructor::ValidateColocationConstraints( const NodeDef& node_def) {
  if (!opts_.validate_colocation_constraints || !opts_.importing)
    return Status::OK();
  const auto iter = node_def.attr().find(kColocationAttrName);
  if (iter == node_def.attr().end()) return Status::OK();
  for (const string& c : iter->second.list().s()) {
    StringPiece s(c);
    if (absl::ConsumePrefix(&s, kColocationGroupPrefix) && gdef_nodes_.find(s) == gdef_nodes_.end()) {
      return errors::InvalidArgument( "Node '", node_def.name(), "' expects to be colocated with unknown node '", s, "'");

    }
  }
  return Status::OK();
}

Status GraphConstructor::MakeNode(NodeDef&& node_def, Node** node) {
  
  Status status;
  *node = g_->AddNode(std::move(node_def), &status);
  if (!status.ok()) return status;
  if (opts_.expect_device_spec) {
    (*node)->set_assigned_device_name((*node)->def().device());
  }
  return Status::OK();
}

Status GraphConstructor::ValidateShape(Node* node) {
  if (!opts_.importing || !opts_.validate_shape) return Status::OK();
  TF_RETURN_IF_ERROR(refiner_->AddNode(node));
  
  std::vector<const TensorShapeProto*> shape_attrs;
  const char* kAttrName = "_output_shapes";
  if (!TryGetNodeAttr(node->attrs(), kAttrName, &shape_attrs)) {
    
    return Status::OK();
  }
  auto* ic = refiner_->GetContext(node);
  DCHECK(ic != nullptr)
      << "ShapeRefiner::AddNode() should have created the InferenceContext";
  if (shape_attrs.size() < node->num_outputs()) {
    return errors::InvalidArgument( "Node '", node->name(), "' has ", node->num_outputs(), " outputs but the ", kAttrName, " attribute specifies shapes for ", shape_attrs.size(), " outputs");


  }
  
  
  
  if (shape_attrs.size() > node->num_outputs()) {
    LOG(WARNING) << "Node '" << node->name() << "' has " << node->num_outputs()
                 << " outputs but the " << kAttrName << " attribute specifies shapes for " << shape_attrs.size()
                 << " outputs. Output shapes may be inaccurate.";
  }
  for (int i = 0; i < node->num_outputs(); ++i) {
    const TensorShapeProto& p = *shape_attrs[i];
    shape_inference::ShapeHandle h;
    Status s = ic->MakeShapeFromShapeProto(p, &h);
    if (!s.ok()) {
      return errors::InvalidArgument("Node '", node->name(), " has an invalid ", kAttrName, " attribute (shape #", i, " error:'", s.error_message(), "'");

    }
    s = refiner_->SetShape(node, i, h);
    if (!s.ok()) {
      return errors::InvalidArgument( "Node '", node->name(), "' has an ", kAttrName, " attribute inconsistent with the GraphDef for output #", i, ": ", s.error_message());


    }
  }
  node->ClearAttr(kAttrName);
  return Status::OK();
}

Status GraphConstructor::ModifyNodeDefForImport(NodeDef* node_def) {
  const OpDef* op_def;
  TF_RETURN_IF_ERROR(g_->op_registry()->LookUpOpDef(node_def->op(), &op_def));
  AddDefaultsToNodeDef(*op_def, node_def);
  TF_RETURN_IF_ERROR(ValidateNodeDef(*node_def, *op_def));
  if (versions()) {
    TF_RETURN_IF_ERROR(CheckOpDeprecation(*op_def, versions()->producer()));
  }
  return Status::OK();
}

void RemoveInputs(const std::vector<int>& inputs_to_remove, NodeDef* node_def, std::vector<bool>* input_already_exists) {
  
  NodeDef copy;
  copy.mutable_input()->Reserve(node_def->input_size() - inputs_to_remove.size());
  for (int i = 0, j = 0; i < node_def->input_size(); ++i) {
    if (j < inputs_to_remove.size() && i == inputs_to_remove[j]) {
      ++j;
    } else {
      copy.add_input()->swap(*node_def->mutable_input(i));
    }
  }
  node_def->mutable_input()->Swap(copy.mutable_input());
  
  for (int idx : inputs_to_remove) {
    input_already_exists->erase(input_already_exists->begin() + idx);
  }
  DCHECK_EQ(input_already_exists->size(), node_def->input_size());
}

void GraphConstructor::RemapNodeDefInputs( NodeDef* node_def, std::vector<bool>* input_already_exists) {
  DCHECK_EQ(input_already_exists->size(), node_def->input_size());
  std::set<TensorId> control_inputs;
  std::vector<int> inputs_to_remove;

  for (int i = 0; i < node_def->input_size(); ++i) {
    auto iter = opts_.input_map.find(ParseTensorName(node_def->input(i)));
    if (iter == opts_.input_map.end()) continue;
    used_input_map_keys_.insert(iter->first);

    TensorId new_input = iter->second;
    if (new_input.second == Graph::kControlSlot) {
      
      
      if (control_inputs.count(new_input) > 0) {
        inputs_to_remove.push_back(i);
        continue;
      }
      control_inputs.insert(new_input);
    }
    node_def->set_input(i, new_input.ToString());
    (*input_already_exists)[i] = true;
  }
  if (!inputs_to_remove.empty()) {
    RemoveInputs(inputs_to_remove, node_def, input_already_exists);
  }
}

void GraphConstructor::AddControlDependencies( NodeDef* node_def, std::vector<bool>* input_already_exists) {
  
  
  bool inherits_deps = false;
  for (int i = 0; i < node_def->input_size(); ++i) {
    
    
    
    if ((*input_already_exists)[i]) continue;

    
    
    
    TensorId id(ParseTensorName(node_def->input(i)));
    auto iter = gdef_nodes_.find(id.first);
    DCHECK(iter != gdef_nodes_.end()) << id.first;
    if (iter->second.node == nullptr) {
      
      continue;
    }
    inherits_deps = true;
  }
  if (inherits_deps) return;

  
  
  for (const string& control_dep : opts_.control_dependencies) {
    string input = TensorId(control_dep, Graph::kControlSlot).ToString();
    bool found = false;
    for (int i = node_def->input_size() - 1; i >= 0; --i) {
      const string& node_input = node_def->input(i);
      if (node_input[0] != '^') {
        
        
        break;
      }
      if (node_input == input) {
        
        found = true;
        break;
      }
    }
    if (found) {
      continue;
    }
    node_def->add_input(input);
    input_already_exists->push_back(true);
  }
}

void GraphConstructor::AddPrefixToNodeDef( const std::vector<bool>& input_already_exists, NodeDef* node_def) {
  if (prefix_.empty()) return;
  node_def->set_name(strings::StrCat(prefix_, node_def->name()));
  
  for (int i = 0; i < node_def->input_size(); ++i) {
    
    
    if (input_already_exists[i]) continue;
    StringPiece input(node_def->input(i));
    if (absl::ConsumePrefix(&input, "^")) {
      node_def->set_input(i, strings::StrCat("^", prefix_, input));
    } else {
      node_def->set_input(i, strings::StrCat(prefix_, input));
    }
  }
  
  if (node_def->attr().find(kColocationAttrName) != node_def->attr().end()) {
    auto* list = node_def->mutable_attr()->at(kColocationAttrName).mutable_list();
    for (int i = 0; i < list->s_size(); ++i) {
      StringPiece v(list->s(i));
      if (absl::ConsumePrefix(&v, kColocationGroupPrefix)) {
        list->set_s(i, strings::StrCat(kColocationGroupPrefix, prefix_, v));
      }
    }
  }
}

void GraphConstructor::UniquifyNames( const std::vector<bool>& input_already_exists, NodeDef* node_def) {
  if (NameExistsInGraph(node_def->name())) {
    string old_name = node_def->name();
    node_def->set_name(FindUniqueName(node_def->name()));
    uniquified_names_[old_name] = node_def->name();
    
    
    
  }
  for (int i = 0; i < node_def->input_size(); ++i) {
    
    
    if (input_already_exists[i]) continue;
    TensorId id = ParseTensorName(node_def->input(i));
    
    
    
    auto iter = uniquified_names_.find(string(id.first));
    if (iter == uniquified_names_.end()) continue;
    id.first = iter->second;
    node_def->set_input(i, id.ToString());
  }
}

void GraphConstructor::UpdateUniquifiedColocationNames() {
  for (const auto& pair : gdef_nodes_) {
    Node* node = pair.second.node;
    if (node == nullptr) continue;
    std::vector<string> coloc_values;
    if (!TryGetNodeAttr(node->attrs(), kColocationAttrName, &coloc_values))
      continue;
    bool updated = false;
    for (size_t i = 0; i < coloc_values.size(); ++i) {
      StringPiece val(coloc_values[i]);
      if (absl::ConsumePrefix(&val, kColocationGroupPrefix)) {
        auto name_pair = uniquified_names_.find(string(val));
        if (name_pair == uniquified_names_.end()) continue;
        updated = true;
        coloc_values[i] = strings::StrCat(kColocationGroupPrefix, name_pair->second);
      }
    }
    if (updated) {
      node->AddAttr(kColocationAttrName, std::move(coloc_values));
    }
  }
}

bool GraphConstructor::NameExistsInGraph(StringPiece name) {
  if (existing_nodes_.find(name) != existing_nodes_.end()) return true;
  if (existing_prefixes_.find(name) != existing_prefixes_.end()) return true;
  return false;
}

bool GraphConstructor::NameExistsInGraphDef(StringPiece name) {
  if (gdef_nodes_.find(name) != gdef_nodes_.end()) return true;
  if (gdef_prefixes_.find(name) != gdef_prefixes_.end()) return true;
  return false;
}

string GraphConstructor::FindUniqueName(StringPiece original_name) {
  string name(original_name);
  int count = 0;
  
  
  while (NameExistsInGraph(name) || (count > 0 && NameExistsInGraphDef(name))) {
    name = strings::StrCat(original_name, "_", ++count);
  }
  return name;
}

Status GraphConstructor::IsNodeFullyMapped(const NodeDef& node_def, bool* is_node_mapped) {
  const OpDef* op_def;
  TF_RETURN_IF_ERROR(g_->op_registry()->LookUpOpDef(node_def.op(), &op_def));
  for (int i = 0; i < op_def->output_arg_size(); ++i) {
    if (opts_.input_map.find({node_def.name(), i}) == opts_.input_map.end()) {
      *is_node_mapped = false;
      return Status::OK();
    }
  }
  *is_node_mapped = true;
  return Status::OK();
}

void GraphConstructor::DFS(int cur_node, std::vector<int>* cur_branch, std::vector<bool>* is_on_cur_branch, absl::flat_hash_set<int>* unvisited) {

  cur_branch->push_back(cur_node);
  is_on_cur_branch->at(cur_node) = true;
  for (auto next_node : outputs_[cur_node]) {
    if (unvisited->find(next_node) != unvisited->end()) {
      if (is_on_cur_branch->at(next_node)) {
        auto iter = std::find(cur_branch->begin(), cur_branch->end(), next_node);
        LOG(WARNING) << "Cycle detected:";
        while (iter != cur_branch->end()) {
          LOG(WARNING) << SummarizeNodeDef(get_node_def(*iter));
          ++iter;
        }
        LOG(WARNING) << "End of cycle";
      } else {
        DFS(next_node, cur_branch, is_on_cur_branch, unvisited);
      }
    }
  }
  cur_branch->pop_back();
  is_on_cur_branch->at(cur_node) = false;
  unvisited->erase(cur_node);
}

void GraphConstructor::PrintCycles() {
  int num_nodes = outputs_.size();
  absl::flat_hash_set<int> unvisited;
  for (int i = 0; i < num_nodes; i++) {
    unvisited.insert(i);
  }
  while (!unvisited.empty()) {
    int cur_node = *unvisited.begin();
    
    
    std::vector<int> cur_branch;
    
    
    
    
    std::vector<bool> is_on_cur_branch(num_nodes, false);
    DFS(cur_node, &cur_branch, &is_on_cur_branch, &unvisited);
  }
}

Status GraphConstructor::Convert() {
  
  
  if (library()) {
    
    
    TF_RETURN_IF_ERROR(g_->AddFunctionLibrary(*library()));
  }

  std::vector<InputInfo> inputs;
  int processed = 0;

  std::vector<bool> input_already_exists;

  
  
  
  
  while (!ready_.empty()) {
    int o = *ready_.begin();
    ready_.erase(ready_.begin());
    ++processed;
    inputs.clear();
    bool has_data_back_edge = false;

    NodeDef node_def = consume_node_def(o);

    
    
    
    
    input_already_exists.clear();
    input_already_exists.resize(node_def.input_size(), false);

    ssize_t string_intern_table_index = -1;

    if (opts_.importing) {
      
      
      string_intern_table_index = string_intern_table_.size();
      string_intern_table_.push_back(node_def.name());

      if (opts_.skip_mapped_nodes) {
        bool is_node_mapped = false;
        TF_RETURN_IF_ERROR(IsNodeFullyMapped(node_def, &is_node_mapped));
        if (is_node_mapped) {
          
          UpdatePendingCountAndReady(o, IsNextIteration(node_def));
          continue;
        }
      }

      if (!opts_.input_map.empty()) {
        
        RemapNodeDefInputs(&node_def, &input_already_exists);
      }
      if (!opts_.control_dependencies.empty()) {
        
        AddControlDependencies(&node_def, &input_already_exists);
      }
      if (!opts_.default_device.empty() && node_def.device().empty()) {
        node_def.set_device(opts_.default_device);
      }
    }

    DCHECK_EQ(node_def.input_size(), input_already_exists.size());
    TF_RETURN_IF_ERROR(ValidateColocationConstraints(node_def));
    for (int i = 0; i < node_def.input_size(); ++i) {
      TensorId tensor_id = ParseTensorName(node_def.input(i));
      Node* src_node;
      int src_index;

      if (!input_already_exists[i]) {
        
        auto iter = gdef_nodes_.find(tensor_id.node());
        DCHECK(iter != gdef_nodes_.end()) << tensor_id.node();
        src_node = iter->second.node;
        src_index = tensor_id.index();
        if (src_node == nullptr) has_data_back_edge = true;
      } else {
        
        auto iter = existing_nodes_.find(tensor_id.node());
        DCHECK(iter != existing_nodes_.end()) << tensor_id.node();
        src_node = iter->second;
        src_index = tensor_id.index();
      }

      if (src_node != nullptr && src_index >= src_node->num_outputs()) {
        std::ostringstream out;
        out << "Node '" << node_def.name() << "': Connecting to invalid output " << tensor_id.index() << " of source node " << tensor_id.node()
            << " which has " << src_node->num_outputs() << " outputs.";

        if (src_node->type_string() == "If" || src_node->type_string() == "StatelessIf" || src_node->type_string() == "While" || src_node->type_string() == "StatelessWhile") {


          out << " Try using " << "tf.compat.v1.experimental.output_all_intermediates(True).";
        }
        return errors::InvalidArgument(out.str());
      }

      inputs.emplace_back(string(tensor_id.node()), src_node, src_index);
    }

    if (has_data_back_edge && !IsMerge(node_def)) {
      return errors::InvalidArgument( "Node '", node_def.name(), "' had a back edge, but only Merge nodes can have back edges.");

    }

    Node* node;
    if (opts_.importing) {
      if (!prefix_.empty()) {
        AddPrefixToNodeDef(input_already_exists, &node_def);
      }
      
      
      if (opts_.uniquify_names && (prefix_.empty() || !opts_.uniquify_prefix)) {
        UniquifyNames(input_already_exists, &node_def);
      }
    }

    if (opts_.importing) {
      TF_RETURN_IF_ERROR(ModifyNodeDefForImport(&node_def));
    } else {
      const OpDef* op_def;
      TF_RETURN_IF_ERROR( g_->op_registry()->LookUpOpDef(node_def.op(), &op_def));
      if (opts_.add_default_attributes) {
        AddDefaultsToNodeDef(*op_def, &node_def);
      }
      if (opts_.validate_nodes) {
        TF_RETURN_IF_ERROR(ValidateNodeDef(node_def, *op_def));
      }
    }

    TF_RETURN_IF_ERROR(MakeNode(std::move(node_def), &node));

    if (opts_.importing) {
      
      DCHECK_GE(string_intern_table_index, 0);
      gdef_nodes_[string_intern_table_[string_intern_table_index]].node = node;
    } else {
      DCHECK_EQ(string_intern_table_index, -1);
      gdef_nodes_[node->name()].node = node;
    }

    
    
    auto first_control = absl::c_find_if(inputs, &InputInfo::IsControlInput);
    auto first_control_copy = first_control;
    std::sort(first_control, inputs.end(), &InputInfo::CompareName);
    inputs.erase( std::unique(first_control_copy, inputs.end(), &InputInfo::IsSameName), inputs.end());


    
    for (size_t i = 0; i < inputs.size(); ++i) {
      if (inputs[i].node == nullptr) {
        
        
        back_edges_.emplace_back(inputs[i].name, inputs[i].index, node, i);
      } else if (inputs[i].index == Graph::kControlSlot) {
        g_->AddControlEdge(inputs[i].node, node, kDoNotCheckDuplicates);
      } else {
        TF_RETURN_IF_ERROR(MakeEdge(inputs[i].node, inputs[i].index, node, i));
      }
    }

    TF_RETURN_IF_ERROR(ValidateShape(node));

    
    UpdatePendingCountAndReady(o, node->IsNextIteration());
  }

  if (processed < node_def_count()) {
    LOG(WARNING) << "IN " << __func__ << " " << (node_def_count() - processed)
                 << " NODES IN A CYCLE";
    for (int64 i = 0; i < node_def_count(); i++) {
      if (pending_count_[i] != 0) {
        LOG(WARNING) << "PENDING: " << SummarizeNodeDef(get_node_def(i))
                     << " WITH PENDING COUNT = " << pending_count_[i];
      }
    }
    PrintCycles();
    return errors::InvalidArgument(node_def_count() - processed, " nodes in a cycle");
  }

  return Status::OK();
}

Status GraphConstructor::AddBackEdges() {
  
  for (const auto& e : back_edges_) {
    Node* src_node = gdef_nodes_[e.src_name].node;
    if (e.src_index == Graph::kControlSlot) {
      g_->AddControlEdge(src_node, e.dst_node, kDoNotCheckDuplicates);
    } else {
      TF_RETURN_IF_ERROR( MakeEdge(src_node, e.src_index, e.dst_node, e.dst_index));
    }

    VLOG(2) << "Add back edge: " << src_node->name() << " -> " << e.dst_node->name();
  }
  return Status::OK();
}

Status GraphConstructor::UpdateVersionDef() {
  if (versions() == nullptr) return Status::OK();

  if (!opts_.importing) {
    g_->set_versions(*versions());
    return Status::OK();
  }
  VersionDef g_versions = g_->versions();
  g_versions.set_producer( std::min(g_versions.producer(), versions()->producer()));
  g_versions.set_min_consumer( std::max(g_versions.min_consumer(), versions()->min_consumer()));
  if (versions()->bad_consumers_size() > 0) {
    std::set<int> bad(g_versions.bad_consumers().begin(), g_versions.bad_consumers().end());
    bad.insert(versions()->bad_consumers().begin(), versions()->bad_consumers().end());
    g_versions.clear_bad_consumers();
    for (int v : bad) {
      g_versions.add_bad_consumers(v);
    }
  }
  g_->set_versions(g_versions);
  return Status::OK();
}

Status GraphConstructor::PopulateReturnTensors() {
  if (opts_.return_tensors.empty()) return Status::OK();
  for (const TensorId& id : opts_.return_tensors) {
    auto iter = opts_.input_map.find(id);
    if (iter == opts_.input_map.end()) {
      
      auto iter = gdef_nodes_.find(id.first);
      if (iter == gdef_nodes_.end()) {
        return errors::InvalidArgument("Requested return tensor '", id.ToString(), "' not found in graph def");

      }
      int num_outputs = iter->second.node->num_outputs();
      if ((id.second < 0 || id.second >= num_outputs) && id.second != Graph::kControlSlot) {
        return errors::InvalidArgument("Invalid return output ", id.second, " of node '", id.first, "', which has ", num_outputs, " output(s)");

      }
      return_tensors_->push_back({iter->second.node, id.second});
    } else {
      
      TensorId remapped_id = iter->second;
      DCHECK_GT(existing_nodes_.count(remapped_id.first), 0);
      Node* node = existing_nodes_[remapped_id.first];
      return_tensors_->push_back({node, remapped_id.second});
    }
  }
  return Status::OK();
}

Status GraphConstructor::PopulateReturnNodes() {
  if (opts_.return_nodes.empty()) return Status::OK();
  for (StringPiece name : opts_.return_nodes) {
    auto iter = gdef_nodes_.find(name);
    if (iter == gdef_nodes_.end()) {
      return errors::InvalidArgument("Requested return node '", name, "' not found in graph def");
    }
    return_nodes_->push_back(iter->second.node);
  }
  return Status::OK();
}

Status GraphConstructor::PopulateMissingUnusedInputMapKeys() {
  if (missing_unused_input_map_keys_ == nullptr) return Status::OK();
  for (const auto& input_map_pair : opts_.input_map) {
    TensorId key = input_map_pair.first;
    if (used_input_map_keys_.count(key) > 0) continue;

    auto pair = gdef_nodes_.find(key.first);
    if (pair == gdef_nodes_.end()) {
      
      missing_unused_input_map_keys_->push_back(key);
      continue;
    }

    
    
    
    const NodeDef& node_def = get_node_def(pair->second.gdef_index);
    const OpDef* op_def;
    TF_RETURN_IF_ERROR(g_->op_registry()->LookUpOpDef(node_def.op(), &op_def));
    int num_outputs;
    TF_RETURN_IF_ERROR(NumOutputsForNode(node_def, *op_def, &num_outputs));
    if (key.second >= num_outputs) {
      
      missing_unused_input_map_keys_->push_back(key);
    }
  }
  return Status::OK();
}

void GraphConstructor::Undo() {
  for (const auto& iter : gdef_nodes_) {
    if (iter.second.node != nullptr) {
      g_->RemoveNode(iter.second.node);
    }
  }
  g_->set_versions(original_versions_);
}

Status GraphConstructor::MakeEdge(Node* src, int output_index, Node* dst, int input_index) {
  DataType src_out = src->output_type(output_index);
  DataType dst_in = dst->input_type(input_index);
  if (!TypesCompatible(dst_in, src_out)) {
    return errors::InvalidArgument( "Input ", input_index, " of node ", dst->name(), " was passed ", DataTypeString(src_out), " from ", src->name(), ":", output_index, " incompatible with expected ", DataTypeString(dst_in), ".");


  }
  g_->AddEdge(src, output_index, dst, input_index);
  return Status::OK();
}

}  

Status ConvertGraphDefToGraph(const GraphConstructorOptions& opts, const GraphDef& gdef, Graph* g) {
  ShapeRefiner refiner(gdef.versions().producer(), g->op_registry());
  return GraphConstructor::Construct( opts, gdef.node(), &gdef.versions(), &gdef.library(), g, &refiner, nullptr, nullptr, nullptr);


}

Status ConvertGraphDefToGraph(const GraphConstructorOptions& opts, GraphDef&& gdef, Graph* g) {
  ShapeRefiner refiner(gdef.versions().producer(), g->op_registry());
  return GraphConstructor::Construct(opts, std::move(gdef), g, &refiner, nullptr, nullptr, nullptr);


}

Status ConvertNodeDefsToGraph(const GraphConstructorOptions& opts, gtl::ArraySlice<NodeDef> nodes, Graph* g) {
  ShapeRefiner refiner(TF_GRAPH_DEF_VERSION, g->op_registry());
  
  std::vector<const NodeDef*> node_defs;
  node_defs.reserve(nodes.size());
  for (const auto& n : nodes) {
    node_defs.push_back(&n);
  }
  return GraphConstructor::Construct(opts, node_defs, nullptr, nullptr, g, &refiner, nullptr, nullptr, nullptr);


}

Status ImportGraphDef(const ImportGraphDefOptions& opts, const GraphDef& gdef, Graph* g, ShapeRefiner* refiner, ImportGraphDefResults* results) {

  if (!opts.return_tensors.empty()) {
    if (results == nullptr) {
      return errors::InvalidArgument( "results argument to ImportGraphDef() must be non-null if " "opts.return_tensors is non-empty");

    }
  }

  if (!opts.return_nodes.empty()) {
    if (opts.skip_mapped_nodes) {
      return errors::InvalidArgument( "Requesting return_nodes with skip_mapped_nodes set is not currently " "supported");

    }
    if (results == nullptr) {
      return errors::InvalidArgument( "results argument to ImportGraphDef() must be non-null if " "opts.return_nodes is non-empty");

    }
  }

  if (results != nullptr) {
    if (!results->return_tensors.empty() || !results->return_nodes.empty() || !results->missing_unused_input_map_keys.empty()) {
      return errors::InvalidArgument( "All fields in results argument to ImportGraphDef() must be empty.");
    }
  }

  ShapeRefiner default_refiner(gdef.versions().producer(), g->op_registry());
  if (refiner == nullptr) {
    refiner = &default_refiner;
  } else {
    
    
    
    if (gdef.versions().producer() > 0 && gdef.versions().producer() < refiner->graph_def_version() && g->num_nodes() > 2) {

      LOG(WARNING) << "Importing a graph with a lower producer version " << gdef.versions().producer()
                   << " into an existing graph with producer version " << refiner->graph_def_version() << ". Shape inference will " << "have run different parts of the graph with different " << "producer versions.";


    }
  }

  
  
  
  
  
  
  
  refiner->set_graph_def_version( std::min(refiner->graph_def_version(), gdef.versions().producer()));

  if (results == nullptr) {
    return GraphConstructor::Construct(opts, gdef.node(), &gdef.versions(), &gdef.library(), g, refiner, nullptr, nullptr, nullptr);

  } else {
    return GraphConstructor::Construct( opts, gdef.node(), &gdef.versions(), &gdef.library(), g, refiner, &results->return_tensors, &results->return_nodes, &results->missing_unused_input_map_keys);


  }
}

void CopyGraph(const Graph& src, Graph* dest) {
  dest->SetConstructionContext(src.GetConstructionContextInternal());

  for (Node* n : dest->nodes()) {
    CHECK(n->IsSource() || n->IsSink()) << "*dest must be empty";
  }

  
  dest->set_versions(src.versions());

  
  
  gtl::FlatMap<const Node*, Node*> node_map;
  node_map[src.source_node()] = dest->source_node();
  node_map[src.sink_node()] = dest->sink_node();
  for (Node* n : src.op_nodes()) {
    node_map[n] = dest->CopyNode(n);
  }

  
  for (const Edge* e : src.edges()) {
    Node* src_copy = node_map[e->src()];
    Node* dst_copy = node_map[e->dst()];
    dest->AddEdge(src_copy, e->src_output(), dst_copy, e->dst_input());
  }
}

}  

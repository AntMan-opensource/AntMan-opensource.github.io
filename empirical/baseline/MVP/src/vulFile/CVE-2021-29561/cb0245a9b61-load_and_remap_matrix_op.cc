

















namespace tensorflow {

namespace {


Status RemapVectorToMap(const TTypes<const int64>::Vec& remapping, std::vector<bool>* id_present, std::unordered_map<int64, int64>* old_id_to_new_id) {

  id_present->clear();
  id_present->resize(remapping.size(), false);
  for (int i = 0; i < remapping.size(); ++i) {
    const int64 old_id = remapping(i);
    if (old_id < 0) continue;
    (*id_present)[i] = true;
    if (!gtl::InsertIfNotPresent(old_id_to_new_id, old_id, i)) {
      return errors::Unimplemented( strings::StrCat("Old ID ", old_id, " is mapped to both new ID ", old_id_to_new_id->at(old_id), " and ", i, ", which is not supported."));


    }
  }
  return Status::OK();
}
}  




class LoadAndRemapMatrixOp : public OpKernel {
 public:
  explicit LoadAndRemapMatrixOp(OpKernelConstruction* context)
      : OpKernel(context) {
    OP_REQUIRES_OK(context, context->GetAttr("num_rows", &num_rows_));
    OP_REQUIRES_OK(context, context->GetAttr("num_cols", &num_cols_));
    OP_REQUIRES_OK( context, context->GetAttr("max_rows_in_memory", &max_rows_in_memory_));
  }

  void Compute(OpKernelContext* context) override {
    
    
    std::unordered_map<int64, int64> old_row_to_new_row_map;
    std::vector<bool> row_id_present;
    const Tensor* row_remapping_t;
    OP_REQUIRES_OK(context, context->input("row_remapping", &row_remapping_t));
    const auto row_remapping = row_remapping_t->vec<int64>();
    OP_REQUIRES(context, row_remapping.size() == num_rows_, errors::InvalidArgument(strings::StrCat( "Size of row_remapping is ", row_remapping.size(), " instead of being equal to num_rows=", num_rows_)));


    OP_REQUIRES_OK(context, RemapVectorToMap(row_remapping, &row_id_present, &old_row_to_new_row_map));

    
    
    int64 min_old_row = -1;
    int64 max_old_row = -1;
    for (int i = 0; i < row_remapping.size(); ++i) {
      if (min_old_row < 0 || (row_remapping(i) >= 0 && row_remapping(i) < min_old_row)) {
        min_old_row = row_remapping(i);
      }
      if (max_old_row < 0 || (row_remapping(i) >= 0 && row_remapping(i) > max_old_row)) {
        max_old_row = row_remapping(i);
      }
    }

    
    std::unordered_map<int64, int64> old_col_to_new_col_map;
    std::vector<bool> col_id_present;
    const Tensor* col_remapping_t;
    OP_REQUIRES_OK(context, context->input("col_remapping", &col_remapping_t));
    const auto col_remapping = col_remapping_t->vec<int64>();
    
    
    
    const bool remap_cols = col_remapping.size() > 0;
    if (remap_cols) {
      OP_REQUIRES( context, col_remapping.size() == num_cols_, errors::InvalidArgument(strings::StrCat( "Provided col_remapping, but its size is ", col_remapping.size(), " instead of being equal to num_cols=", num_cols_)));



      OP_REQUIRES_OK(context, RemapVectorToMap(col_remapping, &col_id_present, &old_col_to_new_col_map));
    } else {
      col_id_present.clear();
      col_id_present.resize(num_cols_, true);
    }

    
    const Tensor* ckpt_path_t;
    OP_REQUIRES_OK(context, context->input("ckpt_path", &ckpt_path_t));
    const string& ckpt_path = ckpt_path_t->scalar<tstring>()();
    const Tensor* old_tensor_name_t;
    OP_REQUIRES_OK(context, context->input("old_tensor_name", &old_tensor_name_t));
    const string& old_tensor_name = old_tensor_name_t->scalar<tstring>()();

    LOG(INFO) << "Processing checkpoint : " << ckpt_path;
    BundleReader reader(context->env(), ckpt_path);
    OP_REQUIRES_OK(context, reader.status());

    DataType tensor_type;
    TensorShape tensor_shape;
    OP_REQUIRES_OK(context, reader.LookupDtypeAndShape( old_tensor_name, &tensor_type, &tensor_shape));
    OP_REQUIRES(context, tensor_type == DT_FLOAT, errors::InvalidArgument(strings::StrCat( "Tensor ", old_tensor_name, " has invalid type ", DataTypeString(tensor_type), " instead of expected type ", DataTypeString(DT_FLOAT))));



    
    OP_REQUIRES( context, tensor_shape.dims() == 2, errors::InvalidArgument(strings::StrCat( "Tensor ", old_tensor_name, " has shape ", tensor_shape.DebugString(), " of invalid rank ", tensor_shape.dims(), " instead of expected shape of rank 2.")));





    if (!remap_cols) {
      
      
      
      OP_REQUIRES(context, num_cols_ == tensor_shape.dim_size(1), errors::InvalidArgument(strings::StrCat( "Tensor ", old_tensor_name, " has shape ", tensor_shape.DebugString(), ", where the size of its 2nd dimension is ", tensor_shape.dim_size(1), " instead of being equal to num_cols=", num_cols_)));





    }

    
    
    std::vector<TensorSlice> tensor_slices;
    TensorSlice slice(tensor_shape.dims());
    if (min_old_row >= 0 && max_old_row >= 0) {
      int64 row_start = min_old_row;
      
      
      
      
      while (row_start <= max_old_row) {
        const int64 slice_length = max_rows_in_memory_ <= 0  ? max_old_row - row_start + 1 : std::min(max_rows_in_memory_, max_old_row - row_start + 1);



        slice.set_start(0, row_start);
        slice.set_length(0, slice_length);
        tensor_slices.push_back(slice);
        row_start += slice_length;
      }
    }

    
    Tensor* output_matrix_t = nullptr;
    OP_REQUIRES_OK(context, context->allocate_output("output_matrix", TensorShape({num_rows_, num_cols_}), &output_matrix_t));


    auto output_matrix = output_matrix_t->matrix<float>();

    
    
    int64 row_index = min_old_row;
    int64 rows_copied = 0;
    Tensor loaded_tensor_t;
    for (const TensorSlice& tensor_slice : tensor_slices) {
      LOG(INFO) << "Loading slice " << tensor_slice.DebugString();
      TensorShape slice_shape;
      OP_REQUIRES_OK(context, tensor_slice.SliceTensorShape(tensor_shape, &slice_shape));
      
      
      if (loaded_tensor_t.shape() != slice_shape) {
        loaded_tensor_t = Tensor(DT_FLOAT, slice_shape);
      }
      OP_REQUIRES_OK(context, reader.LookupSlice(old_tensor_name, tensor_slice, &loaded_tensor_t));

      
      for (int row = 0; row < loaded_tensor_t.dim_size(0); ++row, ++row_index) {
        if (row_index % 500000 == min_old_row) {
          LOG(INFO) << "Processing old row " << row_index;
        }

        
        
        const int64* new_row_ptr = gtl::FindOrNull(old_row_to_new_row_map, row_index);
        if (new_row_ptr == nullptr) {
          continue;
        }
        ++rows_copied;
        const int64 new_row = *new_row_ptr;

        
        
        const auto& loaded_tensor = loaded_tensor_t.matrix<float>();
        for (int old_col = 0; old_col < loaded_tensor_t.dim_size(1);
             ++old_col) {
          int64 new_col = old_col;
          if (remap_cols) {
            const int64* new_col_ptr = gtl::FindOrNull(old_col_to_new_col_map, old_col);
            if (new_col_ptr == nullptr) {
              
              
              
              continue;
            }
            new_col = *new_col_ptr;
          }

          OP_REQUIRES(context, new_row < num_rows_ && new_col < num_cols_ && new_row >= 0 && new_col >= 0, errors::Internal(strings::StrCat( "new_row=", new_row, " and new_col=", new_col, " should have been less than num_rows_=", num_rows_, " and num_cols_=", num_cols_, " and non-negative. This should never have happened " "if the code were correct. Please file a bug.")));







          output_matrix(new_row, new_col) = loaded_tensor(row, old_col);
        }
      }
    }
    LOG(INFO) << "Copied " << rows_copied << " rows from old matrix (with " << tensor_shape.dim_size(0) << " rows) to new matrix (with " << num_rows_ << " rows).";


    
    
    
    
    const Tensor* initializing_values_t;
    OP_REQUIRES_OK( context, context->input("initializing_values", &initializing_values_t));
    const auto initializing_values = initializing_values_t->flat<float>();
    int64 initializing_values_index = 0;
    for (int i = 0; i < num_rows_; ++i) {
      for (int j = 0; j < num_cols_; ++j) {
        if (row_id_present[i] && col_id_present[j]) continue;
        OP_REQUIRES( context, initializing_values_index < initializing_values.size(), errors::InvalidArgument( "initializing_values contained ", initializing_values.size(), " elements, but more missing values remain."));



        output_matrix(i, j) = initializing_values(initializing_values_index);
        ++initializing_values_index;
      }
    }

    
    OP_REQUIRES( context, initializing_values_index == initializing_values.size(), errors::InvalidArgument( "initializing_values contained ", initializing_values.size(), " elements, but only ", initializing_values_index, " elements were used to fill in missing values."));




  }

 private:
  int64 num_rows_;
  int64 num_cols_;
  int64 max_rows_in_memory_;
};

REGISTER_KERNEL_BUILDER(Name("LoadAndRemapMatrix").Device(DEVICE_CPU), LoadAndRemapMatrixOp);

}  

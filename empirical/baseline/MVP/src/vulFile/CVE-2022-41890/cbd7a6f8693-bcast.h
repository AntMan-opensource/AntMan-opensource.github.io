











namespace tensorflow {







inline void ComputeBatchIndices(const int64_t output_batch_size, const gtl::InlinedVector<int64_t, 4>& reshape, const gtl::InlinedVector<int64_t, 4>& bcast, std::vector<int64_t>* out_indices) {


  
  
  
  
  
  out_indices->resize(output_batch_size);
  int64_t num_output_elements = 1;
  int64_t num_input_elements = 1;
  for (int64_t i = reshape.size() - 1; i >= 0; --i) {
    
    
    
    const int64_t dim = std::max(reshape[i], bcast[i]);
    const int64_t incr = bcast[i] > 1 ? 0 : num_input_elements;
    for (int64_t k = 0; k < (dim - 1) * num_output_elements; ++k) {
      (*out_indices)[num_output_elements + k] = (*out_indices)[k] + incr;
    }
    num_output_elements *= dim;
    num_input_elements *= reshape[i];
  }
}

template <int N> class BCastList {
 public:
  
  
  
  
  typedef gtl::InlinedVector<int64_t, 4> Vec;

  
  
  
  
  
  
  
  
  
  
  
  
  explicit BCastList(const Vec (&x)[N], const bool fewer_dims_optimization = true, const bool return_flattened_batch_indices = false);

  ~BCastList() {}

  
  
  bool IsValid() const { return valid_; }
  bool IsBroadcastingRequired() const { return broadcasting_required_; }

  
  
  
  const Vec& reshape(int i) const { return reshape_[i]; }
  const Vec& bcast(int i) const { return bcast_[i]; }
  const Vec& result_shape() const { return result_; }
  const Vec& output_shape() const { return output_; }
  const Vec& grad_reduce_idx(int i) const { return grad_reduce_idx_[i]; }
  const int64_t output_batch_size() const { return output_batch_size_; }

  
  
  
  
  
  
  const std::vector<int64_t>& batch_indices(int i) const {
    return batch_indices_[i];
  }

 protected:
  bool valid_ = true;
  bool broadcasting_required_ = true;
  Vec reshape_[N];
  Vec bcast_[N];
  Vec result_;
  Vec output_;
  Vec grad_reduce_idx_[N];

  int64_t output_batch_size_;
  std::vector<int64_t> batch_indices_[N];

  static void Reverse(Vec* shape) {
    std::reverse(shape->begin(), shape->end());
  }

  TF_DISALLOW_COPY_AND_ASSIGN(BCastList);
};

template <int N> BCastList<N>::BCastList(const BCastList::Vec (&x)[N], const bool fewer_dims_optimization, const bool return_flattened_batch_indices) {


  typedef BCastList::Vec Vec;

  
  auto mul_dims = [](int64_t dim1, int64_t dim2) -> int64 {
    return dim1 != 0 && dim2 != 0 && (dim1 < 0 || dim2 < 0) ? -1 : dim1 * dim2;
  };

  bool all_equal = true;
  size_t largest_rank = 0;
  output_batch_size_ = 1;
  for (int i = 0; i < N; ++i) {
    if (x[i] != x[0]) {
      all_equal = false;
    }
    if (x[i].size() > largest_rank) {
      largest_rank = x[i].size();
    }
  }
  if (all_equal) {
    broadcasting_required_ = false;
  }
  if (all_equal && TF_PREDICT_TRUE(fewer_dims_optimization)) {
    
    int64_t elements = 1;
    const int rank = x[0].size();
    output_.resize(rank);
    for (int i = 0; i < rank; i++) {
      const int64_t dim = x[0][i];
      elements = mul_dims(elements, dim);
      output_[i] = dim;
    }
    result_.push_back(elements);
    output_batch_size_ = elements;
    for (int i = 0; i < N; ++i) {
      reshape_[i].push_back(elements);
      bcast_[i].push_back(1);
    }
    
    return;
  }

  
  
  Vec copy[N];
  for (int i = 0; i < N; ++i) {
    copy[i] = x[i];
    Reverse(&copy[i]);
  }

  
  for (int i = 0; i < N; ++i) {
    if (copy[i].size() < largest_rank) {
      copy[i].resize(largest_rank, 1);
    }
  }
  
  
  

  
  bool prev_is_one[N];
  bool current_is_one[N];
  for (int i = 0; i < N; ++i) {
    prev_is_one[i] = false;
    current_is_one[i] = false;
  }
  Vec output;
  bool output_dim_set = false;
  int output_dim = -1;
  bool none_is_one = true;
  bool set_one = false;
  for (int j = 0; j < largest_rank; ++j) {
    output_dim = -1;
    output_dim_set = false;
    none_is_one = true;
    
    for (int i = 0; i < N; ++i) {
      
      if (copy[i][j] == 1) {
        current_is_one[i] = true;
        none_is_one = false;
      } else {
        current_is_one[i] = false;
        if (!output_dim_set || copy[i][j] == output_dim) {
          output_dim = copy[i][j];
          output_dim_set = true;
        } else {
          valid_ = false;
          return;
        }
      }
    }
    output_.push_back(output_dim_set ? output_dim : 1);
    output_batch_size_ = mul_dims(output_batch_size_, output_.back());
    
    if (!output_dim_set) {
      if (!TF_PREDICT_TRUE(fewer_dims_optimization)) {
        for (int i = 0; i < N; ++i) {
          bcast_[i].push_back(1);
          reshape_[i].push_back(1);
        }
        result_.push_back(1);
      }
      for (int i = 0; i < N; ++i) {
        grad_reduce_idx_[i].push_back(largest_rank - 1 - j);
      }
      
      
      
      
      
      
      
      
      
      
      
      
      
      

      continue;
    } else if (TF_PREDICT_TRUE(fewer_dims_optimization) && std::equal(current_is_one, current_is_one + N, prev_is_one) && set_one) {

      
      
      
      result_.back() = mul_dims(result_.back(), output_dim);
      for (int i = 0; i < N; ++i) {
        reshape_[i].back() = mul_dims(reshape_[i].back(), copy[i][j]);
        bcast_[i].back() = mul_dims(bcast_[i].back(), current_is_one[i] ? output_dim : 1);
        if (current_is_one[i] && !none_is_one) {
          grad_reduce_idx_[i].push_back(largest_rank - 1 - j);
        }
      }
    } else {
      result_.push_back(output_dim);
      for (int i = 0; i < N; ++i) {
        reshape_[i].push_back(copy[i][j]);
        bcast_[i].push_back(current_is_one[i] ? output_dim : 1);
        if (current_is_one[i] && !none_is_one) {
          grad_reduce_idx_[i].push_back(largest_rank - 1 - j);
        }
      }
    }
    set_one = true;
    for (int i = 0; i < N; ++i) {
      prev_is_one[i] = current_is_one[i];
    }
  }
  if (result_.empty()) {
    result_.push_back(1);
    for (int i = 0; i < N; ++i) {
      reshape_[i].push_back(1);
      bcast_[i].push_back(1);
    }
  }
  
  for (int i = 0; i < N; ++i) {
    Reverse(&reshape_[i]);
    Reverse(&bcast_[i]);
    Reverse(&grad_reduce_idx_[i]);
  }
  Reverse(&result_);
  Reverse(&output_);
  
  
  
  if (return_flattened_batch_indices && broadcasting_required_ && output_batch_size_ > 0) {
    for (int i = 0; i < N; ++i) {
      ComputeBatchIndices(output_batch_size_, reshape_[i], bcast_[i], &batch_indices_[i]);
    }
  }
}








































class BCast : public BCastList<2> {
 public:
  
  
  
  
  
  
  
  
  typedef gtl::InlinedVector<int64_t, 4> Vec;

  BCast(const Vec& x, const Vec& y, const bool fewer_dims_optimization = true, const bool return_flattened_batch_indices = false)
      : BCastList<2>({x, y}, fewer_dims_optimization, return_flattened_batch_indices) {}

  ~BCast() {}

  
  
  
  const Vec& x_reshape() const { return reshape_[0]; }
  const Vec& x_bcast() const { return bcast_[0]; }
  const Vec& y_reshape() const { return reshape_[1]; }
  const Vec& y_bcast() const { return bcast_[1]; }
  const Vec& result_shape() const { return result_; }
  const Vec& output_shape() const { return output_; }
  const Vec& grad_x_reduce_idx() const { return grad_reduce_idx_[0]; }
  const Vec& grad_y_reduce_idx() const { return grad_reduce_idx_[1]; }

  
  
  
  
  
  
  const std::vector<int64_t>& x_batch_indices() const {
    return batch_indices_[0];
  }
  
  
  
  
  const std::vector<int64_t>& y_batch_indices() const {
    return batch_indices_[1];
  }

  template <typename IndexType, int NDIMS> static Eigen::array<IndexType, NDIMS> ToIndexArrayType( const BCast::Vec& vec) {

    CHECK_EQ(vec.size(), NDIMS);
    Eigen::array<IndexType, NDIMS> ret;
    for (int i = 0; i < NDIMS; ++i) ret[i] = vec[i];
    return ret;
  }

  template <int NDIMS> static Eigen::array<Eigen::DenseIndex, NDIMS> ToIndexArray( const BCast::Vec& vec) {

    return ToIndexArrayType<Eigen::DenseIndex, NDIMS>(vec);
  }

  
  static Vec FromShape(const TensorShape& shape);
  static TensorShape ToShape(const Vec& vec);

 private:
  TF_DISALLOW_COPY_AND_ASSIGN(BCast);
};

}  



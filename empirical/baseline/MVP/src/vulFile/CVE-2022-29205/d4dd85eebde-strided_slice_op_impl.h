























namespace tensorflow {

template <typename Device, typename T, int NDIM> void HandleStridedSliceCase(OpKernelContext* context, const gtl::ArraySlice<int64_t>& begin, const gtl::ArraySlice<int64_t>& end, const gtl::ArraySlice<int64_t>& strides, const TensorShape& processing_shape, bool is_simple_slice, Tensor* result);






template <typename Device, typename T, int NDIM> void HandleStridedSliceGradCase(OpKernelContext* context, const gtl::ArraySlice<int64_t>& begin, const gtl::ArraySlice<int64_t>& end, const gtl::ArraySlice<int64_t>& strides, const TensorShape& processing_shape, bool is_simple_slice, Tensor* result);






template <typename Device, typename T, int NDIM> class HandleStridedSliceAssignCase {
 public:
  void operator()(OpKernelContext* context, const gtl::ArraySlice<int64_t>& begin, const gtl::ArraySlice<int64_t>& end, const gtl::ArraySlice<int64_t>& strides, const TensorShape& processing_shape, bool is_simple_slice, Tensor* result);




};
}  










namespace tensorflow {

template <typename Device, typename T, int NDIM> void HandleStridedSliceCase(OpKernelContext* context, const gtl::ArraySlice<int64_t>& begin, const gtl::ArraySlice<int64_t>& end, const gtl::ArraySlice<int64_t>& strides, const TensorShape& processing_shape, bool is_simple_slice, Tensor* result) {





  typedef typename proxy_type<Device, T>::type Proxy;

  gtl::InlinedVector<int64_t, 4> processing_dims = processing_shape.dim_sizes();
  if (is_simple_slice) {
    Eigen::DSizes<Eigen::DenseIndex, NDIM> begin_di;
    Eigen::DSizes<Eigen::DenseIndex, NDIM> sizes_di;
    for (int i = 0; i < NDIM; ++i) {
      begin_di[i] = begin[i];
      sizes_di[i] = end[i] - begin[i];
    }
    functor::Slice<Device, Proxy, NDIM>()( context->eigen_device<Device>(), result->bit_casted_shaped<Proxy, NDIM>(processing_dims), context->input(0).bit_casted_tensor<Proxy, NDIM>(), begin_di, sizes_di);


  } else {
    Eigen::DSizes<Eigen::DenseIndex, NDIM> begin_di;
    Eigen::DSizes<Eigen::DenseIndex, NDIM> end_di;
    Eigen::DSizes<Eigen::DenseIndex, NDIM> strides_di;
    for (int i = 0; i < NDIM; ++i) {
      begin_di[i] = begin[i];
      end_di[i] = end[i];
      strides_di[i] = strides[i];
    }
    functor::StridedSlice<Device, Proxy, NDIM>()( context->eigen_device<Device>(), result->bit_casted_shaped<Proxy, NDIM>(processing_dims), context->input(0).bit_casted_tensor<Proxy, NDIM>(), begin_di, end_di, strides_di);



  }
}

template <typename Device, typename T, int NDIM> void HandleStridedSliceGradCase(OpKernelContext* context, const gtl::ArraySlice<int64_t>& begin, const gtl::ArraySlice<int64_t>& end, const gtl::ArraySlice<int64_t>& strides, const TensorShape& processing_shape, bool is_simple_slice, Tensor* result) {





  gtl::InlinedVector<int64_t, 4> processing_dims = processing_shape.dim_sizes();

  Eigen::DSizes<Eigen::DenseIndex, NDIM> begin_di;
  Eigen::DSizes<Eigen::DenseIndex, NDIM> end_di;
  Eigen::DSizes<Eigen::DenseIndex, NDIM> strides_di;
  for (int i = 0; i < NDIM; ++i) {
    begin_di[i] = begin[i];
    end_di[i] = end[i];
    strides_di[i] = strides[i];
  }

  typedef typename proxy_type<Device, T>::type Proxy;
  functor::StridedSliceGrad<Device, Proxy, NDIM>()( context->eigen_device<Device>(), result->bit_casted_tensor<Proxy, NDIM>(), context->input(4).bit_casted_shaped<Proxy, NDIM>(processing_dims), begin_di, end_di, strides_di);


}

template <typename Device, typename T, int NDIM> void HandleStridedSliceAssignCase<Device, T, NDIM>::operator()( OpKernelContext* context, const gtl::ArraySlice<int64_t>& begin, const gtl::ArraySlice<int64_t>& end, const gtl::ArraySlice<int64_t>& strides, const TensorShape& processing_shape, bool is_simple_slice, Tensor* result) {




  gtl::InlinedVector<int64_t, 4> processing_dims = processing_shape.dim_sizes();
  typedef typename proxy_type<Device, T>::type Proxy;
  Eigen::DSizes<Eigen::DenseIndex, NDIM> begin_di;
  Eigen::DSizes<Eigen::DenseIndex, NDIM> end_di;
  Eigen::DSizes<Eigen::DenseIndex, NDIM> strides_di;
  for (int i = 0; i < NDIM; ++i) {
    begin_di[i] = begin[i];
    end_di[i] = end[i];
    strides_di[i] = strides[i];
  }
  functor::StridedSliceAssign<Device, Proxy, NDIM>()( context->eigen_device<Device>(), result->bit_casted_tensor<Proxy, NDIM>(), context->input(4).bit_casted_shaped<Proxy, NDIM>(processing_dims), begin_di, end_di, strides_di);


}

template <typename Device, typename T> class HandleStridedSliceAssignCase<Device, T, 0> {
 public:
  enum { NDIM_PROXY = 1 };
  void operator()(OpKernelContext* context, const gtl::ArraySlice<int64_t>& begin, const gtl::ArraySlice<int64_t>& end, const gtl::ArraySlice<int64_t>& strides, const TensorShape& processing_shape, bool is_simple_slice, Tensor* result) {




    gtl::InlinedVector<int64_t, 1> processing_dims(1);
    processing_dims[0] = 1;

    typedef typename proxy_type<Device, T>::type Proxy;
    functor::StridedSliceAssignScalar<Device, Proxy>()( context->eigen_device<Device>(), result->bit_casted_shaped<Proxy, 1>(processing_dims), context->input(4).bit_casted_shaped<Proxy, 1>(processing_dims));


  }
};


























































































TF_CALL_GPU_PROXY_TYPES(PREVENT_FOR_N_GPU);
TF_CALL_COMPLEX_TYPES(PREVENT_FOR_N_GPU);

TF_CALL_uint8(DECLARE_FOR_N_GPU);
TF_CALL_int8(DECLARE_FOR_N_GPU);
TF_CALL_int32(DECLARE_FOR_N_GPU);
TF_CALL_int64(DECLARE_FOR_N_GPU);
TF_CALL_uint32(DECLARE_FOR_N_GPU);
TF_CALL_GPU_ALL_TYPES(DECLARE_FOR_N_GPU);


TF_CALL_ALL_TYPES(DECLARE_FOR_N_CPU);






}  





















































namespace {





template <typename T> void Im2col(const T* input_data, const int depth, const int height, const int width, const int filter_h, const int filter_w, const int pad_t, const int pad_l, const int pad_b, const int pad_r, const int stride_h, const int stride_w, T* col_data) {



  int height_col = (height + pad_t + pad_b - filter_h) / stride_h + 1;
  int width_col = (width + pad_l + pad_r - filter_w) / stride_w + 1;

  int h_pad = -pad_t;
  for (int h = 0; h < height_col; ++h) {
    int w_pad = -pad_l;
    for (int w = 0; w < width_col; ++w) {
      for (int ih = h_pad; ih < h_pad + filter_h; ++ih) {
        for (int iw = w_pad; iw < w_pad + filter_w; ++iw) {
          if (ih >= 0 && ih < height && iw >= 0 && iw < width) {
            memcpy(col_data, input_data + (ih * width + iw) * depth, sizeof(T) * depth);
          } else {
            
            memset(col_data, 0, sizeof(T) * depth);
          }
          col_data += depth;
        }
      }
      w_pad += stride_w;
    }
    h_pad += stride_h;
  }
}

}  

namespace tensorflow {

typedef Eigen::ThreadPoolDevice CPUDevice;
typedef Eigen::GpuDevice GPUDevice;

template <typename T> struct LaunchConv2DBackpropFilterOp<CPUDevice, T> {
  void operator()(OpKernelContext* ctx, bool use_cudnn, bool cudnn_use_autotune, const Tensor& out_backprop, const Tensor& input, int row_dilation, int col_dilation, int row_stride, int col_stride, const Padding& padding, const std::vector<int64>& explicit_paddings, Tensor* filter_backprop, TensorFormat data_format) {




    std::vector<int32> dilations(4, 1);
    dilations[GetTensorDimIndex(data_format, 'H')] = row_dilation;
    dilations[GetTensorDimIndex(data_format, 'W')] = col_dilation;

    std::vector<int32> strides(4, 1);
    strides[GetTensorDimIndex(data_format, 'H')] = row_stride;
    strides[GetTensorDimIndex(data_format, 'W')] = col_stride;
    TensorShape filter_shape = filter_backprop->shape();

    ConvBackpropDimensions dims;
    OP_REQUIRES_OK( ctx, ConvBackpropComputeDimensionsV2( "Conv2DBackpropFilter", 2, input.shape(), filter_shape, out_backprop.shape(), dilations, strides, padding, explicit_paddings, data_format, &dims));




    int64 padding_top = -1, padding_bottom = -1;
    int64 padding_left = -1, padding_right = -1;
    if (padding == EXPLICIT) {
      GetExplicitPaddingForDim(explicit_paddings, data_format, 'H', &padding_top, &padding_bottom);
      GetExplicitPaddingForDim(explicit_paddings, data_format, 'W', &padding_left, &padding_right);
    }
    int64 expected_out_rows, expected_out_cols;
    
    
    TF_CHECK_OK(GetWindowedOutputSizeVerboseV2( dims.spatial_dims[0].input_size, dims.spatial_dims[0].filter_size, row_dilation, row_stride, padding, &expected_out_rows, &padding_top, &padding_bottom));


    DCHECK_EQ(dims.spatial_dims[0].output_size, expected_out_rows);
    TF_CHECK_OK(GetWindowedOutputSizeVerboseV2( dims.spatial_dims[1].input_size, dims.spatial_dims[1].filter_size, col_dilation, col_stride, padding, &expected_out_cols, &padding_left, &padding_right));


    DCHECK_EQ(dims.spatial_dims[1].output_size, expected_out_cols);

    const CPUDevice& d = ctx->eigen_device<CPUDevice>();

    
    
    

    auto filter_backprop_t = filter_backprop->tensor<T, 4>();
    auto input_t = input.tensor<T, 4>();
    auto out_backprop_t = out_backprop.tensor<T, 4>();

    if (padding != EXPLICIT) {
      
      
      filter_backprop_t.device(d) = Eigen::SpatialConvolutionBackwardKernel( input_t, out_backprop_t, filter_backprop_t.dimension(1), filter_backprop_t.dimension(0), col_stride, row_stride, col_dilation, row_dilation);



    } else {
      
      
      Eigen::array<std::pair<int, int>, 4> paddings;
      paddings[0] = {0, 0};
      paddings[1] = {padding_top, padding_bottom};
      paddings[2] = {padding_left, padding_right};
      paddings[3] = {0, 0};

      auto padded_t = input_t.pad(paddings, T(0));

      
      
      filter_backprop_t.device(d) = Eigen::SpatialConvolutionBackwardKernel( padded_t, out_backprop_t, filter_backprop_t.dimension(1), filter_backprop_t.dimension(0), col_stride, row_stride, col_dilation, row_dilation);


    }
  }
};


template <typename Device, class T> struct LaunchXsmmBackwardFilter {
  bool operator()(OpKernelContext* context, const Device& d, typename TTypes<T, 4>::ConstTensor input_backward, typename TTypes<T, 4>::Tensor kernel, typename TTypes<T, 4>::ConstTensor output_backward, int input_rows, int input_cols, int row_stride, int col_stride, int pad_h, int pad_w, TensorFormat data_format) const {





    return false;
  }
};

template <> struct LaunchXsmmBackwardFilter<CPUDevice, float> {
  bool operator()(OpKernelContext* context, const CPUDevice& d, typename TTypes<float, 4>::ConstTensor input, typename TTypes<float, 4>::Tensor filter, typename TTypes<float, 4>::ConstTensor output, int input_rows, int input_cols, int row_stride, int col_stride, int pad_h, int pad_w, TensorFormat data_format) const {




    auto batch = input.dimension(0);
    auto in_depth = input.dimension(3);
    auto out_depth = output.dimension(3);
    auto filter_rows = filter.dimension(0);
    auto filter_cols = filter.dimension(1);

    auto num_threads = context->device()->tensorflow_cpu_worker_threads()->num_threads;
    
    libxsmm_dnn_conv_desc desc;
    desc.N = batch;
    desc.C = in_depth;
    desc.H = input_rows;
    desc.W = input_cols;
    desc.K = out_depth;
    desc.R = filter_rows;
    desc.S = filter_cols;
    desc.u = row_stride;
    desc.v = col_stride;
    desc.pad_h = pad_h;
    desc.pad_w = pad_w;
    desc.pad_h_in = 0;  
    desc.pad_w_in = 0;  
    desc.pad_h_out = 0;
    desc.pad_w_out = 0;
    desc.threads = num_threads;
    desc.algo = LIBXSMM_DNN_CONV_ALGO_DIRECT;
    desc.buffer_format = LIBXSMM_DNN_TENSOR_FORMAT_NHWC;
    desc.filter_format = LIBXSMM_DNN_TENSOR_FORMAT_RSCK;
    desc.fuse_ops = LIBXSMM_DNN_CONV_FUSE_NONE;
    desc.options = LIBXSMM_DNN_CONV_OPTION_NONE;
    desc.datatype_out = LIBXSMM_DNN_DATATYPE_F32;
    desc.datatype_in = LIBXSMM_DNN_DATATYPE_F32;
    if (!CanUseXsmmConv2D(desc, data_format)) {
      return false;
    }

    auto input_ptr = input.data();
    auto filter_ptr = filter.data();
    auto output_ptr = output.data();
    bool success = functor::XsmmBkwFilterConv2D<CPUDevice, float>()( context, desc, input_ptr, filter_ptr, output_ptr);
    return success;
  }
};


template <typename Device, class T> class Conv2DBackpropFilterOp : public OpKernel {
 public:
  explicit Conv2DBackpropFilterOp(OpKernelConstruction* context)
      : OpKernel(context) {
    string data_format;
    OP_REQUIRES_OK(context, context->GetAttr("data_format", &data_format));
    OP_REQUIRES(context, FormatFromString(data_format, &data_format_), errors::InvalidArgument("Invalid data format"));
    OP_REQUIRES_OK(context, context->GetAttr("strides", &strides_));
    int stride_n = GetTensorDim(strides_, data_format_, 'N');
    int stride_c = GetTensorDim(strides_, data_format_, 'C');
    int stride_h = GetTensorDim(strides_, data_format_, 'H');
    int stride_w = GetTensorDim(strides_, data_format_, 'W');
    OP_REQUIRES( context, (stride_n == 1 && stride_c == 1), errors::InvalidArgument("Current implementation does not yet support " "strides in the batch and depth dimensions."));


    OP_REQUIRES(context, stride_h > 0 && stride_w > 0, errors::InvalidArgument( "Row and column strides should be larger than 0."));

    OP_REQUIRES_OK(context, context->GetAttr("dilations", &dilations_));
    OP_REQUIRES(context, dilations_.size() == 4, errors::InvalidArgument("Sliding window dilations field must " "specify 4 dimensions"));

    int dilation_n = GetTensorDim(dilations_, data_format_, 'N');
    int dilation_c = GetTensorDim(dilations_, data_format_, 'C');
    int dilation_h = GetTensorDim(dilations_, data_format_, 'H');
    int dilation_w = GetTensorDim(dilations_, data_format_, 'W');
    OP_REQUIRES(context, dilation_n == 1 && dilation_c == 1, errors::InvalidArgument( "Current implementation does not yet support " "dilations in the batch and depth dimensions."));


    OP_REQUIRES( context, dilation_h > 0 && dilation_w > 0, errors::InvalidArgument("Dilated rates should be larger than 0."));


    OP_REQUIRES_OK(context, context->GetAttr("padding", &padding_));
    OP_REQUIRES_OK(context, context->GetAttr("explicit_paddings", &explicit_paddings_));
    OP_REQUIRES_OK(context, CheckValidPadding(padding_, explicit_paddings_, 4, data_format_));

    OP_REQUIRES_OK(context, context->GetAttr("use_cudnn_on_gpu", &use_cudnn_));
    cudnn_use_autotune_ = CudnnUseAutotune();

    if (std::is_same<Device, CPUDevice>::value) {
      OP_REQUIRES(context, data_format_ == FORMAT_NHWC, errors::InvalidArgument("Conv2DBackpropFilterOp [CPU] " "only supports NHWC data format."));


      
      OP_REQUIRES( context, (dilation_h == 1 && dilation_w == 1), errors::InvalidArgument("Conv2DBackpropFilterOp [CPU] not yet " "support dilation rates larger than 1."));


    }
  }

  void Compute(OpKernelContext* context) override {
    const Tensor& input = context->input(0);
    const Tensor& filter_sizes = context->input(1);
    const Tensor& out_backprop = context->input(2);
    OP_REQUIRES( context, TensorShapeUtils::IsVector(filter_sizes.shape()), errors::InvalidArgument( "Conv2DBackpropFilter: filter_sizes input must be 1-dim, not ", filter_sizes.dims()));



    TensorShape filter_shape;
    OP_REQUIRES_OK(context, TensorShapeUtils::MakeShape( filter_sizes.vec<int32>(), &filter_shape));

    Tensor* filter_backprop = nullptr;
    OP_REQUIRES_OK(context, context->allocate_output(0, filter_shape, &filter_backprop));

    
    if (filter_shape.num_elements() == 0) {
      return;
    }
    
    if (input.shape().num_elements() == 0) {
      functor::SetZeroFunctor<Device, T> f;
      f(context->eigen_device<Device>(), filter_backprop->flat<T>());
      return;
    }

    
    
    const int stride_rows = GetTensorDim(strides_, data_format_, 'H');
    const int stride_cols = GetTensorDim(strides_, data_format_, 'W');
    const int dilation_rows = GetTensorDim(dilations_, data_format_, 'H');
    const int dilation_cols = GetTensorDim(dilations_, data_format_, 'W');

    VLOG(2) << "Conv2DBackpropFilter:" << " input: " << input.shape().DebugString()
            << " filter:" << filter_shape.DebugString()
            << " out_backprop: " << out_backprop.shape().DebugString()
            << " strides: [" << stride_rows << ", " << stride_cols << "]" << " dilations: [" << dilation_rows << ", " << dilation_cols << "]";

    launcher_(context, use_cudnn_, cudnn_use_autotune_, out_backprop, input, dilation_rows, dilation_cols, stride_rows, stride_cols, padding_, explicit_paddings_, filter_backprop, data_format_);

  }

 private:
  std::vector<int32> dilations_;
  std::vector<int32> strides_;
  Padding padding_;
  std::vector<int64> explicit_paddings_;
  bool use_cudnn_;
  TensorFormat data_format_;
  LaunchConv2DBackpropFilterOp<Device, T> launcher_;
  bool cudnn_use_autotune_;

  TF_DISALLOW_COPY_AND_ASSIGN(Conv2DBackpropFilterOp);
};


template <typename Device, class T> class Conv2DCustomBackpropFilterOp : public OpKernel {
 public:
  explicit Conv2DCustomBackpropFilterOp(OpKernelConstruction* context)
      : OpKernel(context) {
    string data_format;
    OP_REQUIRES_OK(context, context->GetAttr("data_format", &data_format));
    OP_REQUIRES(context, FormatFromString(data_format, &data_format_), errors::InvalidArgument("Invalid data format"));
    OP_REQUIRES(context, data_format_ == FORMAT_NHWC, errors::InvalidArgument( "Conv2DCustomBackpropFilterOp only supports NHWC."));

    OP_REQUIRES_OK(context, context->GetAttr("strides", &strides_));
    OP_REQUIRES(context, strides_.size() == 4, errors::InvalidArgument("Sliding window strides field must " "specify 4 dimensions"));

    OP_REQUIRES( context, (strides_[0] == 1 && strides_[3] == 1), errors::InvalidArgument("Current implementation does not yet support " "strides in the batch and depth dimensions."));


    OP_REQUIRES(context, strides_[1] > 0 && strides_[2] > 0, errors::InvalidArgument( "Row and column strides should be larger than 0."));

    OP_REQUIRES_OK(context, context->GetAttr("padding", &padding_));
    OP_REQUIRES_OK(context, context->GetAttr("explicit_paddings", &explicit_paddings_));
    OP_REQUIRES_OK(context, CheckValidPadding(padding_, explicit_paddings_, 4, data_format_));
    OP_REQUIRES_OK(context, context->GetAttr("dilations", &dilations_));
    OP_REQUIRES(context, dilations_.size() == 4, errors::InvalidArgument("Sliding window dilations field must " "specify 4 dimensions"));

    OP_REQUIRES(context, (dilations_[0] == 1 && dilations_[3] == 1), errors::InvalidArgument( "Current implementation does not yet support " "dilations in the batch and depth dimensions."));


    if (std::is_same<Device, CPUDevice>::value || std::is_same<Device, GPUDevice>::value) {
      
      OP_REQUIRES(context, (dilations_[1] == 1 && dilations_[2] == 1), errors::InvalidArgument( "Current libxsmm and customized CPU implementations do " "not yet support dilation rates larger than 1."));


      dilations_ = {1, 1, 1, 1};
    }
  }

  void Compute(OpKernelContext* context) override {
    const Tensor& input = context->input(0);
    const Tensor& filter_sizes = context->input(1);
    const Tensor& out_backprop = context->input(2);
    OP_REQUIRES( context, TensorShapeUtils::IsVector(filter_sizes.shape()), errors::InvalidArgument( "Conv2DCustomBackpropFilter: filter_sizes input must be 1-dim, " "not ", filter_sizes.dims()));




    TensorShape filter_shape;
    OP_REQUIRES_OK(context, TensorShapeUtils::MakeShape( filter_sizes.vec<int32>(), &filter_shape));

    ConvBackpropDimensions dims;
    OP_REQUIRES_OK( context, ConvBackpropComputeDimensionsV2( "Conv2DCustomBackpropFilter", 2, input.shape(), filter_shape, out_backprop.shape(), dilations_, strides_, padding_, explicit_paddings_, data_format_, &dims));





    Tensor* filter_backprop;
    OP_REQUIRES_OK(context, context->allocate_output(0, filter_shape, &filter_backprop));

    
    if (filter_shape.num_elements() == 0) {
      return;
    }

    int64 pad_top, pad_bottom;
    int64 pad_left, pad_right;
    if (padding_ == Padding::EXPLICIT) {
      pad_top = explicit_paddings_[2];
      pad_bottom = explicit_paddings_[3];
      pad_left = explicit_paddings_[4];
      pad_right = explicit_paddings_[5];
    }
    OP_REQUIRES_OK( context, GetWindowedOutputSizeVerbose( dims.spatial_dims[0].input_size, dims.spatial_dims[0].filter_size, dims.spatial_dims[0].stride, padding_, &dims.spatial_dims[0].output_size, &pad_top, &pad_bottom));




    OP_REQUIRES_OK( context, GetWindowedOutputSizeVerbose( dims.spatial_dims[1].input_size, dims.spatial_dims[1].filter_size, dims.spatial_dims[1].stride, padding_, &dims.spatial_dims[1].output_size, &pad_left, &pad_right));





    if (pad_left == pad_right && pad_top == pad_bottom) {
      if (LaunchXsmmBackwardFilter<Device, T>()( context, context->eigen_device<Device>(), input.tensor<T, 4>(), filter_backprop->tensor<T, 4>(), out_backprop.tensor<T, 4>(), dims.spatial_dims[0].input_size, dims.spatial_dims[1].input_size, static_cast<int>(dims.spatial_dims[0].stride), static_cast<int>(dims.spatial_dims[1].stride), static_cast<int>(pad_top), static_cast<int>(pad_left), data_format_)) {






        return;
      }
    }


    
    const int filter_total_size = dims.spatial_dims[0].filter_size * dims.spatial_dims[1].filter_size * dims.in_depth;

    
    const int output_image_size = dims.spatial_dims[0].output_size * dims.spatial_dims[1].output_size;

    
    
    
    

    
    
    
    
    const size_t target_working_set_size = (30LL << 20) / sizeof(T);

    const size_t size_A = output_image_size * filter_total_size;

    const size_t size_B = output_image_size * dims.out_depth;

    const size_t size_C = filter_total_size * dims.out_depth;

    const size_t work_unit_size = size_A + size_B + size_C;

    const size_t shard_size = (target_working_set_size + work_unit_size - 1) / work_unit_size;

    Tensor col_buffer;
    OP_REQUIRES_OK(context, context->allocate_temp( DataTypeToEnum<T>::value, TensorShape({static_cast<int64>(shard_size), static_cast<int64>(output_image_size), static_cast<int64>(filter_total_size)}), &col_buffer));






    
    const int input_offset = dims.spatial_dims[0].input_size * dims.spatial_dims[1].input_size * dims.in_depth;
    
    const int output_offset = dims.spatial_dims[0].output_size * dims.spatial_dims[1].output_size * dims.out_depth;

    const T* input_data = input.template flat<T>().data();
    T* col_buffer_data = col_buffer.template flat<T>().data();
    const T* out_backprop_data = out_backprop.template flat<T>().data();
    T* filter_backprop_data = filter_backprop->template flat<T>().data();

    typedef Eigen::TensorMap<Eigen::Tensor<T, 2, Eigen::RowMajor>, Eigen::Unaligned> TensorMap;

    typedef Eigen::TensorMap<Eigen::Tensor<const T, 2, Eigen::RowMajor>, Eigen::Unaligned> ConstTensorMap;


    TensorMap C(filter_backprop_data, filter_total_size, dims.out_depth);
    C.setZero();

    
    Eigen::array<Eigen::IndexPair<Eigen::DenseIndex>, 1> contract_dims;
    contract_dims[0].first = 0;
    contract_dims[0].second = 0;

    auto worker_threads = *(context->device()->tensorflow_cpu_worker_threads());

    for (int image_id = 0; image_id < dims.batch_size; image_id += shard_size) {
      const int shard_limit = std::min(static_cast<int>(shard_size), static_cast<int>(dims.batch_size) - image_id);


      auto shard = [&input_data, &col_buffer_data, &dims, &pad_top, &pad_left, &pad_bottom, &pad_right, &input_offset, &size_A](int64 start, int64 limit) {

        for (int shard_id = start; shard_id < limit; ++shard_id) {
          const T* input_data_shard = input_data + shard_id * input_offset;
          T* col_data_shard = col_buffer_data + shard_id * size_A;

          
          
          Im2col<T>( input_data_shard, dims.in_depth, dims.spatial_dims[0].input_size, dims.spatial_dims[1].input_size, dims.spatial_dims[0].filter_size, dims.spatial_dims[1].filter_size, pad_top, pad_left, pad_bottom, pad_right, dims.spatial_dims[0].stride, dims.spatial_dims[1].stride, col_data_shard);




        }
      };
      Shard(worker_threads.num_threads, worker_threads.workers, shard_limit, size_A, shard);

      ConstTensorMap A(col_buffer_data, output_image_size * shard_limit, filter_total_size);
      ConstTensorMap B(out_backprop_data, output_image_size * shard_limit, dims.out_depth);

      
      C.device(context->eigen_cpu_device()) += A.contract(B, contract_dims);

      input_data += input_offset * shard_limit;
      out_backprop_data += output_offset * shard_limit;
    }
  }

 private:
  std::vector<int32> dilations_;
  std::vector<int32> strides_;
  Padding padding_;
  std::vector<int64> explicit_paddings_;
  TensorFormat data_format_;

  TF_DISALLOW_COPY_AND_ASSIGN(Conv2DCustomBackpropFilterOp);
};

















TF_CALL_half(REGISTER_CPU_KERNELS);
TF_CALL_float(REGISTER_CPU_KERNELS);
TF_CALL_double(REGISTER_CPU_KERNELS);



template struct LaunchConv2DBackpropFilterOp<CPUDevice, Eigen::half>;
template struct LaunchConv2DBackpropFilterOp<CPUDevice, float>;
template struct LaunchConv2DBackpropFilterOp<CPUDevice, double>;






struct ConvBackwardFilterAutoTuneGroup {
  static string name() { return "ConvBwdFilter"; }
};

typedef AutoTuneSingleton<ConvBackwardFilterAutoTuneGroup, ConvParameters, se::dnn::AlgorithmConfig> AutoTuneConvBwdFilter;


template <typename T> void LaunchConv2DBackpropFilterOp<Eigen::GpuDevice, T>::operator()( OpKernelContext* ctx, bool use_cudnn, bool cudnn_use_autotune, const Tensor& out_backprop, const Tensor& input, int row_dilation, int col_dilation, int row_stride, int col_stride, const Padding& padding, const std::vector<int64>& explicit_paddings, Tensor* filter_backprop, TensorFormat data_format) {





  using se::dnn::AlgorithmConfig;
  using se::dnn::AlgorithmDesc;
  using se::dnn::ProfileResult;

  std::vector<int32> dilations(4, 1);
  dilations[GetTensorDimIndex(data_format, 'H')] = row_dilation;
  dilations[GetTensorDimIndex(data_format, 'W')] = col_dilation;

  std::vector<int32> strides(4, 1);
  strides[GetTensorDimIndex(data_format, 'H')] = row_stride;
  strides[GetTensorDimIndex(data_format, 'W')] = col_stride;
  TensorShape filter_shape = filter_backprop->shape();

  ConvBackpropDimensions dims;
  OP_REQUIRES_OK( ctx, ConvBackpropComputeDimensionsV2( "Conv2DBackpropFilter", 2, input.shape(), filter_shape, out_backprop.shape(), dilations, strides, padding, explicit_paddings, data_format, &dims));




  int64 padding_top = -1, padding_bottom = -1;
  int64 padding_left = -1, padding_right = -1;
  if (padding == EXPLICIT) {
    GetExplicitPaddingForDim(explicit_paddings, data_format, 'H', &padding_top, &padding_bottom);
    GetExplicitPaddingForDim(explicit_paddings, data_format, 'W', &padding_left, &padding_right);
  }
  int64 expected_out_rows, expected_out_cols;
  
  
  TF_CHECK_OK(GetWindowedOutputSizeVerboseV2( dims.spatial_dims[0].input_size, dims.spatial_dims[0].filter_size, row_dilation, row_stride, padding, &expected_out_rows, &padding_top, &padding_bottom));


  DCHECK_EQ(dims.spatial_dims[0].output_size, expected_out_rows);
  TF_CHECK_OK(GetWindowedOutputSizeVerboseV2( dims.spatial_dims[1].input_size, dims.spatial_dims[1].filter_size, col_dilation, col_stride, padding, &expected_out_cols, &padding_left, &padding_right));


  DCHECK_EQ(dims.spatial_dims[1].output_size, expected_out_cols);

  auto* stream = ctx->op_device_context()->stream();
  OP_REQUIRES(ctx, stream, errors::Internal("No GPU stream available."));

  if (!use_cudnn) {
    ctx->SetStatus(errors::Unimplemented( "Conv2DBackprop for GPU is not currently supported " "without cudnn"));

    return;
  }

  
  
  
  
  bool is_grouped_convolution = filter_shape.dim_size(2) != dims.in_depth;
  bool cudnn_disable_conv_1x1_optimization_ = CudnnDisableConv1x1Optimization();
  if (!cudnn_disable_conv_1x1_optimization_ && dims.spatial_dims[0].filter_size == 1 && dims.spatial_dims[1].filter_size == 1 && !is_grouped_convolution && dims.spatial_dims[0].stride == 1 && dims.spatial_dims[1].stride == 1 && data_format == FORMAT_NHWC && (padding == VALID || padding == SAME)) {



    const uint64 m = dims.in_depth;
    const uint64 k = dims.batch_size * dims.spatial_dims[0].input_size * dims.spatial_dims[1].input_size;
    const uint64 n = dims.out_depth;

    
    
    
    auto a_ptr = AsDeviceMemory(out_backprop.template flat<T>().data(), out_backprop.template flat<T>().size());

    
    
    
    auto b_ptr = AsDeviceMemory(input.template flat<T>().data(), input.template flat<T>().size());

    
    
    
    auto c_ptr = AsDeviceMemory(filter_backprop->template flat<T>().data(), filter_backprop->template flat<T>().size());

    bool blas_launch_status = stream ->ThenBlasGemm(se::blas::Transpose::kNoTranspose, se::blas::Transpose::kTranspose, n, m, k, 1.0f, a_ptr, n, b_ptr, m, 0.0f, &c_ptr, n)



            .ok();
    if (!blas_launch_status) {
      ctx->SetStatus(errors::Internal("Blas SGEMM launch failed : m=", m, ", n=", n, ", k=", k));
    }
    return;
  } else if (dims.spatial_dims[0].filter_size == dims.spatial_dims[0].input_size && dims.spatial_dims[1].filter_size == dims.spatial_dims[1].input_size && !is_grouped_convolution && padding == VALID && data_format == FORMAT_NHWC) {




    
    
    const uint64 m = dims.spatial_dims[0].input_size * dims.spatial_dims[1].input_size * dims.in_depth;
    const uint64 k = dims.batch_size;
    const uint64 n = dims.out_depth;

    auto a_ptr = AsDeviceMemory(input.template flat<T>().data(), input.template flat<T>().size());
    auto b_ptr = AsDeviceMemory(out_backprop.template flat<T>().data(), out_backprop.template flat<T>().size());
    auto c_ptr = AsDeviceMemory(filter_backprop->template flat<T>().data(), filter_backprop->template flat<T>().size());

    bool blas_launch_status = stream ->ThenBlasGemm(se::blas::Transpose::kNoTranspose, se::blas::Transpose::kTranspose, n, m, k, 1.0f, b_ptr, n, a_ptr, m, 0.0f, &c_ptr, n)



            .ok();
    if (!blas_launch_status) {
      ctx->SetStatus(errors::Internal("Blas SGEMM launch failed : m=", m, ", n=", n, ", k=", k));
    }
    return;
  }

  const int64 common_padding_rows = std::min(padding_top, padding_bottom);
  const int64 common_padding_cols = std::min(padding_left, padding_right);
  Tensor compatible_input;
  if (padding_top != padding_bottom || padding_left != padding_right) {
    
    
    
    const int64 padding_rows_diff = std::abs(padding_bottom - padding_top);
    const int64 padding_cols_diff = std::abs(padding_right - padding_left);
    const int64 new_in_rows = dims.spatial_dims[0].input_size + padding_rows_diff;
    const int64 new_in_cols = dims.spatial_dims[1].input_size + padding_cols_diff;
    const int64 input_pad_top = padding_top - common_padding_rows;
    const int64 input_pad_bottom = padding_bottom - common_padding_rows;
    const int64 input_pad_left = padding_left - common_padding_cols;
    const int64 input_pad_right = padding_right - common_padding_cols;
    OP_REQUIRES_OK( ctx, ctx->allocate_temp( DataTypeToEnum<T>::value, ShapeFromFormat(data_format, dims.batch_size, new_in_rows, new_in_cols, dims.in_depth), &compatible_input));





    functor::PadInput<GPUDevice, T, int, 4>()( ctx->template eigen_device<GPUDevice>(), To32Bit(input.tensor<T, 4>()), {{static_cast<int>(input_pad_top), static_cast<int>(input_pad_left)}}, {{static_cast<int>(input_pad_bottom), static_cast<int>(input_pad_right)}}, To32Bit(compatible_input.tensor<T, 4>()), data_format, T{});




  } else {
    compatible_input = input;
  }

  CHECK(common_padding_rows >= 0 && common_padding_cols >= 0)  
      << "Negative row or col paddings: (" << common_padding_rows << ", " << common_padding_cols << ")";

  
  
  
  const bool compute_in_nhwc = DataTypeToEnum<T>::value == DT_HALF && IsVoltaOrLater(*stream->parent());

  
  
  
  const TensorFormat compute_data_format = (compute_in_nhwc && data_format == FORMAT_NHWC) ? FORMAT_NHWC : FORMAT_NCHW;


  VLOG(3) << "Compute Conv2DBackpropFilter with cuDNN:" << " data_format=" << ToString(data_format)
          << " compute_data_format=" << ToString(compute_data_format);

  constexpr auto kComputeInNHWC = std::make_tuple(se::dnn::DataLayout::kBatchYXDepth, se::dnn::FilterLayout::kOutputYXInput);

  constexpr auto kComputeInNCHW = std::make_tuple(se::dnn::DataLayout::kBatchDepthYX, se::dnn::FilterLayout::kOutputInputYX);


  se::dnn::DataLayout compute_data_layout;
  se::dnn::FilterLayout filter_layout;

  std::tie(compute_data_layout, filter_layout) = compute_data_format == FORMAT_NHWC ? kComputeInNHWC : kComputeInNCHW;

  se::dnn::BatchDescriptor input_desc;
  input_desc.set_count(dims.batch_size)
      .set_height(GetTensorDim(compatible_input, data_format, 'H'))
      .set_width(GetTensorDim(compatible_input, data_format, 'W'))
      .set_feature_map_count(dims.in_depth)
      .set_layout(compute_data_layout);
  se::dnn::BatchDescriptor output_desc;
  output_desc.set_count(dims.batch_size)
      .set_height(dims.spatial_dims[0].output_size)
      .set_width(dims.spatial_dims[1].output_size)
      .set_feature_map_count(dims.out_depth)
      .set_layout(compute_data_layout);
  se::dnn::FilterDescriptor filter_desc;
  filter_desc.set_input_filter_height(dims.spatial_dims[0].filter_size)
      .set_input_filter_width(dims.spatial_dims[1].filter_size)
      .set_input_feature_map_count(filter_shape.dim_size(2))
      .set_output_feature_map_count(filter_shape.dim_size(3))
      .set_layout(filter_layout);
  se::dnn::ConvolutionDescriptor conv_desc;
  conv_desc.set_vertical_dilation_rate(dims.spatial_dims[0].dilation)
      .set_horizontal_dilation_rate(dims.spatial_dims[1].dilation)
      .set_vertical_filter_stride(dims.spatial_dims[0].stride)
      .set_horizontal_filter_stride(dims.spatial_dims[1].stride)
      .set_zero_padding_height(common_padding_rows)
      .set_zero_padding_width(common_padding_cols)
      .set_group_count(dims.in_depth / filter_shape.dim_size(2));

  
  
  
  
  
  
  

  Tensor pre_transformed_filter_backprop;
  OP_REQUIRES_OK( ctx, ctx->allocate_temp( DataTypeToEnum<T>::value, TensorShape({filter_shape.dim_size(3), filter_shape.dim_size(2), filter_shape.dim_size(0), filter_shape.dim_size(1)}), &pre_transformed_filter_backprop));






  Tensor transformed_out_backprop;
  if (data_format == FORMAT_NHWC && compute_data_format == FORMAT_NCHW) {
    VLOG(4) << "Convert the `out_backprop` tensor from NHWC to NCHW.";
    TensorShape compute_shape = ShapeFromFormat( compute_data_format, dims.batch_size, dims.spatial_dims[0].output_size, dims.spatial_dims[1].output_size, dims.out_depth);

    if (dims.out_depth > 1) {
      OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<T>::value, compute_shape, &transformed_out_backprop));

      functor::NHWCToNCHW<GPUDevice, T, 4>()( ctx->eigen_device<GPUDevice>(), out_backprop.tensor<T, 4>(), transformed_out_backprop.tensor<T, 4>());

    } else {
      
      CHECK(transformed_out_backprop.CopyFrom(out_backprop, compute_shape));
    }
  } else {
    transformed_out_backprop = out_backprop;
  }

  Tensor transformed_input;
  if (data_format == FORMAT_NHWC && compute_data_format == FORMAT_NCHW) {
    VLOG(4) << "Convert the `input` tensor from NHWC to NCHW.";
    TensorShape compute_shape = ShapeFromFormat( compute_data_format, GetTensorDim(compatible_input, data_format, 'N'), GetTensorDim(compatible_input, data_format, 'H'), GetTensorDim(compatible_input, data_format, 'W'), GetTensorDim(compatible_input, data_format, 'C'));



    if (compute_shape.dim_size(1) > 1) {
      OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<T>::value, compute_shape, &transformed_input));

      functor::NHWCToNCHW<GPUDevice, T, 4>()( ctx->eigen_device<GPUDevice>(), const_cast<const Tensor&>(compatible_input).tensor<T, 4>(), transformed_input.tensor<T, 4>());


    } else {
      
      CHECK(transformed_input.CopyFrom(compatible_input, compute_shape));
    }
  } else {
    transformed_input = compatible_input;
  }

  se::DeviceMemory<T> out_backprop_ptr = AsDeviceMemory(transformed_out_backprop.template flat<T>().data(), transformed_out_backprop.template flat<T>().size());

  se::DeviceMemory<T> filter_backprop_ptr = AsDeviceMemory(pre_transformed_filter_backprop.template flat<T>().data(), pre_transformed_filter_backprop.template flat<T>().size());

  auto input_ptr = AsDeviceMemory(transformed_input.template flat<T>().data(), transformed_input.template flat<T>().size());

  static int64 ConvolveBackwardFilterScratchSize = GetDnnWorkspaceLimit( "TF_CUDNN_WORKSPACE_LIMIT_IN_MB", 1LL << 32 );

  int device_id = stream->parent()->device_ordinal();
  DataType dtype = input.dtype();
  ConvParameters conv_parameters = {
      dims.batch_size,                      dims.in_depth, {{input_desc.height(), input_desc.width()}}, compute_data_format, dims.out_depth, {{dims.spatial_dims[0].filter_size, dims.spatial_dims[1].filter_size, filter_shape.dim_size(2)}}, {{dims.spatial_dims[0].dilation, dims.spatial_dims[1].dilation}}, {{dims.spatial_dims[0].stride, dims.spatial_dims[1].stride}}, {{common_padding_rows, common_padding_cols}}, dtype, device_id, conv_desc.group_count()
















  };

  
  
  
  cudnn_use_autotune = true;

  AlgorithmConfig algorithm_config;

  if (cudnn_use_autotune && !AutoTuneConvBwdFilter::GetInstance()->Find( conv_parameters, &algorithm_config)) {
    std::vector<std::unique_ptr<se::dnn::ConvolveExecutionPlan>> plans;

    std::vector<AlgorithmDesc> algorithms;
    std::vector<AlgorithmConfig> configs;

    if (CudnnUseFrontend()) {
      OP_REQUIRES( ctx, stream->parent()->GetConvolveExecutionPlans( se::dnn::ConvolutionKind::BACKWARD_FILTER, se::dnn::ToDataType<T>::value, stream, input_desc, filter_desc, output_desc, conv_desc, &plans), errors::Unknown("Failed to get convolution execution plan. This is " "probably because cuDNN failed to initialize, so try " "looking to see if a warning log message was printed " "above."));








      for (const auto& plan : plans) {
        configs.push_back( AlgorithmConfig(AlgorithmDesc{plan->getTag(), plan->get_raw_desc()}, plan->getWorkspaceSize()));

      }
    } else {
      OP_REQUIRES( ctx, stream->parent()->GetConvolveBackwardFilterAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo<T>( stream->parent()), &algorithms), errors::Unknown("Failed to get convolution execution plan. This is " "probably because cuDNN failed to initialize, so try " "looking to see if a warning log message was printed " "above."));








      for (const auto& algorithm : algorithms) {
        configs.push_back(AlgorithmConfig(algorithm));
      }
    }

    se::TfAllocatorAdapter tf_allocator_adapter(ctx->device()->GetAllocator({}), stream);
    se::RedzoneAllocator rz_allocator(stream, &tf_allocator_adapter, se::GpuAsmOpts());

    se::DeviceMemory<T> filter_backprop_ptr_rz( WrapRedzoneBestEffort(&rz_allocator, filter_backprop_ptr));

    std::vector<tensorflow::AutotuneResult> results;
    for (auto& profile_config : configs) {
      
      
      DnnScratchAllocator scratch_allocator(ConvolveBackwardFilterScratchSize, ctx);
      se::RedzoneAllocator rz_scratch_allocator( stream, &tf_allocator_adapter, se::GpuAsmOpts(), ConvolveBackwardFilterScratchSize);

      se::ScratchAllocator* allocator_used = !RedzoneCheckDisabled()
              ? static_cast<se::ScratchAllocator*>(&rz_scratch_allocator)
              : static_cast<se::ScratchAllocator*>(&scratch_allocator);

      ProfileResult profile_result;

      Status cudnn_launch_status;
      if (CudnnUseFrontend()) {
        cudnn_launch_status = stream->ConvolveBackwardFilterWithExecutionPlan( input_desc, input_ptr, output_desc, out_backprop_ptr, conv_desc, filter_desc, &filter_backprop_ptr_rz, allocator_used, profile_config, &profile_result);


      } else {
        cudnn_launch_status = stream->ConvolveBackwardFilterWithAlgorithm( input_desc, input_ptr, output_desc, out_backprop_ptr, conv_desc, filter_desc, &filter_backprop_ptr_rz, allocator_used, profile_config, &profile_result);


      }
      if (cudnn_launch_status.ok() && profile_result.is_valid()) {
        results.emplace_back();
        auto& result = results.back();
        if (CudnnUseFrontend()) {
          result.mutable_cuda_conv_plan()->set_exec_plan_id( profile_config.algorithm()->exec_plan_id());
        } else {
          result.mutable_conv()->set_algorithm( profile_config.algorithm()->algo_id());
          result.mutable_conv()->set_tensor_ops_enabled( profile_config.algorithm()->tensor_ops_enabled());
        }

        result.set_scratch_bytes( !RedzoneCheckDisabled()
                ? rz_scratch_allocator.TotalAllocatedBytesExcludingRedzones()
                : scratch_allocator.TotalByteSize());
        *result.mutable_run_time() = proto_utils::ToDurationProto( absl::Milliseconds(profile_result.elapsed_time_in_ms()));

        CheckRedzones(rz_scratch_allocator, &result);
        CheckRedzones(rz_allocator, &result);
      } else if (CudnnUseFrontend()) {
        
        
        
        results.emplace_back();
        auto& result = results.back();
        result.mutable_failure()->set_kind(AutotuneResult::UNKNOWN);
        result.mutable_failure()->set_msg( absl::StrCat("Profiling failure on CUDNN engine: ", profile_config.algorithm()->exec_plan_id()));

      }
    }

    DnnScratchAllocator scratch_allocator(ConvolveBackwardFilterScratchSize, ctx);

    std::vector<ProfileResult> algorithms;
    OP_REQUIRES( ctx, stream->parent()->GetMIOpenConvolveAlgorithms( se::dnn::ConvolutionKind::BACKWARD_FILTER, se::dnn::ToDataType<T>::value, stream, input_desc, input_ptr, filter_desc, filter_backprop_ptr, output_desc, out_backprop_ptr, conv_desc, &scratch_allocator, &algorithms), errors::Unknown( "Failed to get convolution algorithm. This is probably " "because MIOpen failed to initialize, so try looking to " "see if a warning log message was printed above."));










    std::vector<tensorflow::AutotuneResult> results;
    if (algorithms.size() == 1) {
      auto profile_result = algorithms[0];
      results.emplace_back();
      auto& result = results.back();
      result.mutable_conv()->set_algorithm( profile_result.algorithm().algo_id());
      result.mutable_conv()->set_tensor_ops_enabled( profile_result.algorithm().tensor_ops_enabled());

      result.set_scratch_bytes(profile_result.scratch_size());
      *result.mutable_run_time() = proto_utils::ToDurationProto( absl::Milliseconds(profile_result.elapsed_time_in_ms()));
    } else {
      for (auto miopen_algorithm : algorithms) {
        auto profile_algorithm = miopen_algorithm.algorithm();
        ProfileResult profile_result;
        auto miopen_launch_status = stream->ConvolveBackwardFilterWithAlgorithm( input_desc, input_ptr, output_desc, out_backprop_ptr, conv_desc, filter_desc, &filter_backprop_ptr, &scratch_allocator, AlgorithmConfig(profile_algorithm, miopen_algorithm.scratch_size()), &profile_result);




        if (miopen_launch_status.ok() && profile_result.is_valid()) {
          results.emplace_back();
          auto& result = results.back();
          result.mutable_conv()->set_algorithm(profile_algorithm.algo_id());
          result.mutable_conv()->set_tensor_ops_enabled( profile_algorithm.tensor_ops_enabled());
          result.set_scratch_bytes(scratch_allocator.TotalByteSize());
          *result.mutable_run_time() = proto_utils::ToDurationProto( absl::Milliseconds(profile_result.elapsed_time_in_ms()));
        }
      }
    }

    LogConvAutotuneResults(se::dnn::ConvolutionKind::BACKWARD_FILTER, se::dnn::ToDataType<T>::value, input_ptr, filter_backprop_ptr, out_backprop_ptr, input_desc, filter_desc, output_desc, conv_desc, stream->parent(), results);



    if (CudnnUseFrontend()) {
      OP_REQUIRES_OK( ctx, BestCudnnConvAlgorithm(results, &plans, &algorithm_config));
    } else {
      OP_REQUIRES_OK( ctx, BestCudnnConvAlgorithm(results, nullptr, &algorithm_config));
    }
    AutoTuneConvBwdFilter::GetInstance()->Insert(conv_parameters, algorithm_config);
  }

  Status cudnn_launch_status;
  DnnScratchAllocator scratch_allocator(ConvolveBackwardFilterScratchSize, ctx);
  if (CudnnUseFrontend()) {
    if (algorithm_config.algorithm().has_value()) {
      VLOG(4) << "Conv2DBackpropFilter Execution Plan: " << algorithm_config.algorithm()->exec_plan_id();
    } else {
      VLOG(4) << "Convolution AutoTune has been turned off";
    }
    cudnn_launch_status = stream->ConvolveBackwardFilterWithExecutionPlan( input_desc, input_ptr, output_desc, out_backprop_ptr, conv_desc, filter_desc, &filter_backprop_ptr, &scratch_allocator, algorithm_config, nullptr);


  } else {
    cudnn_launch_status = stream->ConvolveBackwardFilterWithAlgorithm( input_desc, input_ptr, output_desc, out_backprop_ptr, conv_desc, filter_desc, &filter_backprop_ptr, &scratch_allocator, algorithm_config, nullptr);


  }

  if (!cudnn_launch_status.ok()) {
    ctx->SetStatus(cudnn_launch_status);
    return;
  }

  FilterTensorFormat src_filter_format = compute_data_format == FORMAT_NCHW ? FORMAT_OIHW : FORMAT_OHWI;

  auto toConstTensor = [](const Tensor& x) -> const Tensor { return x; };
  functor::ReverseTransformFilter<GPUDevice, T, 4>()( ctx->eigen_device<GPUDevice>(), src_filter_format, toConstTensor(pre_transformed_filter_backprop).template tensor<T, 4>(), filter_backprop->tensor<T, 4>());


}


namespace functor {















DECLARE_GPU_SPEC(float);
DECLARE_GPU_SPEC(Eigen::half);
DECLARE_GPU_SPEC(double);

}  

REGISTER_KERNEL_BUILDER(Name("Conv2DBackpropFilter")
                            .Device(DEVICE_GPU)
                            .TypeConstraint<double>("T")
                            .HostMemory("filter_sizes"), Conv2DBackpropFilterOp<GPUDevice, double>);
REGISTER_KERNEL_BUILDER(Name("Conv2DBackpropFilter")
                            .Device(DEVICE_GPU)
                            .TypeConstraint<float>("T")
                            .HostMemory("filter_sizes"), Conv2DBackpropFilterOp<GPUDevice, float>);
REGISTER_KERNEL_BUILDER(Name("Conv2DBackpropFilter")
                            .Device(DEVICE_GPU)
                            .TypeConstraint<Eigen::half>("T")
                            .HostMemory("filter_sizes"), Conv2DBackpropFilterOp<GPUDevice, Eigen::half>);



template struct LaunchConv2DBackpropFilterOp<GPUDevice, float>;
template struct LaunchConv2DBackpropFilterOp<GPUDevice, Eigen::half>;
template struct LaunchConv2DBackpropFilterOp<GPUDevice, double>;



}  

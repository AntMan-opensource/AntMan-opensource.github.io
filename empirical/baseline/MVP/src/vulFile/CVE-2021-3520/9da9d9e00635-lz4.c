




static int real_LZ4_compress(const char *source, char *dest, int isize, int osize);
static int LZ4_uncompress_unknownOutputSize(const char *source, char *dest, int isize, int maxOutputSize);
static int LZ4_compressCtx(void *ctx, const char *source, char *dest, int isize, int osize);
static int LZ4_compress64kCtx(void *ctx, const char *source, char *dest, int isize, int osize);

static void *lz4_alloc(int flags);
static void lz4_free(void *ctx);


size_t lz4_compress_zfs(void *s_start, void *d_start, size_t s_len, size_t d_len, int n)

{
	uint32_t bufsiz;
	char *dest = d_start;

	ASSERT(d_len >= sizeof (bufsiz));

	bufsiz = real_LZ4_compress(s_start, &dest[sizeof (bufsiz)], s_len, d_len - sizeof (bufsiz));

	
	if (bufsiz == 0)
		return (s_len);

	
	*(uint32_t *)dest = BE_32(bufsiz);

	return (bufsiz + sizeof (bufsiz));
}


int lz4_decompress_zfs(void *s_start, void *d_start, size_t s_len, size_t d_len, int n)

{
	const char *src = s_start;
	uint32_t bufsiz = BE_IN32(src);

	
	if (bufsiz + sizeof (bufsiz) > s_len)
		return (1);

	
	return (LZ4_uncompress_unknownOutputSize(&src[sizeof (bufsiz)], d_start, bufsiz, d_len) < 0);
}
















































































typedef struct _U16_S {
	U16 v;
} U16_S;
typedef struct _U32_S {
	U32 v;
} U32_S;
typedef struct _U64_S {
	U64 v;
} U64_S;































































struct refTables {
	HTYPE hashTable[HASHTABLESIZE];
};












static inline int LZ4_NbCommonBytes(register U64 val)
{


	return (__builtin_clzll(val) >> 3);

	int r;
	if (!(val >> 32)) {
		r = 4;
	} else {
		r = 0;
		val >>= 32;
	}
	if (!(val >> 16)) {
		r += 2;
		val >>= 8;
	} else {
		val >>= 24;
	}
	r += (!val);
	return (r);



	return (__builtin_ctzll(val) >> 3);

	static const int DeBruijnBytePos[64] = { 0, 0, 0, 0, 0, 1, 1, 2, 0, 3, 1, 3, 1, 4, 2, 7, 0, 2, 3, 6, 1, 5, 3, 5, 1, 3, 4, 4, 2, 5, 6, 7, 7, 0, 1, 2, 3, 3, 4, 6, 2, 6, 5, 5, 3, 4, 5, 6, 7, 1, 2, 4, 6, 4, 4, 5, 7, 2, 6, 5, 7, 6, 7, 7 };




	return DeBruijnBytePos[((U64) ((val & -val) * 0x0218A392CDABBD3F)) >> 58];


}



static inline int LZ4_NbCommonBytes(register U32 val)
{


	return (__builtin_clz(val) >> 3);

	int r;
	if (!(val >> 16)) {
		r = 2;
		val >>= 8;
	} else {
		r = 0;
		val >>= 24;
	}
	r += (!val);
	return (r);



	return (__builtin_ctz(val) >> 3);

	static const int DeBruijnBytePos[32] = {
		0, 0, 3, 0, 3, 1, 3, 0, 3, 2, 2, 1, 3, 2, 0, 1, 3, 3, 1, 2, 2, 2, 2, 0, 3, 1, 2, 0, 1, 0, 1, 1 };



	return DeBruijnBytePos[((U32) ((val & -(S32) val) * 0x077CB531U)) >> 27];


}






static int LZ4_compressCtx(void *ctx, const char *source, char *dest, int isize, int osize)

{
	struct refTables *srt = (struct refTables *)ctx;
	HTYPE *HashTable = (HTYPE *) (srt->hashTable);

	const BYTE *ip = (BYTE *) source;
	INITBASE(base);
	const BYTE *anchor = ip;
	const BYTE *const iend = ip + isize;
	const BYTE *const oend = (BYTE *) dest + osize;
	const BYTE *const mflimit = iend - MFLIMIT;


	BYTE *op = (BYTE *) dest;

	int len, length;
	const int skipStrength = SKIPSTRENGTH;
	U32 forwardH;


	
	if (isize < MINLENGTH)
		goto _last_literals;

	
	HashTable[LZ4_HASH_VALUE(ip)] = ip - base;
	ip++;
	forwardH = LZ4_HASH_VALUE(ip);

	
	for (;;) {
		int findMatchAttempts = (1U << skipStrength) + 3;
		const BYTE *forwardIp = ip;
		const BYTE *ref;
		BYTE *token;

		
		do {
			U32 h = forwardH;
			int step = findMatchAttempts++ >> skipStrength;
			ip = forwardIp;
			forwardIp = ip + step;

			if (unlikely(forwardIp > mflimit)) {
				goto _last_literals;
			}

			forwardH = LZ4_HASH_VALUE(forwardIp);
			ref = base + HashTable[h];
			HashTable[h] = ip - base;

		} while ((ref < ip - MAX_DISTANCE) || (A32(ref) != A32(ip)));

		
		while ((ip > anchor) && (ref > (BYTE *) source) && unlikely(ip[-1] == ref[-1])) {
			ip--;
			ref--;
		}

		
		length = ip - anchor;
		token = op++;

		
		if (unlikely(op + length + (2 + 1 + LASTLITERALS) + (length >> 8) > oend))
			return (0);

		if (length >= (int)RUN_MASK) {
			*token = (RUN_MASK << ML_BITS);
			len = length - RUN_MASK;
			for (; len > 254; len -= 255)
				*op++ = 255;
			*op++ = (BYTE)len;
		} else *token = (length << ML_BITS);

		
		LZ4_BLINDCOPY(anchor, op, length);

		_next_match:
		
		LZ4_WRITE_LITTLEENDIAN_16(op, ip - ref);

		
		ip += MINMATCH;
		ref += MINMATCH;	
		anchor = ip;
		while (likely(ip < matchlimit - (STEPSIZE - 1))) {
			UARCH diff = AARCH(ref) ^ AARCH(ip);
			if (!diff) {
				ip += STEPSIZE;
				ref += STEPSIZE;
				continue;
			}
			ip += LZ4_NbCommonBytes(diff);
			goto _endCount;
		}

		if ((ip < (matchlimit - 3)) && (A32(ref) == A32(ip))) {
			ip += 4;
			ref += 4;
		}

		if ((ip < (matchlimit - 1)) && (A16(ref) == A16(ip))) {
			ip += 2;
			ref += 2;
		}
		if ((ip < matchlimit) && (*ref == *ip))
			ip++;
		_endCount:

		
		len = (ip - anchor);
		
		if (unlikely(op + (1 + LASTLITERALS) + (len >> 8) > oend))
			return (0);
		if (len >= (int)ML_MASK) {
			*token += ML_MASK;
			len -= ML_MASK;
			for (; len > 509; len -= 510) {
				*op++ = 255;
				*op++ = 255;
			}
			if (len > 254) {
				len -= 255;
				*op++ = 255;
			}
			*op++ = (BYTE)len;
		} else *token += len;

		
		if (ip > mflimit) {
			anchor = ip;
			break;
		}
		
		HashTable[LZ4_HASH_VALUE(ip - 2)] = ip - 2 - base;

		
		ref = base + HashTable[LZ4_HASH_VALUE(ip)];
		HashTable[LZ4_HASH_VALUE(ip)] = ip - base;
		if ((ref > ip - (MAX_DISTANCE + 1)) && (A32(ref) == A32(ip))) {
			token = op++;
			*token = 0;
			goto _next_match;
		}
		
		anchor = ip++;
		forwardH = LZ4_HASH_VALUE(ip);
	}

	_last_literals:
	
	{
		int lastRun = iend - anchor;
		if (op + lastRun + 1 + ((lastRun + 255 - RUN_MASK) / 255) > oend)
			return (0);
		if (lastRun >= (int)RUN_MASK) {
			*op++ = (RUN_MASK << ML_BITS);
			lastRun -= RUN_MASK;
			for (; lastRun > 254; lastRun -= 255) {
				*op++ = 255;
			}
			*op++ = (BYTE)lastRun;
		} else *op++ = (lastRun << ML_BITS);
		(void) memcpy(op, anchor, iend - anchor);
		op += iend - anchor;
	}

	
	return (int)(((char *)op) - dest);
}











static int LZ4_compress64kCtx(void *ctx, const char *source, char *dest, int isize, int osize)

{
	struct refTables *srt = (struct refTables *)ctx;
	U16 *HashTable = (U16 *) (srt->hashTable);

	const BYTE *ip = (BYTE *) source;
	const BYTE *anchor = ip;
	const BYTE *const base = ip;
	const BYTE *const iend = ip + isize;
	const BYTE *const oend = (BYTE *) dest + osize;
	const BYTE *const mflimit = iend - MFLIMIT;


	BYTE *op = (BYTE *) dest;

	int len, length;
	const int skipStrength = SKIPSTRENGTH;
	U32 forwardH;

	
	if (isize < MINLENGTH)
		goto _last_literals;

	
	ip++;
	forwardH = LZ4_HASH64K_VALUE(ip);

	
	for (;;) {
		int findMatchAttempts = (1U << skipStrength) + 3;
		const BYTE *forwardIp = ip;
		const BYTE *ref;
		BYTE *token;

		
		do {
			U32 h = forwardH;
			int step = findMatchAttempts++ >> skipStrength;
			ip = forwardIp;
			forwardIp = ip + step;

			if (forwardIp > mflimit) {
				goto _last_literals;
			}

			forwardH = LZ4_HASH64K_VALUE(forwardIp);
			ref = base + HashTable[h];
			HashTable[h] = ip - base;

		} while (A32(ref) != A32(ip));

		
		while ((ip > anchor) && (ref > (BYTE *) source) && (ip[-1] == ref[-1])) {
			ip--;
			ref--;
		}

		
		length = ip - anchor;
		token = op++;

		
		if (unlikely(op + length + (2 + 1 + LASTLITERALS) + (length >> 8) > oend))
			return (0);

		if (length >= (int)RUN_MASK) {
			*token = (RUN_MASK << ML_BITS);
			len = length - RUN_MASK;
			for (; len > 254; len -= 255)
				*op++ = 255;
			*op++ = (BYTE)len;
		} else *token = (length << ML_BITS);

		
		LZ4_BLINDCOPY(anchor, op, length);

		_next_match:
		
		LZ4_WRITE_LITTLEENDIAN_16(op, ip - ref);

		
		ip += MINMATCH;
		ref += MINMATCH;	
		anchor = ip;
		while (ip < matchlimit - (STEPSIZE - 1)) {
			UARCH diff = AARCH(ref) ^ AARCH(ip);
			if (!diff) {
				ip += STEPSIZE;
				ref += STEPSIZE;
				continue;
			}
			ip += LZ4_NbCommonBytes(diff);
			goto _endCount;
		}

		if ((ip < (matchlimit - 3)) && (A32(ref) == A32(ip))) {
			ip += 4;
			ref += 4;
		}

		if ((ip < (matchlimit - 1)) && (A16(ref) == A16(ip))) {
			ip += 2;
			ref += 2;
		}
		if ((ip < matchlimit) && (*ref == *ip))
			ip++;
		_endCount:

		
		len = (ip - anchor);
		
		if (unlikely(op + (1 + LASTLITERALS) + (len >> 8) > oend))
			return (0);
		if (len >= (int)ML_MASK) {
			*token += ML_MASK;
			len -= ML_MASK;
			for (; len > 509; len -= 510) {
				*op++ = 255;
				*op++ = 255;
			}
			if (len > 254) {
				len -= 255;
				*op++ = 255;
			}
			*op++ = (BYTE)len;
		} else *token += len;

		
		if (ip > mflimit) {
			anchor = ip;
			break;
		}
		
		HashTable[LZ4_HASH64K_VALUE(ip - 2)] = ip - 2 - base;

		
		ref = base + HashTable[LZ4_HASH64K_VALUE(ip)];
		HashTable[LZ4_HASH64K_VALUE(ip)] = ip - base;
		if (A32(ref) == A32(ip)) {
			token = op++;
			*token = 0;
			goto _next_match;
		}
		
		anchor = ip++;
		forwardH = LZ4_HASH64K_VALUE(ip);
	}

	_last_literals:
	
	{
		int lastRun = iend - anchor;
		if (op + lastRun + 1 + ((lastRun + 255 - RUN_MASK) / 255) > oend)
			return (0);
		if (lastRun >= (int)RUN_MASK) {
			*op++ = (RUN_MASK << ML_BITS);
			lastRun -= RUN_MASK;
			for (; lastRun > 254; lastRun -= 255)
				*op++ = 255;
			*op++ = (BYTE)lastRun;
		} else *op++ = (lastRun << ML_BITS);
		(void) memcpy(op, anchor, iend - anchor);
		op += iend - anchor;
	}

	
	return (int)(((char *)op) - dest);
}

static int real_LZ4_compress(const char *source, char *dest, int isize, int osize)
{
	void *ctx;
	int result;

	ctx = lz4_alloc(KM_SLEEP);

	
	if (ctx == NULL)
		return (0);

	memset(ctx, 0, sizeof (struct refTables));

	if (isize < LZ4_64KLIMIT)
		result = LZ4_compress64kCtx(ctx, source, dest, isize, osize);
	else result = LZ4_compressCtx(ctx, source, dest, isize, osize);

	lz4_free(ctx);
	return (result);
}





static const int dec32table[] = {0, 3, 2, 3, 0, 0, 0, 0};

static const int dec64table[] = {0, 0, 0, -1, 0, 1, 2, 3};


static int LZ4_uncompress_unknownOutputSize(const char *source, char *dest, int isize, int maxOutputSize)

{
	
	const BYTE *restrict ip = (const BYTE *) source;
	const BYTE *const iend = ip + isize;
	const BYTE *ref;

	BYTE *op = (BYTE *) dest;
	BYTE *const oend = op + maxOutputSize;
	BYTE *cpy;

	
	while (ip < iend) {
		unsigned token;
		size_t length;

		
		token = *ip++;
		if ((length = (token >> ML_BITS)) == RUN_MASK) {
			int s = 255;
			while ((ip < iend) && (s == 255)) {
				s = *ip++;
				if (unlikely(length > (size_t)(length + s)))
					goto _output_error;
				length += s;
			}
		}
		
		cpy = op + length;
		
		if (cpy < op)
			goto _output_error;	
		if ((cpy > oend - COPYLENGTH) || (ip + length > iend - COPYLENGTH)) {
			if (cpy > oend)
				
				goto _output_error;
			if (ip + length != iend)
				
				goto _output_error;
			(void) memcpy(op, ip, length);
			op += length;
			
			break;
		}
		LZ4_WILDCOPY(ip, op, cpy);
		ip -= (op - cpy);
		op = cpy;

		
		LZ4_READ_LITTLEENDIAN_16(ref, cpy, ip);
		ip += 2;
		if (ref < (BYTE * const) dest)
			
			goto _output_error;

		
		if ((length = (token & ML_MASK)) == ML_MASK) {
			while (ip < iend) {
				int s = *ip++;
				if (unlikely(length > (size_t)(length + s)))
					goto _output_error;
				length += s;
				if (s == 255)
					continue;
				break;
			}
		}
		
		if (unlikely(op - ref < STEPSIZE)) {

			int dec64 = dec64table[op - ref];

			const int dec64 = 0;

			op[0] = ref[0];
			op[1] = ref[1];
			op[2] = ref[2];
			op[3] = ref[3];
			op += 4;
			ref += 4;
			ref -= dec32table[op - ref];
			A32(op) = A32(ref);
			op += STEPSIZE - 4;
			ref -= dec64;
		} else {
			LZ4_COPYSTEP(ref, op);
		}
		cpy = op + length - (STEPSIZE - 4);
		if (cpy > oend - COPYLENGTH) {
			if (cpy > oend)
				
				goto _output_error;

			if ((ref + COPYLENGTH) > oend)

			if ((ref + COPYLENGTH) > oend || (op + COPYLENGTH) > oend)

				goto _output_error;
			LZ4_SECURECOPY(ref, op, (oend - COPYLENGTH));
			while (op < cpy)
				*op++ = *ref++;
			op = cpy;
			if (op == oend)
				
				goto _output_error;
			continue;
		}
		LZ4_SECURECOPY(ref, op, cpy);
		op = cpy;	
	}

	
	return (int)(((char *)op) - dest);

	
	_output_error:
	return (-1);
}



_Static_assert(sizeof(struct refTables) <= 16384, "refTables too big for malloc");
_Static_assert((sizeof(struct refTables) % 4096) == 0, "refTables not a multiple of page size");





static kmem_cache_t *lz4_cache;

void lz4_init(void)
{
	lz4_cache = kmem_cache_create("lz4_cache", sizeof (struct refTables), 0, NULL, NULL, NULL, NULL, NULL, 0);
}

void lz4_fini(void)
{
	if (lz4_cache) {
		kmem_cache_destroy(lz4_cache);
		lz4_cache = NULL;
	}
}

static void * lz4_alloc(int flags)
{
	ASSERT(lz4_cache != NULL);
	return (kmem_cache_alloc(lz4_cache, flags));
}

static void lz4_free(void *ctx)
{
	kmem_cache_free(lz4_cache, ctx);
}

void lz4_init(void)
{
}

void lz4_fini(void)
{
}

static void * lz4_alloc(int flags)
{
	return (kmem_alloc(sizeof (struct refTables), flags));
}

static void lz4_free(void *ctx)
{
	kmem_free(ctx, sizeof (struct refTables));
}


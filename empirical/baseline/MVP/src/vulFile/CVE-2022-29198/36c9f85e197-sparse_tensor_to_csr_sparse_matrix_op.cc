



























using ::perftools::gputools::cuda::ScopedActivateExecutorContext;


using ::perftools::gputools::rocm::ScopedActivateExecutorContext;


namespace tensorflow {

typedef Eigen::ThreadPoolDevice CPUDevice;
typedef Eigen::GpuDevice GPUDevice;







template <typename T> class SparseTensorToCSRSparseMatrixCPUOp : public OpKernel {
 public:
  explicit SparseTensorToCSRSparseMatrixCPUOp(OpKernelConstruction* c)
      : OpKernel(c) {}

  void Compute(OpKernelContext* ctx) final {
    const Tensor& indices = ctx->input(0);
    const Tensor& values = ctx->input(1);
    const Tensor& dense_shape = ctx->input(2);
    const int rank = dense_shape.NumElements();
    OP_REQUIRES(ctx, rank == 2 || rank == 3, errors::InvalidArgument("SparseTensor must have rank 2 or 3; ", "but indices has rank: ", rank));

    auto dense_shape_vec = dense_shape.vec<int64_t>();
    const int64_t batch_size = (rank == 2) ? 1 : dense_shape_vec(0);
    const int64_t num_rows = dense_shape_vec((rank == 2) ? 0 : 1);
    const int64_t total_nnz = values.NumElements();

    
    TensorShape batch_ptr_shape;
    OP_REQUIRES_OK( ctx, TensorShape::BuildTensorShape({batch_size + 1}, &batch_ptr_shape));
    Tensor batch_ptr(cpu_allocator(), DT_INT32, batch_ptr_shape);
    TensorShape csr_col_ind_shape;
    OP_REQUIRES_OK( ctx, TensorShape::BuildTensorShape({total_nnz}, &csr_col_ind_shape));
    Tensor csr_col_ind(cpu_allocator(), DT_INT32, csr_col_ind_shape);
    TensorShape csr_row_ind_shape;
    OP_REQUIRES_OK(ctx, TensorShape::BuildTensorShape( {(num_rows + 1) * batch_size}, &csr_row_ind_shape));
    Tensor csr_row_ptr(cpu_allocator(), DT_INT32, csr_row_ind_shape);

    
    functor::SetZeroFunctor<CPUDevice, int32> set_zero;
    set_zero(ctx->eigen_device<CPUDevice>(), csr_row_ptr.flat<int32>());

    
    functor::SparseTensorToCSRSparseMatrixCPUFunctor coo_to_csr;
    OP_REQUIRES_OK( ctx, coo_to_csr(batch_size, num_rows, indices.template matrix<int64_t>(), batch_ptr.vec<int32>(), csr_row_ptr.vec<int32>(), csr_col_ind.vec<int32>()));




    
    
    CSRSparseMatrix output_csr_matrix;
    OP_REQUIRES_OK( ctx, CSRSparseMatrix::CreateCSRSparseMatrix( DataTypeToEnum<T>::value, dense_shape, batch_ptr, csr_row_ptr, csr_col_ind, values, &output_csr_matrix));


    Tensor* output_csr_matrix_tensor;
    AllocatorAttributes cpu_alloc;
    cpu_alloc.set_on_host(true);
    OP_REQUIRES_OK( ctx, ctx->allocate_output(0, TensorShape({}), &output_csr_matrix_tensor, cpu_alloc));

    output_csr_matrix_tensor->scalar<Variant>()() = std::move(output_csr_matrix);
  }
};



template <typename Device, typename T> class SparseTensorToCSRSparseMatrixGPUOp : public AsyncOpKernel {
 public:
  explicit SparseTensorToCSRSparseMatrixGPUOp(OpKernelConstruction* c)
      : AsyncOpKernel(c) {}

  void ComputeAsync(OpKernelContext* c, DoneCallback done) final {
    auto stream = c->op_device_context()->stream();
    const Device& d = c->eigen_device<Device>();

    const Tensor& indices_t = c->input(0);
    const Tensor& values_t = c->input(1);
    const Tensor& dense_shape_t = c->input(2);
    const int rank = dense_shape_t.NumElements();
    OP_REQUIRES_ASYNC( c, rank == 2 || rank == 3, errors::InvalidArgument("sparse tensor must have rank == 2 or 3; ", "but indices has ", rank, " columns"), done);



    auto dense_shape = dense_shape_t.vec<int64_t>();
    const int64_t batch_size = (rank == 2) ? 1 : dense_shape(0);
    const int64_t rows = dense_shape((rank == 2) ? 0 : 1);
    const int64_t cols = dense_shape((rank == 2) ? 1 : 2);

    ScratchSpace<int32> nnz_per_batch_host(c, batch_size,  true);

    Tensor nnz_per_batch_device_t;
    if (rank == 2) {
      
      nnz_per_batch_host.mutable_data()[0] = indices_t.dim_size(0);
    } else {
      OP_REQUIRES_OK_ASYNC(c, c->allocate_temp(DT_INT32, TensorShape({batch_size}), &nnz_per_batch_device_t), done);


      auto nnz_per_batch_device = nnz_per_batch_device_t.vec<int32>();

      functor::CalculateNNZPerBatchMatrixFromIndices<Device> calculate_nnz_from_indices;
      auto indices = indices_t.matrix<int64_t>();
      OP_REQUIRES_OK_ASYNC( c, calculate_nnz_from_indices(c, indices, nnz_per_batch_device), done);


      perftools::gputools::DeviceMemoryBase nnz_per_batch_device_ptr( static_cast<void*>(nnz_per_batch_device.data()));

      OP_REQUIRES_ASYNC( c, stream ->ThenMemcpy(nnz_per_batch_host.mutable_data() , nnz_per_batch_device_ptr , batch_size * sizeof(int32) )




              .ok(), errors::Internal("SparseTensorToSparseMatrixGPUOp: failed to copy " "nnz_per_batch from device"), done);


    }

    TensorReference nnz_per_batch_device_ref(nnz_per_batch_device_t);
    auto convert_to_csr = [this, c, batch_size, nnz_per_batch_host, nnz_per_batch_device_ref, stream, &d, &values_t, &indices_t, &dense_shape_t, dense_shape, rows, cols, rank, done]() {


      
      
      nnz_per_batch_device_ref.Unref();

      auto nnz_per_batch = nnz_per_batch_host.tensor().vec<int32>();

      
      
      ScopedActivateExecutorContext scoped_activation{stream->parent()};
      Tensor batch_ptr_t(cpu_allocator(), DT_INT32, TensorShape({batch_size + 1}));

      auto batch_ptr = batch_ptr_t.vec<int32>();
      auto indices = indices_t.matrix<int64_t>();

      batch_ptr(0) = 0;
      for (int i = 0; i < batch_size; ++i) {
        batch_ptr(i + 1) = batch_ptr(i) + nnz_per_batch(i);
      }
      int total_nnz = batch_ptr(batch_size);
      OP_REQUIRES_ASYNC( c, total_nnz == values_t.NumElements(), errors::Internal("nnz returned by " "CalculateNNZPerBatchMatrixFromInd" "ices != len(values): ", total_nnz, " vs. ", values_t.NumElements()), done);






      Tensor coo_col_ind_t;
      Tensor csr_row_ptr_t;
      Tensor csr_values_t = values_t;

      Tensor coo_row_ind_t;
      OP_REQUIRES_OK_ASYNC( c, c->allocate_temp(DT_INT32, TensorShape({total_nnz}), &coo_row_ind_t), done);


      OP_REQUIRES_OK_ASYNC( c, c->allocate_temp(DT_INT32, TensorShape({total_nnz}), &coo_col_ind_t), done);


      OP_REQUIRES_OK_ASYNC( c, c->allocate_temp(DT_INT32, TensorShape({batch_size * (rows + 1)}), &csr_row_ptr_t), done);




      auto coo_row_ind = coo_row_ind_t.vec<int32>();
      auto coo_col_ind = coo_col_ind_t.vec<int32>();
      auto csr_row_ptr = csr_row_ptr_t.vec<int32>();

      
      if (total_nnz > 0) {
        functor::SparseTensorToCOOSparseMatrix<Device> st_to_coo;
        st_to_coo(d, dense_shape, indices, coo_row_ind, coo_col_ind);
      }

      
      
      
      
      
      
      functor::SetZeroFunctor<Device, int32> set_zero;
      set_zero(d, csr_row_ptr_t.flat<int32>());

      functor::COOSparseMatrixToCSRSparseMatrix<Device> coo_to_csr;
      for (int i = 0; i < batch_size; ++i) {
        int nnz_i = batch_ptr(i + 1) - batch_ptr(i);
        if (nnz_i == 0) {
          
          
        } else {
          
          auto coo_row_ind_i = TTypes<int32>::UnalignedVec(&coo_row_ind(batch_ptr(i)), nnz_i);
          auto csr_row_ptr_i = TTypes<int32>::UnalignedVec( &csr_row_ptr((rows + 1) * i), rows + 1);
          OP_REQUIRES_OK_ASYNC( c, coo_to_csr(c, rows, cols, coo_row_ind_i, csr_row_ptr_i), done);
        }
      }

      CSRSparseMatrix matrix;
      OP_REQUIRES_OK_ASYNC( c, CSRSparseMatrix::CreateCSRSparseMatrix( values_t.dtype(), dense_shape_t, batch_ptr_t, csr_row_ptr_t, coo_col_ind_t, csr_values_t, &matrix), done);




      Tensor* matrix_t;
      AllocatorAttributes cpu_alloc;
      cpu_alloc.set_on_host(true);
      OP_REQUIRES_OK_ASYNC( c, c->allocate_output(0, TensorShape({}), &matrix_t, cpu_alloc), done);

      matrix_t->scalar<Variant>()() = std::move(matrix);

      done();
    };

    if (rank == 2) {
      convert_to_csr();
    } else {
      
      c->device()->tensorflow_accelerator_device_info()->event_mgr->ThenExecute( stream, convert_to_csr);
    }
  }
};

namespace functor {

template <> Status CalculateNNZPerBatchMatrixFromIndices<GPUDevice>::operator()( OpKernelContext* c, TTypes<int64_t>::ConstMatrix indices, TTypes<int32>::Vec nnz_per_batch);


extern template struct CalculateNNZPerBatchMatrixFromIndices<GPUDevice>;

template <> struct SparseTensorToCOOSparseMatrix<GPUDevice> {
  void operator()(const GPUDevice& d, TTypes<int64_t>::ConstVec host_dense_shape, TTypes<int64_t>::ConstMatrix indices, TTypes<int>::Vec coo_row_ind, TTypes<int>::Vec coo_col_ind);


};
extern template struct SparseTensorToCOOSparseMatrix<GPUDevice>;

template <> struct COOSparseMatrixToCSRSparseMatrix<GPUDevice> {
  Status operator()(OpKernelContext* c, const int rows, const int cols, TTypes<int>::UnalignedVec coo_row_ind, TTypes<int>::UnalignedVec csr_row_ptr) {

    GpuSparse cuda_sparse(c);
    TF_RETURN_IF_ERROR(cuda_sparse.Initialize());
    return cuda_sparse.Coo2csr(coo_row_ind.data(), coo_row_ind.size(), rows, csr_row_ptr.data());

  }
};
extern template struct COOSparseMatrixToCSRSparseMatrix<GPUDevice>;

}  







REGISTER_GPU(float)
REGISTER_GPU(double)
REGISTER_GPU(complex64)
REGISTER_GPU(complex128)










REGISTER_CPU(float)
REGISTER_CPU(double)
REGISTER_CPU(complex64)
REGISTER_CPU(complex128)



}  

package org.apache.beam.sdk.io.mongodb;
import static org.apache.beam.sdk.io.mongodb.FindQuery.bson2BsonDocument;
import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;
import com.google.auto.value.AutoValue;
import com.mongodb.BasicDBObject;
import com.mongodb.MongoBulkWriteException;
import com.mongodb.MongoClient;
import com.mongodb.MongoClientOptions;
import com.mongodb.MongoClientURI;
import com.mongodb.client.AggregateIterable;
import com.mongodb.client.MongoCollection;
import com.mongodb.client.MongoCursor;
import com.mongodb.client.MongoDatabase;
import com.mongodb.client.model.Aggregates;
import com.mongodb.client.model.Filters;
import com.mongodb.client.model.InsertManyOptions;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collections;
import java.util.List;
import java.util.stream.Collectors;
import javax.annotation.Nullable;
import org.apache.beam.sdk.annotations.Experimental;
import org.apache.beam.sdk.coders.Coder;
import org.apache.beam.sdk.coders.SerializableCoder;
import org.apache.beam.sdk.io.BoundedSource;
import org.apache.beam.sdk.options.PipelineOptions;
import org.apache.beam.sdk.transforms.DoFn;
import org.apache.beam.sdk.transforms.PTransform;
import org.apache.beam.sdk.transforms.ParDo;
import org.apache.beam.sdk.transforms.SerializableFunction;
import org.apache.beam.sdk.transforms.display.DisplayData;
import org.apache.beam.sdk.values.PBegin;
import org.apache.beam.sdk.values.PCollection;
import org.apache.beam.sdk.values.PDone;
import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;
import org.bson.BsonDocument;
import org.bson.BsonInt32;
import org.bson.BsonString;
import org.bson.Document;
import org.bson.conversions.Bson;
import org.bson.types.ObjectId;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
public class MongoDbIO {
  private static final Logger LOG = LoggerFactory.getLogger(MongoDbIO.class);
  public static Read read() {
    return new AutoValue_MongoDbIO_Read.Builder() .setMaxConnectionIdleTime(60000) .setNumSplits(0) .setBucketAuto(false) .setSslEnabled(false) .setIgnoreSSLCertificate(false) .setSslInvalidHostNameAllowed(false) .setQueryFn(FindQuery.create()) .build();
  }
  
  public static Write write() {
    return new AutoValue_MongoDbIO_Write.Builder() .setMaxConnectionIdleTime(60000) .setBatchSize(1024L) .setSslEnabled(false) .setIgnoreSSLCertificate(false) .setSslInvalidHostNameAllowed(false) .setOrdered(true) .build();
  }
  
  private MongoDbIO() {}
  public abstract static class Read extends PTransform<PBegin, PCollection<Document>> {
    @Nullable
    abstract String uri();
    abstract int maxConnectionIdleTime();
    abstract boolean sslEnabled();
    abstract boolean sslInvalidHostNameAllowed();
    abstract boolean ignoreSSLCertificate();
    @Nullable
    abstract String database();
    @Nullable
    abstract String collection();
    abstract int numSplits();
    abstract boolean bucketAuto();
    abstract SerializableFunction<MongoCollection<Document>, MongoCursor<Document>> queryFn();
    abstract Builder builder();
    @AutoValue.Builder
    abstract static class Builder {
      abstract Builder setUri(String uri);
      abstract Builder setMaxConnectionIdleTime(int maxConnectionIdleTime);
      abstract Builder setSslEnabled(boolean value);
      abstract Builder setSslInvalidHostNameAllowed(boolean value);
      abstract Builder setIgnoreSSLCertificate(boolean value);
      abstract Builder setDatabase(String database);
      abstract Builder setCollection(String collection);
      abstract Builder setNumSplits(int numSplits);
      abstract Builder setBucketAuto(boolean bucketAuto);
      abstract Builder setQueryFn( SerializableFunction<MongoCollection<Document>, MongoCursor<Document>> queryBuilder);
      abstract Read build();
    }
    
    public Read withUri(String uri) {
      checkArgument(uri != null, "MongoDbIO.read().withUri(uri) called with null uri");
      return builder().setUri(uri).build();
    }
    
    public Read withMaxConnectionIdleTime(int maxConnectionIdleTime) {
      return builder().setMaxConnectionIdleTime(maxConnectionIdleTime).build();
    }
    
    public Read withSSLEnabled(boolean sslEnabled) {
      return builder().setSslEnabled(sslEnabled).build();
    }
    
    public Read withSSLInvalidHostNameAllowed(boolean invalidHostNameAllowed) {
      return builder().setSslInvalidHostNameAllowed(invalidHostNameAllowed).build();
    }
    
    public Read withIgnoreSSLCertificate(boolean ignoreSSLCertificate) {
      return builder().setIgnoreSSLCertificate(ignoreSSLCertificate).build();
    }
    
    public Read withDatabase(String database) {
      checkArgument(database != null, "database can not be null");
      return builder().setDatabase(database).build();
    }
    
    public Read withCollection(String collection) {
      checkArgument(collection != null, "collection can not be null");
      return builder().setCollection(collection).build();
    }
    
    public Read withFilter(String filter) {
      checkArgument(filter != null, "filter can not be null");
      checkArgument( this.queryFn().getClass() != FindQuery.class, "withFilter is only supported for FindQuery API");
      FindQuery findQuery = (FindQuery) queryFn();
      FindQuery queryWithFilter = findQuery.toBuilder().setFilters(bson2BsonDocument(Document.parse(filter))).build();
      return builder().setQueryFn(queryWithFilter).build();
    }
    
    public Read withProjection(final String... fieldNames) {
      checkArgument(fieldNames.length > 0, "projection can not be null");
      checkArgument( this.queryFn().getClass() != FindQuery.class, "withFilter is only supported for FindQuery API");
      FindQuery findQuery = (FindQuery) queryFn();
      FindQuery queryWithProjection = findQuery.toBuilder().setProjection(Arrays.asList(fieldNames)).build();
      return builder().setQueryFn(queryWithProjection).build();
    }
    
    public Read withNumSplits(int numSplits) {
      checkArgument(numSplits >= 0, "invalid num_splits: must be >= 0, but was %s", numSplits);
      return builder().setNumSplits(numSplits).build();
    }
    
    public Read withBucketAuto(boolean bucketAuto) {
      return builder().setBucketAuto(bucketAuto).build();
    }
    
    public Read withQueryFn( SerializableFunction<MongoCollection<Document>, MongoCursor<Document>> queryBuilderFn) {
      return builder().setQueryFn(queryBuilderFn).build();
    }
    
    public PCollection<Document> expand(PBegin input) {
      checkArgument(uri() != null, "withUri() is required");
      checkArgument(database() != null, "withDatabase() is required");
      checkArgument(collection() != null, "withCollection() is required");
      return input.apply(org.apache.beam.sdk.io.Read.from(new BoundedMongoDbSource(this)));
    }
    
    public void populateDisplayData(DisplayData.Builder builder) {
      super.populateDisplayData(builder);
      builder.add(DisplayData.item("uri", uri()));
      builder.add(DisplayData.item("maxConnectionIdleTime", maxConnectionIdleTime()));
      builder.add(DisplayData.item("sslEnabled", sslEnabled()));
      builder.add(DisplayData.item("sslInvalidHostNameAllowed", sslInvalidHostNameAllowed()));
      builder.add(DisplayData.item("ignoreSSLCertificate", ignoreSSLCertificate()));
      builder.add(DisplayData.item("database", database()));
      builder.add(DisplayData.item("collection", collection()));
      builder.add(DisplayData.item("numSplit", numSplits()));
      builder.add(DisplayData.item("bucketAuto", bucketAuto()));
      builder.add(DisplayData.item("queryFn", queryFn().toString()));
    }
    
  }
  
  private static MongoClientOptions.Builder getOptions( int maxConnectionIdleTime, boolean sslEnabled, boolean sslInvalidHostNameAllowed) {
    MongoClientOptions.Builder optionsBuilder = new MongoClientOptions.Builder();
    optionsBuilder.maxConnectionIdleTime(maxConnectionIdleTime);
    if (sslEnabled) {
      optionsBuilder .sslEnabled(sslEnabled) .sslInvalidHostNameAllowed(sslInvalidHostNameAllowed) .sslContext(SSLUtils.ignoreSSLCertificate());
    }
    
    return optionsBuilder;
  }
  
  @VisibleForTesting
  static class BoundedMongoDbSource extends BoundedSource<Document> {
    private final Read spec;
    private BoundedMongoDbSource(Read spec) {
      this.spec = spec;
    }
    
    public Coder<Document> getOutputCoder() {
      return SerializableCoder.of(Document.class);
    }
    
    public void populateDisplayData(DisplayData.Builder builder) {
      spec.populateDisplayData(builder);
    }
    
    public BoundedReader<Document> createReader(PipelineOptions options) {
      return new BoundedMongoDbReader(this);
    }
    
    public long getEstimatedSizeBytes(PipelineOptions pipelineOptions) {
      try (MongoClient mongoClient = new MongoClient( new MongoClientURI( spec.uri(), getOptions( spec.maxConnectionIdleTime(), spec.sslEnabled(), spec.sslInvalidHostNameAllowed())))) {
        return getEstimatedSizeBytes(mongoClient, spec.database(), spec.collection());
      }
      
    }
    
    private long getEstimatedSizeBytes( MongoClient mongoClient, String database, String collection) {
      MongoDatabase mongoDatabase = mongoClient.getDatabase(database);
      BasicDBObject stat = new BasicDBObject();
      stat.append("collStats", collection);
      Document stats = mongoDatabase.runCommand(stat);
      return stats.get("size", Number.class).longValue();
    }
    
    public List<BoundedSource<Document>> split( long desiredBundleSizeBytes, PipelineOptions options) {
      try (MongoClient mongoClient = new MongoClient( new MongoClientURI( spec.uri(), getOptions( spec.maxConnectionIdleTime(), spec.sslEnabled(), spec.sslInvalidHostNameAllowed())))) {
        MongoDatabase mongoDatabase = mongoClient.getDatabase(spec.database());
        List<Document> splitKeys;
        List<BoundedSource<Document>> sources = new ArrayList<>();
        if (spec.queryFn().getClass() == AutoValue_FindQuery.class) {
          if (spec.bucketAuto()) {
            splitKeys = buildAutoBuckets(mongoDatabase, spec);
          }
           else {
            if (spec.numSplits() > 0) {
              long estimatedSizeBytes = getEstimatedSizeBytes(mongoClient, spec.database(), spec.collection());
              desiredBundleSizeBytes = estimatedSizeBytes / spec.numSplits();
            }
            
            if (desiredBundleSizeBytes < 1024L * 1024L) {
              desiredBundleSizeBytes = 1024L * 1024L;
            }
            
            BasicDBObject splitVectorCommand = new BasicDBObject();
            splitVectorCommand.append("splitVector", spec.database() + "." + spec.collection());
            splitVectorCommand.append("keyPattern", new BasicDBObject().append("_id", 1));
            splitVectorCommand.append("force", false);
            LOG.debug("Splitting in chunk of {} MB", desiredBundleSizeBytes / 1024 / 1024);
            splitVectorCommand.append("maxChunkSize", desiredBundleSizeBytes / 1024 / 1024);
            Document splitVectorCommandResult = mongoDatabase.runCommand(splitVectorCommand);
            splitKeys = (List<Document>) splitVectorCommandResult.get("splitKeys");
          }
          
          if (splitKeys.size() < 1) {
            LOG.debug("Split keys is low, using an unique source");
            return Collections.singletonList(this);
          }
          
          for (String shardFilter : splitKeysToFilters(splitKeys)) {
            SerializableFunction<MongoCollection<Document>, MongoCursor<Document>> queryFn = spec.queryFn();
            BsonDocument filters = bson2BsonDocument(Document.parse(shardFilter));
            FindQuery findQuery = (FindQuery) queryFn;
            final BsonDocument allFilters = bson2BsonDocument( findQuery.filters() != null ? Filters.and(findQuery.filters(), filters) : filters);
            FindQuery queryWithFilter = findQuery.toBuilder().setFilters(allFilters).build();
            LOG.debug("using filters: " + allFilters.toJson());
            sources.add(new BoundedMongoDbSource(spec.withQueryFn(queryWithFilter)));
          }
          
        }
         else {
          SerializableFunction<MongoCollection<Document>, MongoCursor<Document>> queryFn = spec.queryFn();
          AggregationQuery aggregationQuery = (AggregationQuery) queryFn;
          if (aggregationQuery.mongoDbPipeline().stream() .anyMatch(s -> s.keySet().contains("$limit"))) {
            return Collections.singletonList(this);
          }
          
          splitKeys = buildAutoBuckets(mongoDatabase, spec);
          for (BsonDocument shardFilter : splitKeysToMatch(splitKeys)) {
            AggregationQuery queryWithBucket = aggregationQuery.toBuilder().setBucket(shardFilter).build();
            sources.add(new BoundedMongoDbSource(spec.withQueryFn(queryWithBucket)));
          }
          
        }
        
        return sources;
      }
      
    }
    
    @VisibleForTesting
    static List<String> splitKeysToFilters(List<Document> splitKeys) {
      ArrayList<String> filters = new ArrayList<>();
      String lowestBound = null; 
      for (int i = 0; i < splitKeys.size(); i++) {
        String splitKey = splitKeys.get(i).get("_id").toString();
        String rangeFilter;
        if (i == 0) {
          rangeFilter = String.format("{ $and: [ {\"_id\":{$lte:ObjectId(\"%s\")}}", splitKey);
          filters.add(String.format("%s ]}", rangeFilter));
          if (splitKeys.size() == 1) {
            rangeFilter = String.format("{ $and: [ {\"_id\":{$gt:ObjectId(\"%s\")}}", splitKey);
            filters.add(String.format("%s ]}", rangeFilter));
          }
          
        }
         else if (i == splitKeys.size() - 1) {
          rangeFilter = String.format( "{ $and: [ {\"_id\":{$gt:ObjectId(\"%s\")," + "$lte:ObjectId(\"%s\")}}", lowestBound, splitKey);
          filters.add(String.format("%s ]}", rangeFilter));
          rangeFilter = String.format("{ $and: [ {\"_id\":{$gt:ObjectId(\"%s\")}}", splitKey);
          filters.add(String.format("%s ]}", rangeFilter));
        }
         else {
          rangeFilter = String.format( "{ $and: [ {\"_id\":{$gt:ObjectId(\"%s\")," + "$lte:ObjectId(\"%s\")}}", lowestBound, splitKey);
          filters.add(String.format("%s ]}", rangeFilter));
        }
        
        lowestBound = splitKey;
      }
      
      return filters;
    }
    
    @VisibleForTesting
    static List<BsonDocument> splitKeysToMatch(List<Document> splitKeys) {
      List<Bson> aggregates = new ArrayList<>();
      ObjectId lowestBound = null; 
      for (int i = 0; i < splitKeys.size(); i++) {
        ObjectId splitKey = splitKeys.get(i).getObjectId("_id");
        String rangeFilter;
        if (i == 0) {
          aggregates.add(Aggregates.match(Filters.lte("_id", splitKey)));
          if (splitKeys.size() == 1) {
            aggregates.add(Aggregates.match(Filters.and(Filters.gt("_id", splitKey))));
          }
          
        }
         else if (i == splitKeys.size() - 1) {
          aggregates.add( Aggregates.match( Filters.and(Filters.gt("_id", lowestBound), Filters.lte("_id", splitKey))));
          aggregates.add(Aggregates.match(Filters.and(Filters.gt("_id", splitKey))));
        }
         else {
          aggregates.add( Aggregates.match( Filters.and(Filters.gt("_id", lowestBound), Filters.lte("_id", splitKey))));
        }
        
        lowestBound = splitKey;
      }
      
      return aggregates.stream() .map(s -> s.toBsonDocument(BasicDBObject.class, MongoClient.getDefaultCodecRegistry())) .collect(Collectors.toList());
    }
    
    @VisibleForTesting
    static List<Document> buildAutoBuckets(MongoDatabase mongoDatabase, Read spec) {
      List<Document> splitKeys = new ArrayList<>();
      MongoCollection<Document> mongoCollection = mongoDatabase.getCollection(spec.collection());
      BsonDocument bucketAutoConfig = new BsonDocument();
      bucketAutoConfig.put("groupBy", new BsonString("$_id"));
      bucketAutoConfig.put("buckets", new BsonInt32(spec.numSplits() > 0 ? spec.numSplits() : 10));
      BsonDocument bucketAuto = new BsonDocument("$bucketAuto", bucketAutoConfig);
      List<BsonDocument> aggregates = new ArrayList<>();
      aggregates.add(bucketAuto);
      AggregateIterable<Document> buckets = mongoCollection.aggregate(aggregates);
      for (Document bucket : buckets) {
        Document filter = new Document();
        filter.put("_id", ((Document) bucket.get("_id")).get("min"));
        splitKeys.add(filter);
      }
      
      return splitKeys;
    }
    
  }
  
  private static class BoundedMongoDbReader extends BoundedSource.BoundedReader<Document> {
    private final BoundedMongoDbSource source;
    private MongoClient client;
    private MongoCursor<Document> cursor;
    private Document current;
    BoundedMongoDbReader(BoundedMongoDbSource source) {
      this.source = source;
    }
    
    public boolean start() {
      Read spec = source.spec;
      client = createClient(spec);
      MongoDatabase mongoDatabase = client.getDatabase(spec.database());
      MongoCollection<Document> mongoCollection = mongoDatabase.getCollection(spec.collection());
      cursor = spec.queryFn().apply(mongoCollection);
      return advance();
    }
    
    public boolean advance() {
      if (cursor.hasNext()) {
        current = cursor.next();
        return true;
      }
      
      return false;
    }
    
    public BoundedMongoDbSource getCurrentSource() {
      return source;
    }
    
    public Document getCurrent() {
      return current;
    }
    
    public void close() {
      try {
        if (cursor != null) {
          cursor.close();
        }
        
      }
       catch (Exception e) {
        LOG.warn("Error closing MongoDB cursor", e);
      }
      
      try {
        client.close();
      }
       catch (Exception e) {
        LOG.warn("Error closing MongoDB client", e);
      }
      
    }
    
    private MongoClient createClient(Read spec) {
      return new MongoClient( new MongoClientURI( spec.uri(), getOptions( spec.maxConnectionIdleTime(), spec.sslEnabled(), spec.sslInvalidHostNameAllowed())));
    }
    
  }
  
  public abstract static class Write extends PTransform<PCollection<Document>, PDone> {
    @Nullable
    abstract String uri();
    abstract int maxConnectionIdleTime();
    abstract boolean sslEnabled();
    abstract boolean sslInvalidHostNameAllowed();
    abstract boolean ignoreSSLCertificate();
    abstract boolean ordered();
    @Nullable
    abstract String database();
    @Nullable
    abstract String collection();
    abstract long batchSize();
    abstract Builder builder();
    @AutoValue.Builder
    abstract static class Builder {
      abstract Builder setUri(String uri);
      abstract Builder setMaxConnectionIdleTime(int maxConnectionIdleTime);
      abstract Builder setSslEnabled(boolean value);
      abstract Builder setSslInvalidHostNameAllowed(boolean value);
      abstract Builder setIgnoreSSLCertificate(boolean value);
      abstract Builder setOrdered(boolean value);
      abstract Builder setDatabase(String database);
      abstract Builder setCollection(String collection);
      abstract Builder setBatchSize(long batchSize);
      abstract Write build();
    }
    
    public Write withUri(String uri) {
      checkArgument(uri != null, "uri can not be null");
      return builder().setUri(uri).build();
    }
    
    public Write withMaxConnectionIdleTime(int maxConnectionIdleTime) {
      return builder().setMaxConnectionIdleTime(maxConnectionIdleTime).build();
    }
    
    public Write withSSLEnabled(boolean sslEnabled) {
      return builder().setSslEnabled(sslEnabled).build();
    }
    
    public Write withSSLInvalidHostNameAllowed(boolean invalidHostNameAllowed) {
      return builder().setSslInvalidHostNameAllowed(invalidHostNameAllowed).build();
    }
    
    public Write withOrdered(boolean ordered) {
      return builder().setOrdered(ordered).build();
    }
    
    public Write withIgnoreSSLCertificate(boolean ignoreSSLCertificate) {
      return builder().setIgnoreSSLCertificate(ignoreSSLCertificate).build();
    }
    
    public Write withDatabase(String database) {
      checkArgument(database != null, "database can not be null");
      return builder().setDatabase(database).build();
    }
    
    public Write withCollection(String collection) {
      checkArgument(collection != null, "collection can not be null");
      return builder().setCollection(collection).build();
    }
    
    public Write withBatchSize(long batchSize) {
      checkArgument(batchSize >= 0, "Batch size must be >= 0, but was %s", batchSize);
      return builder().setBatchSize(batchSize).build();
    }
    
    public PDone expand(PCollection<Document> input) {
      checkArgument(uri() != null, "withUri() is required");
      checkArgument(database() != null, "withDatabase() is required");
      checkArgument(collection() != null, "withCollection() is required");
      input.apply(ParDo.of(new WriteFn(this)));
      return PDone.in(input.getPipeline());
    }
    
    public void populateDisplayData(DisplayData.Builder builder) {
      builder.add(DisplayData.item("uri", uri()));
      builder.add(DisplayData.item("maxConnectionIdleTime", maxConnectionIdleTime()));
      builder.add(DisplayData.item("sslEnable", sslEnabled()));
      builder.add(DisplayData.item("sslInvalidHostNameAllowed", sslInvalidHostNameAllowed()));
      builder.add(DisplayData.item("ignoreSSLCertificate", ignoreSSLCertificate()));
      builder.add(DisplayData.item("ordered", ordered()));
      builder.add(DisplayData.item("database", database()));
      builder.add(DisplayData.item("collection", collection()));
      builder.add(DisplayData.item("batchSize", batchSize()));
    }
    
    static class WriteFn extends DoFn<Document, Void> {
      private final Write spec;
      private transient MongoClient client;
      private List<Document> batch;
      WriteFn(Write spec) {
        this.spec = spec;
      }
      
      public void createMongoClient() {
        client = new MongoClient( new MongoClientURI( spec.uri(), getOptions( spec.maxConnectionIdleTime(), spec.sslEnabled(), spec.sslInvalidHostNameAllowed())));
      }
      
      public void startBundle() {
        batch = new ArrayList<>();
      }
      
      public void processElement(ProcessContext ctx) {
        batch.add(new Document(ctx.element()));
        if (batch.size() >= spec.batchSize()) {
          flush();
        }
        
      }
      
      public void finishBundle() {
        flush();
      }
      
      private void flush() {
        if (batch.isEmpty()) {
          return;
        }
        
        MongoDatabase mongoDatabase = client.getDatabase(spec.database());
        MongoCollection<Document> mongoCollection = mongoDatabase.getCollection(spec.collection());
        try {
          mongoCollection.insertMany(batch, new InsertManyOptions().ordered(spec.ordered()));
        }
         catch (MongoBulkWriteException e) {
          if (spec.ordered()) {
            throw e;
          }
          
        }
        
        batch.clear();
      }
      
      public void closeMongoClient() {
        client.close();
        client = null;
      }
      
    }
    
  }
  
}



commit 24ee2062cd46400a8656ffbc7d6fccbc75fc516f
Author: Lokesh Jain <ljain@apache.org>
Date:   Wed Jul 28 12:11:52 2021 +0530

    HDDS-5184. Use separate DB profile for Datanodes. (#2214)

diff --git a/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/utils/db/DatanodeDBProfile.java b/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/utils/db/DatanodeDBProfile.java
new file mode 100644
index 0000000000..508ac838ce
--- /dev/null
+++ b/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/utils/db/DatanodeDBProfile.java
@@ -0,0 +1,145 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ *  with the License.  You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *  Unless required by applicable law or agreed to in writing, software
+ *  distributed under the License is distributed on an "AS IS" BASIS,
+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *  See the License for the specific language governing permissions and
+ *  limitations under the License.
+ */
+
+package org.apache.hadoop.ozone.container.common.utils.db;
+
+import org.apache.hadoop.hdds.conf.ConfigurationSource;
+import org.apache.hadoop.hdds.conf.StorageUnit;
+import org.apache.hadoop.hdds.utils.db.DBProfile;
+import org.rocksdb.BlockBasedTableConfig;
+import org.rocksdb.DBOptions;
+import org.rocksdb.ColumnFamilyOptions;
+import org.rocksdb.LRUCache;
+
+import java.util.concurrent.atomic.AtomicReference;
+
+import static org.apache.hadoop.ozone.OzoneConfigKeys.HDDS_DATANODE_METADATA_ROCKSDB_CACHE_SIZE;
+import static org.apache.hadoop.ozone.OzoneConfigKeys.HDDS_DATANODE_METADATA_ROCKSDB_CACHE_SIZE_DEFAULT;
+
+/**
+ * The class manages DBProfiles for Datanodes. Since ColumnFamilyOptions need to
+ * be shared across containers the options are maintained in the profile itself.
+ */
+public abstract class DatanodeDBProfile {
+
+  /**
+   * Returns DBOptions to be used for rocksDB in datanodes.
+   */
+  public abstract DBOptions getDBOptions();
+
+  /**
+   * Returns ColumnFamilyOptions to be used for rocksDB column families in
+   * datanodes.
+   */
+  public abstract ColumnFamilyOptions getColumnFamilyOptions(
+      ConfigurationSource config);
+
+  /**
+   * Returns DatanodeDBProfile for corresponding storage type.
+   */
+  public static DatanodeDBProfile getProfile(DBProfile dbProfile) {
+    switch (dbProfile) {
+    case SSD:
+      return new SSD();
+    case DISK:
+      return new Disk();
+    default:
+      throw new IllegalArgumentException(
+          "DatanodeDBProfile does not exist for " + dbProfile);
+    }
+  }
+
+  /**
+   * DBProfile for SSD datanode disks.
+   */
+  public static class SSD extends DatanodeDBProfile {
+    private static final StorageBasedProfile SSD_STORAGE_BASED_PROFILE =
+        new StorageBasedProfile(DBProfile.SSD);
+
+    @Override
+    public DBOptions getDBOptions() {
+      return SSD_STORAGE_BASED_PROFILE.getDBOptions();
+    }
+
+    @Override
+    public ColumnFamilyOptions getColumnFamilyOptions(
+        ConfigurationSource config) {
+      return SSD_STORAGE_BASED_PROFILE.getColumnFamilyOptions(config);
+    }
+  }
+
+  /**
+   * DBProfile for HDD datanode disks.
+   */
+  public static class Disk extends DatanodeDBProfile {
+    private static final StorageBasedProfile DISK_STORAGE_BASED_PROFILE =
+        new StorageBasedProfile(DBProfile.DISK);
+
+    @Override
+    public DBOptions getDBOptions() {
+      return DISK_STORAGE_BASED_PROFILE.getDBOptions();
+    }
+
+    @Override
+    public ColumnFamilyOptions getColumnFamilyOptions(
+        ConfigurationSource config) {
+      return DISK_STORAGE_BASED_PROFILE.getColumnFamilyOptions(config);
+    }
+  }
+
+  /**
+   * Base profile for datanode storage disks.
+   */
+  private static final class StorageBasedProfile {
+    private final AtomicReference<ColumnFamilyOptions> cfOptions =
+        new AtomicReference<>();
+    private final DBProfile baseProfile;
+
+    private StorageBasedProfile(DBProfile profile) {
+      baseProfile = profile;
+    }
+
+    private DBOptions getDBOptions() {
+      return baseProfile.getDBOptions();
+    }
+
+    private ColumnFamilyOptions getColumnFamilyOptions(
+        ConfigurationSource config) {
+      cfOptions.updateAndGet(op -> op != null ? op :
+          baseProfile.getColumnFamilyOptions()
+              .setTableFormatConfig(getBlockBasedTableConfig(config)));
+      return cfOptions.get();
+    }
+
+    private BlockBasedTableConfig getBlockBasedTableConfig(
+        ConfigurationSource config) {
+      BlockBasedTableConfig blockBasedTableConfig =
+          baseProfile.getBlockBasedTableConfig();
+      if (config == null) {
+        return blockBasedTableConfig;
+      }
+
+      long cacheSize = (long) config
+          .getStorageSize(HDDS_DATANODE_METADATA_ROCKSDB_CACHE_SIZE,
+              HDDS_DATANODE_METADATA_ROCKSDB_CACHE_SIZE_DEFAULT,
+              StorageUnit.BYTES);
+      blockBasedTableConfig.setBlockCache(new LRUCache(cacheSize));
+      return blockBasedTableConfig;
+    }
+  }
+}
diff --git a/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/utils/db/package-info.java b/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/utils/db/package-info.java
new file mode 100644
index 0000000000..a96c1a941a
--- /dev/null
+++ b/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/utils/db/package-info.java
@@ -0,0 +1,22 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * This package contains files related to db use in datanodes.
+ */
+package org.apache.hadoop.ozone.container.common.utils.db;
\ No newline at end of file
diff --git a/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/metadata/AbstractDatanodeStore.java b/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/metadata/AbstractDatanodeStore.java
index c7aa299172..15a8a9eb5b 100644
--- a/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/metadata/AbstractDatanodeStore.java
+++ b/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/metadata/AbstractDatanodeStore.java
@@ -21,18 +21,15 @@
 import org.apache.hadoop.hdds.StringUtils;
 import org.apache.hadoop.hdds.annotation.InterfaceAudience;
 import org.apache.hadoop.hdds.conf.ConfigurationSource;
-import org.apache.hadoop.hdds.conf.StorageUnit;
 import org.apache.hadoop.hdds.utils.MetadataKeyFilters;
 import org.apache.hadoop.hdds.utils.MetadataKeyFilters.KeyPrefixFilter;
 import org.apache.hadoop.hdds.utils.db.*;
 import org.apache.hadoop.ozone.container.common.helpers.BlockData;
 import org.apache.hadoop.ozone.container.common.helpers.ChunkInfoList;
 import org.apache.hadoop.ozone.container.common.interfaces.BlockIterator;
-import org.rocksdb.BlockBasedTableConfig;
-import org.rocksdb.BloomFilter;
+import org.apache.hadoop.ozone.container.common.utils.db.DatanodeDBProfile;
 import org.rocksdb.ColumnFamilyOptions;
 import org.rocksdb.DBOptions;
-import org.rocksdb.LRUCache;
 import org.rocksdb.Statistics;
 import org.rocksdb.StatsLevel;
 import org.slf4j.Logger;
@@ -40,18 +37,13 @@
 
 import java.io.Closeable;
 import java.io.IOException;
-import java.util.Collections;
-import java.util.Map;
 import java.util.NoSuchElementException;
-import java.util.concurrent.ConcurrentHashMap;
 
 import static org.apache.hadoop.hdds.HddsConfigKeys.HDDS_DB_PROFILE;
 import static org.apache.hadoop.hdds.utils.db.DBStoreBuilder.HDDS_DEFAULT_DB_PROFILE;
 import static org.apache.hadoop.ozone.OzoneConfigKeys.OZONE_METADATA_STORE_ROCKSDB_STATISTICS;
 import static org.apache.hadoop.ozone.OzoneConfigKeys.OZONE_METADATA_STORE_ROCKSDB_STATISTICS_DEFAULT;
 import static org.apache.hadoop.ozone.OzoneConfigKeys.OZONE_METADATA_STORE_ROCKSDB_STATISTICS_OFF;
-import static org.apache.hadoop.ozone.OzoneConfigKeys.HDDS_DATANODE_METADATA_ROCKSDB_CACHE_SIZE;
-import static org.apache.hadoop.ozone.OzoneConfigKeys.HDDS_DATANODE_METADATA_ROCKSDB_CACHE_SIZE_DEFAULT;
 
 /**
  * Implementation of the {@link DatanodeStore} interface that contains
@@ -74,9 +66,7 @@ public abstract class AbstractDatanodeStore implements DatanodeStore {
   private final long containerID;
   private final ColumnFamilyOptions cfOptions;
 
-  private final DBProfile dbProfile;
-  private static final Map<ConfigurationSource, ColumnFamilyOptions>
-      OPTIONS_CACHE = new ConcurrentHashMap<>();
+  private static DatanodeDBProfile dbProfile;
   private final boolean openReadOnly;
 
   /**
@@ -89,16 +79,13 @@ protected AbstractDatanodeStore(ConfigurationSource config, long containerID,
       AbstractDatanodeDBDefinition dbDef, boolean openReadOnly)
       throws IOException {
 
-    dbProfile = config.getEnum(HDDS_DB_PROFILE,
-        HDDS_DEFAULT_DB_PROFILE);
+    dbProfile = DatanodeDBProfile
+        .getProfile(config.getEnum(HDDS_DB_PROFILE, HDDS_DEFAULT_DB_PROFILE));
 
     // The same config instance is used on each datanode, so we can share the
     // corresponding column family options, providing a single shared cache
     // for all containers on a datanode.
-    if (!OPTIONS_CACHE.containsKey(config)) {
-      OPTIONS_CACHE.put(config, buildColumnFamilyOptions(config));
-    }
-    cfOptions = OPTIONS_CACHE.get(config);
+    cfOptions = dbProfile.getColumnFamilyOptions(config);
 
     this.dbDef = dbDef;
     this.containerID = containerID;
@@ -214,13 +201,7 @@ public void compactDB() throws IOException {
   }
 
   @VisibleForTesting
-  public static Map<ConfigurationSource, ColumnFamilyOptions>
-      getColumnFamilyOptionsCache() {
-    return Collections.unmodifiableMap(OPTIONS_CACHE);
-  }
-
-  @VisibleForTesting
-  public DBProfile getDbProfile() {
+  public DatanodeDBProfile getDbProfile() {
     return dbProfile;
   }
 
@@ -236,22 +217,6 @@ private static void checkTableStatus(Table<?, ?> table, String name)
     }
   }
 
-  private ColumnFamilyOptions buildColumnFamilyOptions(
-      ConfigurationSource config) {
-    long cacheSize = (long) config.getStorageSize(
-        HDDS_DATANODE_METADATA_ROCKSDB_CACHE_SIZE,
-        HDDS_DATANODE_METADATA_ROCKSDB_CACHE_SIZE_DEFAULT,
-        StorageUnit.BYTES);
-
-    BlockBasedTableConfig tableConfig = new BlockBasedTableConfig();
-    tableConfig.setBlockCache(new LRUCache(cacheSize))
-        .setPinL0FilterAndIndexBlocksInCache(true)
-        .setFilterPolicy(new BloomFilter());
-
-    return dbProfile.getColumnFamilyOptions()
-        .setTableFormatConfig(tableConfig);
-  }
-
   /**
    * Block Iterator for KeyValue Container. This block iterator returns blocks
    * which match with the {@link MetadataKeyFilters.KeyPrefixFilter}. If no
diff --git a/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/TestKeyValueContainer.java b/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/TestKeyValueContainer.java
index fa97cd1c29..68cd2f6fb5 100644
--- a/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/TestKeyValueContainer.java
+++ b/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/TestKeyValueContainer.java
@@ -27,12 +27,14 @@
 import org.apache.hadoop.hdds.scm.container.common.helpers
     .StorageContainerException;
 import org.apache.hadoop.hdds.utils.db.DBProfile;
+import org.apache.hadoop.hdds.utils.db.RDBStore;
 import org.apache.hadoop.hdds.utils.db.Table;
 import org.apache.hadoop.ozone.OzoneConsts;
 import org.apache.hadoop.ozone.container.common.helpers.BlockData;
 import org.apache.hadoop.ozone.container.common.impl.ChunkLayOutVersion;
 import org.apache.hadoop.ozone.container.common.impl.ContainerDataYaml;
 import org.apache.hadoop.ozone.container.common.utils.StorageVolumeUtil;
+import org.apache.hadoop.ozone.container.common.utils.db.DatanodeDBProfile;
 import org.apache.hadoop.ozone.container.common.volume.HddsVolume;
 import org.apache.hadoop.ozone.container.common.volume
     .RoundRobinVolumeChoosingPolicy;
@@ -54,7 +56,9 @@
 import org.junit.runner.RunWith;
 import org.junit.runners.Parameterized;
 import org.mockito.Mockito;
+import org.rocksdb.ColumnFamilyHandle;
 import org.rocksdb.ColumnFamilyOptions;
+import org.rocksdb.RocksDBException;
 
 import java.io.File;
 
@@ -67,6 +71,7 @@
 import java.util.List;
 import java.util.UUID;
 import java.util.concurrent.atomic.AtomicReference;
+import java.util.function.Supplier;
 import java.util.stream.Collectors;
 import java.util.stream.IntStream;
 
@@ -434,29 +439,59 @@ public void testUpdateContainerUnsupportedRequest() throws Exception {
   }
 
   @Test
-  public void testContainersShareColumnFamilyOptions() throws Exception {
-    // Get a read only view (not a copy) of the options cache.
-    Map<ConfigurationSource, ColumnFamilyOptions> cachedOptions =
-        AbstractDatanodeStore.getColumnFamilyOptionsCache();
-
-    // Create Container 1
+  public void testContainerRocksDB()
+      throws StorageContainerException, RocksDBException {
+    closeContainer();
+    keyValueContainer = new KeyValueContainer(
+        keyValueContainerData, CONF);
     keyValueContainer.create(volumeSet, volumeChoosingPolicy, scmId);
-    ColumnFamilyOptions options1 = cachedOptions.get(CONF);
-    Assert.assertNotNull(options1);
 
-    // Create Container 2
-    keyValueContainerData = new KeyValueContainerData(2L,
-        layout,
-        (long) StorageUnit.GB.toBytes(5), UUID.randomUUID().toString(),
-        datanodeId.toString());
-    keyValueContainer = new KeyValueContainer(keyValueContainerData, CONF);
-    keyValueContainer.create(volumeSet, volumeChoosingPolicy, scmId);
+    try(ReferenceCountedDB db =
+        BlockUtils.getDB(keyValueContainerData, CONF)) {
+      RDBStore store = (RDBStore) db.getStore().getStore();
+      long defaultCacheSize = 64 * OzoneConsts.MB;
+      long cacheSize = Long.parseLong(store
+          .getProperty("rocksdb.block-cache-capacity"));
+      Assert.assertEquals(defaultCacheSize, cacheSize);
+      for (ColumnFamilyHandle handle : store.getColumnFamilyHandles()) {
+        cacheSize = Long.parseLong(
+            store.getProperty(handle, "rocksdb.block-cache-capacity"));
+        Assert.assertEquals(defaultCacheSize, cacheSize);
+      }
+    }
+  }
 
-    ColumnFamilyOptions options2 = cachedOptions.get(CONF);
-    Assert.assertNotNull(options2);
+  @Test
+  public void testContainersShareColumnFamilyOptions() {
+    ConfigurationSource conf = new OzoneConfiguration();
+
+    // Make sure ColumnFamilyOptions are same for a particular db profile
+    for (Supplier<DatanodeDBProfile> dbProfileSupplier : new Supplier[] {
+        DatanodeDBProfile.Disk::new, DatanodeDBProfile.SSD::new }) {
+      // ColumnFamilyOptions should be same across configurations
+      ColumnFamilyOptions columnFamilyOptions1 = dbProfileSupplier.get()
+          .getColumnFamilyOptions(new OzoneConfiguration());
+      ColumnFamilyOptions columnFamilyOptions2 = dbProfileSupplier.get()
+          .getColumnFamilyOptions(new OzoneConfiguration());
+      Assert.assertEquals(columnFamilyOptions1, columnFamilyOptions2);
+
+      // ColumnFamilyOptions should be same when queried multiple times
+      // for a particulat configuration
+      columnFamilyOptions1 = dbProfileSupplier.get()
+          .getColumnFamilyOptions(conf);
+      columnFamilyOptions2 = dbProfileSupplier.get()
+          .getColumnFamilyOptions(conf);
+      Assert.assertEquals(columnFamilyOptions1, columnFamilyOptions2);
+    }
 
-    // Column family options object should be reused.
-    Assert.assertSame(options1, options2);
+    // Make sure ColumnFamilyOptions are different for different db profile
+    DatanodeDBProfile diskProfile = new DatanodeDBProfile.Disk();
+    DatanodeDBProfile ssdProfile = new DatanodeDBProfile.SSD();
+    Assert.assertNotEquals(
+        diskProfile.getColumnFamilyOptions(new OzoneConfiguration()),
+        ssdProfile.getColumnFamilyOptions(new OzoneConfiguration()));
+    Assert.assertNotEquals(diskProfile.getColumnFamilyOptions(conf),
+        ssdProfile.getColumnFamilyOptions(conf));
   }
 
   @Test
@@ -464,7 +499,7 @@ public void testDBProfileAffectsDBOptions() throws Exception {
     // Create Container 1
     keyValueContainer.create(volumeSet, volumeChoosingPolicy, scmId);
 
-    DBProfile outProfile1;
+    DatanodeDBProfile outProfile1;
     try (ReferenceCountedDB db1 =
         BlockUtils.getDB(keyValueContainer.getContainerData(), CONF)) {
       DatanodeStore store1 = db1.getStore();
@@ -484,7 +519,7 @@ public void testDBProfileAffectsDBOptions() throws Exception {
     keyValueContainer = new KeyValueContainer(keyValueContainerData, otherConf);
     keyValueContainer.create(volumeSet, volumeChoosingPolicy, scmId);
 
-    DBProfile outProfile2;
+    DatanodeDBProfile outProfile2;
     try (ReferenceCountedDB db2 =
         BlockUtils.getDB(keyValueContainer.getContainerData(), otherConf)) {
       DatanodeStore store2 = db2.getStore();
diff --git a/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/DBProfile.java b/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/DBProfile.java
index 442fd25a19..2fa5d22187 100644
--- a/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/DBProfile.java
+++ b/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/DBProfile.java
@@ -48,25 +48,13 @@ public String toString() {
 
     @Override
     public ColumnFamilyOptions getColumnFamilyOptions() {
-
-      // Set BlockCacheSize to 256 MB. This should not be an issue for HADOOP.
-      final long blockCacheSize = toLong(StorageUnit.MB.toBytes(256.00));
-
-      // Set the Default block size to 16KB
-      final long blockSize = toLong(StorageUnit.KB.toBytes(16));
-
       // Write Buffer Size -- set to 128 MB
       final long writeBufferSize = toLong(StorageUnit.MB.toBytes(128));
 
       return new ColumnFamilyOptions()
           .setLevelCompactionDynamicLevelBytes(true)
           .setWriteBufferSize(writeBufferSize)
-          .setTableFormatConfig(
-              new BlockBasedTableConfig()
-                  .setBlockCache(new LRUCache(blockCacheSize))
-                  .setBlockSize(blockSize)
-                  .setPinL0FilterAndIndexBlocksInCache(true)
-                  .setFilterPolicy(new BloomFilter()));
+          .setTableFormatConfig(getBlockBasedTableConfig());
     }
 
     @Override
@@ -85,6 +73,20 @@ public DBOptions getDBOptions() {
           .setCreateMissingColumnFamilies(createMissingColumnFamilies);
     }
 
+    @Override
+    public BlockBasedTableConfig getBlockBasedTableConfig() {
+      // Set BlockCacheSize to 256 MB. This should not be an issue for HADOOP.
+      final long blockCacheSize = toLong(StorageUnit.MB.toBytes(256.00));
+
+      // Set the Default block size to 16KB
+      final long blockSize = toLong(StorageUnit.KB.toBytes(16));
+
+      return new BlockBasedTableConfig()
+          .setBlockCache(new LRUCache(blockCacheSize))
+          .setBlockSize(blockSize)
+          .setPinL0FilterAndIndexBlocksInCache(true)
+          .setFilterPolicy(new BloomFilter());
+    }
 
   },
   DISK {
@@ -106,7 +108,10 @@ public ColumnFamilyOptions getColumnFamilyOptions() {
       return columnFamilyOptions;
     }
 
-
+    @Override
+    public BlockBasedTableConfig getBlockBasedTableConfig() {
+      return SSD.getBlockBasedTableConfig();
+    }
   };
 
   private static long toLong(double value) {
@@ -117,4 +122,6 @@ private static long toLong(double value) {
   public abstract DBOptions getDBOptions();
 
   public abstract ColumnFamilyOptions getColumnFamilyOptions();
+
+  public abstract BlockBasedTableConfig getBlockBasedTableConfig();
 }
diff --git a/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/RDBStore.java b/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/RDBStore.java
index c67238d353..b50b46225e 100644
--- a/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/RDBStore.java
+++ b/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/RDBStore.java
@@ -24,6 +24,7 @@
 import java.io.IOException;
 import java.nio.file.Paths;
 import java.util.ArrayList;
+import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
@@ -369,6 +370,10 @@ public Map<Integer, String> getTableNames() {
     return tableNames;
   }
 
+  public List<ColumnFamilyHandle> getColumnFamilyHandles() {
+    return Collections.unmodifiableList(columnFamilyHandles);
+  }
+
   @Override
   public CodecRegistry getCodecRegistry() {
     return codecRegistry;
@@ -424,6 +429,15 @@ public RocksDB getDb() {
     return db;
   }
 
+  public String getProperty(String property) throws RocksDBException {
+    return db.getProperty(property);
+  }
+
+  public String getProperty(ColumnFamilyHandle handle, String property)
+      throws RocksDBException {
+    return db.getProperty(handle, property);
+  }
+
   public RDBMetrics getMetrics() {
     return rdbMetrics;
   }
